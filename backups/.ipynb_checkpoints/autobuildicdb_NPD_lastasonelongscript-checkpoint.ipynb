{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>autobuildicdb NPD</h1><br>\n",
    "Auto-populate a blank IC database (SQL Server) with live well data from the <a href=\"https://factpages.npd.no/factpages/Default.aspx?culture=en\", target=\"_blank\">NPD FactPages</a>.\n",
    "\n",
    "<b>Part 1. Download the following data types, reformat for IC and export to .csv</b>\n",
    "\n",
    "Exploration well headers<br>\n",
    "Development well headers<br>\n",
    "Core intervals<br>\n",
    "Core photos<br>\n",
    "Thin sections<br>\n",
    "CO2<br>\n",
    "Oil samples<br>\n",
    "Lithostratigraphy<br>\n",
    "Drill stem tests<br>\n",
    "Casing and leak-off tests<br>\n",
    "Drilling mud<br>\n",
    "References and Documents<br>\n",
    "\n",
    "<b>Part 2. Connect to the database and populate tables</b><br>\n",
    "\n",
    "dbo.WELLS<br>\n",
    "dbo.t_WellsUserFields<br>\n",
    "dbo.t_WellsUserFieldsValues<br>\n",
    "dbo.PROJECTS<br>\n",
    "dbo.T_WELLQUERYFOLDERS<br>\n",
    "dbo.WELLQUERIES<br>\n",
    "dbo.PROJECTWELLS<br>\n",
    "dbo.tablenames<br>\n",
    "dbo.intervalcolumns<br>\n",
    "DATA_Core_NPD<br>\n",
    "DATA_Petrography_NPD<br>\n",
    "DATA_CO2_NPD<br>\n",
    "DATA_OilSample_NPD<br>\n",
    "DATA_Lithostrat_NPD<br>\n",
    "DATA_DrillStemTest_NPD<br>\n",
    "DATA_CasingLOT_NPD<br>\n",
    "DATA_DrillingMud_NPD<br>\n",
    "DATA_Tops_NPD\n",
    "\n",
    "Only handles data for Well Headers, References and Lithostrat just now.<br>\n",
    "Creates dynamic IC projects, well queries, and builds text dictionaries from Lithostrat.<br>\n",
    "\n",
    "<b>Additional scripts</b>\n",
    "\n",
    "Build Text Dictionaries from Lithostrat.<br>\n",
    "Download all Core Photos to file (using hyperlinks).<br>\n",
    "Download and unzip shapefiles from 'NPD Map Services'.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import ExcelFile\n",
    "from pandas import ExcelWriter\n",
    "import requests, zipfile, io\n",
    "import pyodbc\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, delete, insert, select, func, sql\n",
    "from sqlalchemy.types import SmallInteger, Integer, String, Float, NVARCHAR\n",
    "from datetime import datetime\n",
    "import pprint\n",
    "import urllib\n",
    "import urllib.request\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# %pprint\n",
    "# pp = pprint.PrettyPrinter(indent=10)\n",
    "\n",
    "print('Pandas version: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Pandas display settings to show all columns\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "#pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC database folder\n",
    "dbdir = 'C:\\ICData\\Test51'\n",
    "\n",
    "# Output folder\n",
    "outdir = '{}\\output_data'.format(dbdir)\n",
    "\n",
    "# Create outdir folder within IC database folder\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to save dataframe to oudir and return header\n",
    "\n",
    "def output_to_csv(outname, df):\n",
    "\n",
    "    filepath = '{}\\{}.csv'.format(outdir, outname)\n",
    "    print('Saved to:', filepath)\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    return pd.read_csv(filepath).head(3)\n",
    "\n",
    "# Example: output_to_csv(outname='IC_wellbore_exploration_all', df=df_explo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment your chosen data source -\n",
    "    # web: select to download data live from NPD FactPages using parameterized query strings (see https://factpages.npd.no/factpages/Parameters.aspx)\n",
    "    # file: select if you have manually downloaded data in Excel format and saved to 'input data' folder \n",
    "\n",
    "#data_source = 'web'\n",
    "data_source = 'file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data, reformat for IC and output to .csv\n",
    "    \n",
    "## Well Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the latest NPD well headers in Excel format\n",
    "# Navigate to NPD Factpages > Wellbore > Table View > Exploration/Development > All - Long List> Export Excel.\n",
    "# Assign to two dataframes, one for Exploraion wells and one for Development wells\n",
    "\n",
    "if data_source == 'web':\n",
    "    df_explo = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_exploration_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.169&CultureCode=en')\n",
    "    df_dev = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_development_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.169&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    # Navigate to NPD Factpages > Wellbore > Table View > Exploration/Development > All - Long List> Export Excel.\n",
    "    df_explo = pd.read_excel('input data/wellbore_exploration_all.xlsx')\n",
    "    df_dev = pd.read_excel('input data/wellbore_development_all.xlsx')\n",
    "\n",
    "# Print the original column titles in each dataframe.\n",
    "print(\"\\nExploration well header column titles:\")\n",
    "print(list(df_explo.columns))\n",
    "print(\"\\nDevelopment well header column titles:\")\n",
    "print(list(df_dev.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_explo_rows, num_explo_cols) = df_explo.shape\n",
    "(num_dev_rows, num_dev_cols) = df_dev.shape\n",
    "print('{} rows and {} columns in Exploration wells.'.format(num_explo_rows, num_explo_cols))\n",
    "print('{} rows and {} columns in Development wells.'.format(num_dev_rows, num_dev_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Exploration well headers:')\n",
    "df_explo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Development well headers:')\n",
    "df_dev.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column headers unique to Explo & Dev wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explo_columns = df_explo.columns.tolist()\n",
    "dev_columns = df_dev.columns.tolist()\n",
    "\n",
    "# List well headers unqiue to each dataframe\n",
    "print('Attributes unique to Exploration wells:\\n', sorted(set(explo_columns) - set(dev_columns)))\n",
    "print('\\nAttributes unique to Development wells:\\n', sorted(set(dev_columns) - set(explo_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename attributes for IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are IC's default well header attributes (when matching columns in Import Well Header File)\n",
    "# Try to use as many of these as possible when renaming below.\n",
    "# Any other columns will need to be added to IC as Well Attributes.\n",
    "\n",
    "ic_default_attributes = {'Name', 'Code', 'Alternate 1', 'Alternate 2', 'API number', 'UWI number', 'Comment', 'Geodatum', \n",
    "                         'Longitude', 'Latitude', 'Grid system', 'Surface X', 'Surface Y', 'Elevation Reference',\n",
    "                         'Elevation', 'KBE', 'RTE', 'DFE', 'GLE', 'SPUD date', 'Completion date', 'Status', \n",
    "                         'Quadrant', 'Block', 'Sub block', 'Field', 'Location', 'Operator', 'Country',\n",
    "                         'Basin', 'Province', 'County', 'State', 'Section', 'Township', 'Range', 'Terminal depth',\n",
    "                         'Water depth', 'Facility', 'Discovery name', 'Seismic line', 'Intent', 'Licence number'}\n",
    "\n",
    "# Rename columns from/to. \n",
    "# Check spelling and capitalisation carefully when renaming to match IC's default attributes.\n",
    "\n",
    "attributes_to_rename = {'Wellbore name' : 'Name',\n",
    "                        'Well name' : 'Alternate 1',\n",
    "                        'Drilling operator' : 'Operator',\n",
    "                        'Drilled in production licence' : 'Licence number',\n",
    "                        'Purpose' : 'Intent',\n",
    "                        'Purpose - planned' : 'Intent - planned',\n",
    "                        'Status' : 'Well status',\n",
    "                        'Content' : 'Well content',\n",
    "                        'Entered date' : 'SPUD date',\n",
    "                        'Completed date' : 'Completion date',\n",
    "                        'Discovery' : 'Discovery name',\n",
    "                        'Seismic location' : 'Seismic line',\n",
    "                        'Kelly bushing elevation [m]' : 'KBE',\n",
    "                        'Total depth (MD) [m RKB]' : 'Terminal depth',\n",
    "                        'Water depth [m]' : 'Water depth',\n",
    "                        'Kick off  point [m RKB]' : 'Kick off point [m RKB]', #remove extra space\n",
    "                        'Main area' : 'Location',\n",
    "                        'Drilling facility' : 'Facility',\n",
    "                        '1st level with HC, formation' : '1st level with HC formation', #remove commas to be csv friendly\n",
    "                        '1st level with HC, age' : '1st level with HC age',\n",
    "                        '2nd level with HC, formation' : '2nd level with HC formation',\n",
    "                        '2nd level with HC, age' : '2nd level with HC age',\n",
    "                        '3rd level with HC, formation' : '3rd level with HC formation',\n",
    "                        '3rd level with HC, age' : '3rd level with HC age',\n",
    "                        'Geodetic datum' : 'Geodatum',\n",
    "                        'NS decimal degrees' : 'Latitude',\n",
    "                        'EW decimal degrees' : 'Longitude',\n",
    "                        'NS UTM [m]' : 'Surface Y',\n",
    "                        'EW UTM [m]' : 'Surface X',\n",
    "                        'Wellbore name, part 1' : 'Quadrant',\n",
    "                        'Wellbore name, part 2' : 'Block', \n",
    "                        'Pressrelease url' : 'Press Release URL',\n",
    "                        'FactPage url' : 'FactPage URL',\n",
    "                        'Factmaps' : 'FactMaps URL'}\n",
    "\n",
    "# Apply renaming to each of the dataframes\n",
    "df_explo.rename(columns=attributes_to_rename, inplace=True)\n",
    "df_dev.rename(columns=attributes_to_rename, inplace=True)\n",
    "\n",
    "# QC only renamed columns\n",
    "print(\"Renamed attributes only:\")\n",
    "renamed_columns = list(attributes_to_rename.values())\n",
    "df_explo[renamed_columns].head(3)\n",
    "#df_dev[renamed_columns].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete attributes containing duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates are repeated elsewhere so we can delete the component parts from the dataframes.\n",
    "# And we've renamed Wellbore name parts 1 and 2 to Quadrant and Block, and do not need the other parts.\n",
    "\n",
    "attributes_to_drop = ['Plot symbol', 'NS degrees', 'NS minutes', 'NS seconds', 'NS code', 'EW degrees', 'EW minutes', 'EW seconds', 'EW code', \n",
    "                      'Wellbore name, part 3', 'Wellbore name, part 4', 'Wellbore name, part 5', 'Wellbore name, part 6']\n",
    "\n",
    "df_explo.drop(attributes_to_drop, axis=1, inplace=True)\n",
    "df_dev.drop(attributes_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print('Prove we still have well names and coordinates:')\n",
    "df_explo[['Name', 'Latitude', 'Longitude', 'Surface Y', 'Surface X']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate well list based on column and value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the column and values you want to return, e.g. Location: BARENTS SEA, or Quadrant: 6204, 6205.\n",
    "fltr_column = 'Location'\n",
    "\n",
    "# List the names you want to *KEEP*!\n",
    "fltr_value = ['NORTH SEA', 'NORWEGIAN SEA', 'BARENTS SEA']\n",
    "\n",
    "# Apply the filter to the dataframes\n",
    "indexNames = df_explo[~df_explo[fltr_column].isin(fltr_value)].index\n",
    "df_explo.drop(indexNames , inplace=True)\n",
    "\n",
    "indexNames = df_dev[~df_dev[fltr_column].isin(fltr_value)].index\n",
    "df_dev.drop(indexNames , inplace=True)\n",
    "\n",
    "# Get dataframe shape and unpack tuples\n",
    "(exploRows, exploCols) = df_explo.shape\n",
    "(devRows, devCols) = df_dev.shape\n",
    "\n",
    "# Print out the results\n",
    "print(\"After filtering on {}: {}, you are left with:\\n {} rows for Exploration wells, and {} rows for Development wells.\"\n",
    "      .format(fltr_column, fltr_value, exploRows, devRows))\n",
    "print('The first and last rows are:')\n",
    "\n",
    "# Print the first and last rows of the Exploration dataframe to check that the filter has worked\n",
    "df_explo.iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE FILES - create Reference files for IC containing URLs for Explo and Dev wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts three URL columns into three rows. Adds a Title column and sorts by Well and Title.\n",
    "df_explo_references = df_explo[['Name', 'NPDID wellbore', 'Press Release URL', 'FactPage URL', 'FactMaps URL']]\n",
    "df_explo_references = pd.melt(df_explo_references, id_vars=['Name', 'NPDID wellbore'], value_vars=['Press Release URL', 'FactPage URL', 'FactMaps URL'], var_name='Title', value_name='URL')\n",
    "df_explo_references.sort_values(['Name', 'Title'], inplace=True)\n",
    "\n",
    "# Remove empty rows, specifically where no 'Press Release URL' for Exploration references\n",
    "df_explo_references['URL'].replace(' ', np.nan, inplace=True)\n",
    "df_explo_references.dropna(subset=['URL'], inplace=True)\n",
    "\n",
    "# Name and create file for Exploration wells\n",
    "output_to_csv(outname='IC_explo_references', df=df_explo_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, but creates 'Reference' file for Development Wells (minus the Press Release URL)\n",
    "df_dev_references = df_dev[['Name', 'NPDID wellbore', 'FactPage URL', 'FactMaps URL']]\n",
    "df_dev_references = pd.melt(df_dev_references, id_vars=['Name', 'NPDID wellbore'], \n",
    "                            value_vars=['FactPage URL', 'FactMaps URL'], \n",
    "                            var_name='Title', value_name='URL')\n",
    "df_dev_references.sort_values(['Name', 'Title'], inplace=True)\n",
    "\n",
    "# Name and create file for Development wells\n",
    "output_to_csv(outname='IC_dev_references', df=df_dev_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop URL attributes\n",
    "# Now that we've output the URLs to separate files, we no longer need them in the Exploration and Development dataframes.\n",
    "df_explo.drop(['Press Release URL', 'FactPage URL', 'FactMaps URL'], axis=1, inplace=True)\n",
    "df_dev.drop(['FactPage URL', 'FactMaps URL'], axis=1, inplace=True)\n",
    "\n",
    "df_explo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column(s) and assign constant value, e.g. Country: NORWAY.\n",
    "df_explo['Country'] = 'NORWAY' \n",
    "df_dev['Country'] = 'NORWAY'\n",
    "\n",
    "# IC Version 4.3.1 and earlier only. Fixed in 4.3.2.\n",
    "# First lets rename an extraordinarily long string in column 'Seismic line' to avoid an error in IC.\n",
    "#df_explo['Seismic line'] = df_explo['Seismic line'].replace('TUN15M01 3D bin datasett: Inline reference: 12688 Croslline reference: between 12383 and 12384', 'TUN15M01 3D bin: Inline 12688 Crossline 12383-12384')\n",
    "\n",
    "# Remove decimal places introduced to the 'NPDIP' columns\n",
    "df_explo['NPDID discovery'] = df_explo['NPDID discovery'].fillna(0).astype(int)\n",
    "df_dev['NPDID discovery'] = df_dev['NPDID discovery'].fillna(0).astype(int)\n",
    "\n",
    "df_explo['NPDID drilling facility'] = df_explo['NPDID drilling facility'].fillna(0).astype(int)\n",
    "df_dev['NPDID drilling facility'] = df_dev['NPDID drilling facility'].fillna(0).astype(int)\n",
    "\n",
    "df_explo['NPDID field'] = df_explo['NPDID field'].fillna(0).astype(int)\n",
    "df_dev['NPDID field'] = df_dev['NPDID field'].fillna(0).astype(int)\n",
    "\n",
    "# Copy data from one column to another, preserving the original.\n",
    "df_explo['UWI number'] = df_explo['NPDID wellbore']\n",
    "df_dev['UWI number'] = df_dev['NPDID wellbore']\n",
    "\n",
    "# Check the result\n",
    "df_explo[['Name', 'Country', 'NPDID drilling facility', 'NPDID wellbore']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Well Status & Well Content to match IC's Well Symbols dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a new column called 'Status', combining 'Well Status' and 'Well Content'\n",
    "# These values should match IC's Well Symbols graphic dictionary entries, e.g. \"P & A Oil Shows\"\n",
    "\n",
    "# Change 'P&A' to 'P & A'.\n",
    "df_explo['Well status'] = df_explo['Well status'].replace(to_replace='P&A', value='P & A')\n",
    "# First letter of each word capitalised\n",
    "df_explo['Status'] = df_explo['Well status'].str.title() + ' ' + df_explo['Well content'].str.title()\n",
    "\n",
    "# As above but for Development wells\n",
    "df_dev['Well status'] = df_dev['Well status'].replace(to_replace='P&A', value='P & A')\n",
    "df_dev['Status'] = df_dev['Well status'].str.title() + ' ' + df_dev['Well content'].str.title()\n",
    "\n",
    "# Replace a few other things to help with matching\n",
    "df_explo = df_explo.replace({'Status' : { ' Not Available' : '', ' Not Applicable' : '', '/' : ' ', 'Oil Gas ' : 'Oil & Gas '}}, regex=True)\n",
    "df_dev = df_dev.replace({'Status' :     { ' Not Available' : '', ' Not Applicable' : '', '/' : ' ', 'Oil Gas ' : 'Oil & Gas '}}, regex=True)\n",
    "\n",
    "# Rename Status to status? (links to symbol_id??? e.g. 22)\n",
    "\n",
    "# Check the results\n",
    "df_explo[['Name', 'Status']].head(n=10)\n",
    "#df_dev[['Name', 'Status']].tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique entries under Status for all wells.\n",
    "# In IC, open Database > Graphic Dictionaries > Well Symbols, and ensure you have dictionary entries for each.\n",
    "\n",
    "lst_explo_status = sorted(set(df_explo['Status'].astype(str)))\n",
    "lst_dev_status = sorted(set(df_dev['Status'].astype(str)))\n",
    "\n",
    "lst_all_status = lst_explo_status + lst_dev_status\n",
    "\n",
    "print(\"{} unique status values to include in IC 'Well Symbols' graphic dictionary:\".format(len(lst_all_status)))\n",
    "print('')\n",
    "lst_unique_status = sorted(set(lst_all_status))\n",
    "print(', '.join(lst_unique_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate cells to create 'Grid system' in IC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# At time of writing, there are several problems with 'Geodatum' in the NPD datasets, including:\n",
    "#  - trailing spaces ('ED50 ') in all Explo wells\n",
    "#  - erroneous '56ED50', '60ED50' and '61ED50' values in Dev wells\n",
    "#  - missing 'ED50' values in two explo wells\n",
    "# Luckily, we can just force 'ED50' on all these cells!\n",
    "\n",
    "df_explo['Geodatum'] = 'ED50'\n",
    "df_dev['Geodatum'] = 'ED50'\n",
    "\n",
    "# Concatenate cells to create a new column 'Grid system' in IC format (e.g. \"ED50 / UTM Zone 31N\")\n",
    "\n",
    "df_explo['Grid system'] = df_explo['Geodatum'] + ' / ' + 'UTM zone ' + df_explo['UTM zone'].map(str) + 'N'\n",
    "df_dev['Grid system'] = df_dev['Geodatum'] + ' / ' + 'UTM zone ' + df_dev['UTM zone'].map(str) + 'N'\n",
    "\n",
    "print('Geodatum and Grid systems for IC:')\n",
    "df_explo[['Name', 'Geodatum', 'Grid system']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Well Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out attributes lists, reflecting all the changes above.\n",
    "# Use these lists to check the current order of your columns in each, and consider how you might like to re-order them.\n",
    "# Any columns created above (including: Country, Status, Grid system) currently appear at the end of the lists.\n",
    "\n",
    "print('--- BEFORE RE-ORDERING ---\\n')\n",
    "print(len(df_explo.columns), 'Exploration attributes:\\n', list(df_explo.columns), '\\n')\n",
    "print(len(df_dev.columns), 'Development attributes:\\n', list(df_dev.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order all columns (OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specifies the order of columns for Exploration and Development wells in the final outputs.\n",
    "# # It's not compulsory to re-order columns, as IC lists all non-default attributes alphabetically.\n",
    "\n",
    "# explo_order = ['Name', 'Alternate 1', 'UWI number', 'Quadrant', 'Block', 'Operator', 'Licence number', 'Intent', \n",
    "#                 'Intent - planned', 'Well status', 'Well content', 'Status', 'Type', 'Subsea', 'SPUD date', \n",
    "#                 'Completion date', 'Field', 'Drill permit', 'Discovery name', 'Discovery wellbore', \n",
    "#                 'Bottom hole temperature [°C]', 'Seismic line', 'Maximum inclination [°]', 'KBE', \n",
    "#                 'Final vertical depth (TVD) [m RKB]', 'Terminal depth', 'Water depth', 'Kick off point [m RKB]', \n",
    "#                 'Oldest penetrated age', 'Oldest penetrated formation', 'Location', 'Country', 'Facility', \n",
    "#                 'Drilling facility type', 'Drilling facility category', 'Licensing activity awarded in', \n",
    "#                 'Multilateral', 'Entry year', 'Completed year', 'Reclassified from/to wellbore', 'Reentry activity', \n",
    "#                 'Plot symbol', '1st level with HC formation', '1st level with HC age', '2nd level with HC formation', \n",
    "#                 '2nd level with HC age', '3rd level with HC formation', '3rd level with HC age', 'Drilling days', \n",
    "#                 'Reentry', 'Geodatum', 'Latitude', 'Longitude', 'Surface X', 'Surface Y', 'UTM zone', 'Grid system', \n",
    "#                 'DISKOS Well Type', 'DISKOS Wellbore Parent', \n",
    "#                 'Publication date', 'Release date', 'NPDID wellbore', 'NPDID discovery', 'NPDID field', \n",
    "#                 'NPDID drilling facility', 'NPDID wellbore reclassified from', 'NPDID production licence drilled in', \n",
    "#                 'Date main level updated', 'Date all updated', 'Date sync NPD']\n",
    "\n",
    "# dev_order = ['Name', 'Alternate 1', 'UWI number', 'Quadrant', 'Block', 'Operator', 'Licence number', 'Intent', \n",
    "#               'Intent - planned', 'Well status', 'Well content',  'Status', 'Content - planned', 'Type', 'Subsea',\n",
    "#               'SPUD date', 'Completion date', 'Field', 'Predrilled entry date','Predrilled completion date', \n",
    "#               'Drill permit', 'Discovery name', 'Discovery wellbore', 'KBE', 'Final vertical depth (TVD) [m RKB]',\n",
    "#               'Terminal depth', 'Water depth', 'Kick off point [m RKB]', 'Location', 'Country', 'Facility', \n",
    "#               'Drilling facility type', 'Drilling facility category', 'Licensing activity awarded in', \n",
    "#               'Production facility', 'Multilateral', 'Entry year', 'Completed year','Reclassified from/to wellbore', \n",
    "#               'Plot symbol', 'Geodatum', 'Latitude', 'Longitude', 'Surface Y', 'Surface X', 'UTM zone',  'Grid system', \n",
    "#               'DISKOS Well Type', 'DISKOS Wellbore Parent', 'NPDID wellbore', \n",
    "#               'NPDID discovery', 'NPDID field', 'Publication date', 'Release date', 'NPDID production licence drilled in', \n",
    "#               'NPDID drilling facility', 'NPDID production facility','NPDID wellbore reclassified from', \n",
    "#               'Date main level updated', 'Date all updated', 'Date sync NPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if your list of re-ordered attributes is complete.\n",
    "# missing_explo = set(df_explo.columns).difference(explo_order)\n",
    "# missing_dev = set(df_dev.columns).difference(dev_order)\n",
    "\n",
    "# if len(missing_explo) > 0:\n",
    "#     print('Your re-ordered list of Exploration attributes is incomplete. You must include:\\n {}.\\n'.format(missing_explo))\n",
    "# else:\n",
    "#     print('Your re-ordered list of Exploration attributes is complete.\\n')\n",
    "    \n",
    "# if len(missing_dev) > 0:\n",
    "#     print('Your re-ordered list of Development attributes is incomplete. You must include:\\n {}.'.format(missing_dev))\n",
    "# else:\n",
    "#     print('Your re-ordered list of Development attributes is complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only when your re-ordered lists of Exploration and Development attributes are complete should you run this cell,\n",
    "# # Otherwise these will not be included in the output file!\n",
    "# # Applies the re-ordering to the dataframes\n",
    "\n",
    "# df_explo = df_explo.reindex(columns=explo_order)\n",
    "# df_dev = df_dev.reindex(columns=dev_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all unique values for selected attributes (example: Operator and Field)\n",
    "\n",
    "def lstheaderfields (*args):\n",
    "    for arg in args:\n",
    "        print('---' , arg, '---')\n",
    "        print('')\n",
    "        words = [x for x in df_explo[arg].unique()]\n",
    "        print('Exploration wells:')\n",
    "        print(words)\n",
    "        print('')\n",
    "        words = [x for x in df_dev[arg].unique()]\n",
    "        print('Development wells:')\n",
    "        print(words)\n",
    "        print(\"\")\n",
    "        \n",
    "# Enter the names of columns you would like to check\n",
    "lstheaderfields('Operator', 'Field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE FILES - create well header files for explo and dev wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output exploration well headers\n",
    "\n",
    "print('{} exploration wells from {} to {}.'.format(len(df_explo), \n",
    "                                                   df_explo['Name'][df_explo.index[0]], \n",
    "                                                   df_explo['Name'][df_explo.index[-1]]))\n",
    "\n",
    "output_to_csv(outname='IC_wellbore_exploration_all', df=df_explo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output development well headers\n",
    "\n",
    "print('{} development wells {} to {}.'.format(len(df_dev), \n",
    "                                                   df_dev['Name'][df_dev.index[0]], \n",
    "                                                   df_dev['Name'][df_dev.index[-1]]))\n",
    "\n",
    "output_to_csv(outname='IC_wellbore_development_all', df=df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well Attributes to create in IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following attributes are not IC defaults and need to be created under Wells > Attributes.\n",
    "# Alternatively, use the SQL code produced in the next cell to create these rows in SSMS. \n",
    "\n",
    "# Find the full list of attributes after all the editing you've done above\n",
    "all_attributes = set(list(df_explo.columns) + list(df_dev.columns))\n",
    "\n",
    "# Find and count those attributes you'll need to create in IC\n",
    "non_default_attributes = list(set(all_attributes).difference(ic_default_attributes))\n",
    "non_default_attributes.sort()\n",
    "num_non_default_attributes = len(non_default_attributes)\n",
    "\n",
    "print('The following {} attributes are not IC defaults and must be added to IC:\\n'.format(num_non_default_attributes))\n",
    "print(list(non_default_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you have database administration privileges, you can use this cell \n",
    "# to generate the SQL Query code that will create Well Attributes in IC in the format:\n",
    "    \n",
    "    #INSERT INTO t_WellsUserFields (f_FieldId, f_FieldName, f_IsInputUsed, f_InputID, f_Description, f_Origin, f_SortOrder)\n",
    "        #VALUES (1, 'Attribute', 'False', 0, 'Description of attribute', 0, 1);\n",
    "\n",
    "        # This assumes you have no yet created any Well Attributes in IC. \n",
    "        # If you have already, you'll need to tweak the 3 variables below.\n",
    "\n",
    "pk_index = 0 #Enter one less than your highest pk_index\n",
    "original_pk_index = 0 #Enter the same number as above (this one we won't change)\n",
    "f_sortorder = 0 #Enter the next appropriate f_sortorder\n",
    "\n",
    "print(\"INSERT INTO t_WellsUserFields\")\n",
    "print(\"  (f_FieldId, f_FieldName, f_IsInputUsed, f_InputID, f_Description, f_Origin, f_SortOrder)\")\n",
    "print(\"VALUES\")\n",
    "\n",
    "for i in non_default_attributes:\n",
    "    pk_index += 1\n",
    "    f_sortorder += 1\n",
    "    if pk_index < (num_non_default_attributes + original_pk_index):\n",
    "        print(\"  ({x}, '{y}', 'False', 0, 'Userfield {y}', 0, {z}),\".format(x = pk_index, y = i, z = f_sortorder))\n",
    "    else:\n",
    "        print(\"  ({x}, '{y}', 'False', 0, 'Userfield {y}', 0, {z});\".format(x = pk_index, y = i, z = f_sortorder))\n",
    "\n",
    "# Follow these steps:\n",
    "    # 1. Open your IC database in SQL Server Management Studio. IC must be closed/computer restarted to open a LocalDB in SSMS.\n",
    "    # 2. Expand 'Tables', scroll down to 't_WellsUserFields' and right-click 'Edit Top 200 Rows'.\n",
    "    # 3. Press Ctrl+N to create a new query, copy and paste the following SQL code to the blank query and hit F5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install correct co-ordinate systems to IC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In IC, open Project > Properties > Coords > Coordinate Systems\n",
    "# Ensure each of the following co-ordinate system are installed **before importing well headers**\n",
    "# Or use the cell below to write to file\n",
    "\n",
    "lst_geodatum = sorted(set(df_explo['Geodatum'].astype(str)))\n",
    "print('Geodatum:', ', '.join(lst_geodatum))\n",
    "\n",
    "lst_gridsystem = sorted(set(df_explo['Grid system'].astype(str)))\n",
    "print('Grid systems:', ', '.join(lst_gridsystem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Norwegian UTM Zones to Projections.def\n",
    "# Note that Projections.def already contains geodatum ED50.\n",
    "\n",
    "f = open(dbdir + '\\Support\\Projections.def', 'w+')\n",
    "\n",
    "f.write('''# ED50\n",
    "<4230> +proj=longlat +ellps=intl +no_defs <>\n",
    "# ED50 / UTM zone 31N\n",
    "<23031> +proj=utm +zone=31 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 32N\n",
    "<23032> +proj=utm +zone=32 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 33N\n",
    "<23033> +proj=utm +zone=33 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 34N\n",
    "<23034> +proj=utm +zone=34 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 35N\n",
    "<23035> +proj=utm +zone=35 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 36N\n",
    "<23036> +proj=utm +zone=36 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "# ED50 / UTM zone 37N\n",
    "<23037> +proj=utm +zone=37 +ellps=intl +towgs84=-87,-98,-121,0,0,0,0 +units=m +no_defs <>\n",
    "''')\n",
    "\n",
    "f.seek(0)\n",
    "contents = f.read()\n",
    "print(contents)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data to IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before importing data to IC, ensure you have followed the last few steps to:\n",
    "# - Create the appropriate Well Attributes in your IC Database.\n",
    "# - Add the correct coordinate systems to your IC Project.\n",
    "\n",
    "# Import reference files via Import > Well References\n",
    "# Import well headers via Import > Headers.\n",
    "\n",
    "# Note that, while the well header data imports very quickly, IC is a bit slow to create the wells if they don't already exist. Patience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Core (Core Interval)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_core = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_core&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=165.225.81.99&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_core = pd.read_excel('input data/wellbore_core.xlsx')\n",
    "    \n",
    "df_core.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core = df_core.replace(0, np.nan)\n",
    "df_core.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (df_core['Core sample - top depth'].isnull()) | (df_core['Core sample -  bottom depth'].isnull()) | (df_core['Core sample depth - uom'].isnull())\n",
    "\n",
    "# (df_core['Core sample - top depth'] == 0.0) | (df_core['Core sample -  bottom depth'] == 0.0) |\n",
    "\n",
    "df_core[filt].count()\n",
    "df_core[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core = df_core[~filt]\n",
    "df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_core.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core['Core sample depth - uom'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core.iterrows():\n",
    "    if row['Core sample depth - uom'] == '[ft  ]':\n",
    "        df_core.loc[index, 'Top depth'] = (row['Core sample - top depth'] * 0.3048)\n",
    "    else:\n",
    "        df_core.loc[index, 'Top depth'] = row['Core sample - top depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core.iterrows():\n",
    "    if row['Core sample depth - uom'] == '[ft  ]':\n",
    "        df_core.loc[index, 'Base depth'] = (row['Core sample -  bottom depth'] * 0.3048)\n",
    "    else:\n",
    "        df_core.loc[index, 'Base depth'] = row['Core sample -  bottom depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core.dtypes\n",
    "# Note extra space in 'Core sample -  bottom depth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core = df_core[['Wellbore', 'NPDID wellbore', 'Top depth', 'Base depth', 'Core sample number', ]].round(2)\n",
    "df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Core sample number': 'Legend'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_core.rename(columns=rename_cols, inplace=True)\n",
    "df_core.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_core', df=df_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Core Photos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs three files:\n",
    "    # wellbore_core_photo_ERRONEOUS_withURL.csv (erroneous 'Core photo title' columns)\n",
    "    # wellbore_core_photo_withURL.csv\n",
    "    # wellbore_core_photo.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_core_photo = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_core_photo&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.129.189&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_core_photo = pd.read_excel('input data/wellbore_core_photo.xlsx')\n",
    "\n",
    "df_core_photo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo = df_core_photo.drop('Date updated', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://pythex.org/\n",
    "\n",
    "# Match pattern:\n",
    "# 10208-10228ft\n",
    "# 1802-1805m\n",
    "\n",
    "pat = '\\d{3,5}-\\d{3,5}\\D{1,2}'\n",
    "    \n",
    "#filt = df_core_photo['Core photo title'].str.extract(pat)\n",
    "filt = df_core_photo['Core photo title'].str.contains(pat)\n",
    "\n",
    "# Check rows that match pattern\n",
    "df_core_photo[filt].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows that do not match pattern and make corrections\n",
    "df_core_photo[~filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note 95 rows with erroneous values\n",
    "# Apply obvious corrections then drop the rest.\n",
    "\n",
    "# Values for well 2/4-X-47 are obviously in ft.\n",
    "filt_correction = df_core_photo['Wellbore'] == '2/4-X-47'\n",
    "df_core_photo.loc[filt_correction, 'Core photo title'] = (df_core_photo['Core photo title'] + 'm')\n",
    "df_core_photo.loc[filt_correction]\n",
    "\n",
    "# There are other obvious corrections to be made, but leave for now.\n",
    "# Example below, but don't do this on .loc as index may as more wells added.\n",
    "\n",
    "#['Core photo title'].loc[14234] = '2482-2483m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign rows that do not match pattern to new dataframe\n",
    "\n",
    "df_core_photo_ERRONEOUS = df_core_photo[~filt]\n",
    "df_core_photo_ERRONEOUS\n",
    "\n",
    "# Output data\n",
    "output_to_csv(outname='wellbore_core_photo_ERRONEOUS_withURL', df=df_core_photo_ERRONEOUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows that do match pattern\n",
    "# Dumps the rest (e.g. '2044', 'Core 2')\n",
    "\n",
    "df_core_photo = df_core_photo[filt]\n",
    "print(df_core_photo.shape)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_core_photo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "df_core_photo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo['Core photo title'].replace({'mj': 'm', #one erroneous 'mj' value\n",
    "                                           'n': ',m', #one erroneous 'n' value\n",
    "                                           'm': ',m', #then replace all 'm'\n",
    "                                           'M': ',m',\n",
    "                                           'ft': ',ft',\n",
    "                                           'FT': ',ft'\n",
    "                                          }, regex=True, inplace=True)\n",
    "\n",
    "df_core_photo['Core photo title'].replace({'-': ','}, regex=True, inplace=True)\n",
    "\n",
    "df_core_photo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo[['Top depth', 'Base depth', 'Unit']] = df_core_photo['Core photo title'].str.split(pat=',', n=2, expand=True)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo['Unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that contain nulls\n",
    "df_core_photo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all top depths to metres\n",
    "\n",
    "for index, row in df_core_photo.iterrows():\n",
    "    if row['Unit'] == 'ft':\n",
    "        df_core_photo.loc[index, 'Top depth'] = int(row['Top depth']) * 0.3048\n",
    "    else:\n",
    "        df_core_photo.loc[index, 'Top depth'] = int(row['Top depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all base depths to metres\n",
    "\n",
    "for index, row in df_core_photo.iterrows():\n",
    "    if row['Unit'] == 'ft':\n",
    "        df_core_photo.loc[index, 'Base depth'] = int(row['Base depth']) * 0.3048\n",
    "    else:\n",
    "        df_core_photo.loc[index, 'Base depth'] = int(row['Base depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.round(2).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the filepath, e.g. '.\\core_photo_jpgs\\9_3-1\\921_01_1798-1802m.jpg'\n",
    "# Where . represents the current directory\n",
    "\n",
    "df_core_photo['Folder'] = df_core_photo['Wellbore'].str.replace('/', '_')\n",
    "df_core_photo['Legend'] = '.\\\\' + 'core_photo_jpgs\\\\' + df_core_photo['Folder'] + '\\\\'+ df_core_photo['Core photo URL'].str.split('/').str[-1]\n",
    "\n",
    "df_core_photo\n",
    "\n",
    "# If you only want file name use:\n",
    "# df_core_photo['Legend'] = '.\\\\' + df_core_photo['Core photo URL'].str.split('/').str[-1]\n",
    "# df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well'}\n",
    "\n",
    "# Apply renaming\n",
    "df_core_photo.rename(columns=rename_cols, inplace=True)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file that includes URLs before going on to generate file for IC\n",
    "output_to_csv(outname='wellbore_core_photo_withURL', df=df_core_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo = df_core_photo[['Well', 'NPDID wellbore', 'Top depth', 'Base depth', 'Legend']]\n",
    "df_core_photo.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_core_photo', df=df_core_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thin Section</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_thin_section = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_thin_section&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_thin_section = pd.read_excel('input data/wellbore_thin_section.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_thin_section.shape)\n",
    "df_thin_section.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section['Unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_thin_section.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_thin_section.iterrows():\n",
    "    if row['Unit'] == '[ft  ]':\n",
    "        df_thin_section.loc[index, 'Depth'] = row['Depth'] * 0.3048\n",
    "    else:\n",
    "        df_thin_section.loc[index, 'Depth'] = row['Depth']\n",
    "        \n",
    "df_thin_section.drop(columns='Unit', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_thin_section.iterrows():\n",
    "    df_thin_section.loc[index, 'Legend'] = 'Thin section no. ' + str(row['Number'])\n",
    "    \n",
    "df_thin_section.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well'}\n",
    "    \n",
    "# Apply renaming\n",
    "df_thin_section.rename(columns=rename_cols, inplace=True)\n",
    "df_thin_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section = df_thin_section[['Well', 'NPDID wellbore', 'Depth', 'Legend']]\n",
    "df_thin_section = df_thin_section.round(2)\n",
    "df_thin_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_thin_section', df=df_thin_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point - comment\n",
    "# No new IC data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CO2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_co2 = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_co2&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en',\n",
    "                     skiprows=[0])\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_co2 = pd.read_excel('input data/wellbore_co2.xlsx', sheet_name='wellbore_co2', \n",
    "                           skiprows=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_co2.shape)\n",
    "df_co2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_co2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.drop(labels=['Sample method', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore name' : 'Well',\n",
    "               'Sample top depth [m]' : 'Top depth',\n",
    "               'Sample bottom depth [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_co2.rename(columns=rename_cols, inplace=True)\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_co2', df=df_co2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Oil Samples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_oil_sample = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_oil_sample&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_oil_sample = pd.read_excel('input data/wellbore_oil_sample.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_oil_sample.shape)\n",
    "df_oil_sample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_oil_sample.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.drop(labels=['Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Top depth MD [m]' : 'Top depth',\n",
    "               'Bottom depth MD [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_oil_sample.rename(columns=rename_cols, inplace=True)\n",
    "df_oil_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_oil_sample', df=df_oil_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.columns\n",
    "\n",
    "# What to do about rows with only with 0/Nan values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lithostratigraphy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lithostrat available in two places. \n",
    "# Compare the length, and number of unique wells in both sources.\n",
    "\n",
    "# (A) NPD FactPages > Wellbore > Table View > With > Lithostratigraphy\n",
    "    # File: wellbore_formation_top.xlsx\n",
    "    # Sheet: wellbore_formation_top\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_a = pd.read_excel('input data/wellbore_formation_top.xlsx', sheet_name='wellbore_formation_top')\n",
    "print('Source A:', df_a.shape)\n",
    "print(df_a['Wellbore name'].nunique())\n",
    "\n",
    "# (B) NPD FactPages > Stratigraphy > Table View > Wellbores\n",
    "    # File: strat_litho_wellbore.xlsx\n",
    "    # Sheet: strat_litho_wellbore\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/strat_litho_wellbore&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_b = pd.read_excel('input data/strat_litho_wellbore.xlsx', sheet_name='strat_litho_wellbore')\n",
    "print('Source B:', df_b.shape)\n",
    "print(df_b['Wellbore name'].nunique())\n",
    "\n",
    "# Both contain the same number of rows.\n",
    "# Source A is preferrable as it has an exra column, 'Lithostrat. unit, parent'\n",
    "# which will come in handy assigning parents to each text dictionary entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Source A\n",
    "\n",
    "if data_source == 'web':\n",
    "    df_lithostrat = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "    \n",
    "if data_source == 'file':\n",
    "    df_lithostrat = pd.read_excel('input data/wellbore_formation_top.xlsx')\n",
    "\n",
    "# Print column titles\n",
    "print(\"Lithostratigraphy wellbore well header column titles:\")\n",
    "print(list(df_lithostrat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_lithostrat_rows, num_lithostrat_cols) = df_lithostrat.shape\n",
    "print('{} rows and {} columns in Exploration wells.'.format(num_lithostrat_rows, num_lithostrat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for csv\n",
    "rename_stratcols = {'Wellbore name' : 'Well',\n",
    "                    'Top depth [m]' : 'Top depth',\n",
    "                    'Bottom depth [m]' : 'Base depth',\n",
    "                    'Lithostrat. unit' : 'Legend'\n",
    "                    }\n",
    "\n",
    "#Apply renaming to dataframe\n",
    "df_lithostrat.rename(columns=rename_stratcols, inplace=True)\n",
    "\n",
    "# Create new dataframe called \"df_formation_top\"\n",
    "# Need to keep other columns df_lithostrat for later when writing to database\n",
    "\n",
    "df_formation_top = df_lithostrat[['Well', 'Top depth', 'Base depth', 'Legend']]\n",
    "df_formation_top.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_formation_top', df=df_formation_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Drill stem tests</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_dst = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_dst&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_dst = pd.read_excel('input data/wellbore_dst.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_dst.shape)\n",
    "df_dst.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dst.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dst.drop(labels=['Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Rename well header columns to match dbo.WELLS (does capitalisation matter?)\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'From depth MD [m]' : 'Top depth',\n",
    "               'To depth MD [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "#Apply renaming\n",
    "df_dst.rename(columns=rename_cols, inplace=True)\n",
    "df_dst.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_dst', df=df_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Casing and leak-off tests</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_casinglot = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_casing_and_lot&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "if data_source == 'file':\n",
    "    df_casinglot = pd.read_excel('input data/wellbore_casing_and_lot.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_casinglot.shape)\n",
    "df_casinglot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_casinglot.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casinglot.drop(labels=['Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casinglot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Casing depth [m]' : 'Depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_casinglot.rename(columns=rename_cols, inplace=True)\n",
    "df_casinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_casing_and_lot', df=df_casinglot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Drilling mud</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_mud = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_mud&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "    \n",
    "if data_source == 'file':\n",
    "    df_mud = pd.read_excel('input data/wellbore_mud.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_mud.shape)\n",
    "df_mud.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop blank first column\n",
    "df_mud.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "df_mud.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_mud.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mud.drop(labels=['Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mud.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Depth MD [m]' : 'Depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_mud.rename(columns=rename_cols, inplace=True)\n",
    "df_mud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_mud', df=df_mud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Documents</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'web':\n",
    "    df_document = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_document&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.188&CultureCode=en')\n",
    "    \n",
    "if data_source == 'file':\n",
    "    df_document = pd.read_excel('input data/wellbore_document.xlsx')\n",
    "\n",
    "df_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document['Title'] = df_document['Wellbore'] + ' ' + df_document['Document type'] + ': ' + df_document['Document name'] + ' (' + df_document['Document format'] + ')'\n",
    "df_document[['Wellbore', 'Title']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document = df_document[['Wellbore', 'NPDID wellbore', 'Title', 'Document URL']]\n",
    "df_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_document', df=df_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References and Documents combined</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in both df_document and df_explo_references before merge\n",
    "\n",
    "# Rename columns in df_document\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Document URL' : 'URL'}\n",
    "    \n",
    "#Apply renaming\n",
    "df_document.rename(columns=rename_cols, inplace=True)\n",
    "df_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_explo_references\n",
    "rename_cols = {'Name' : 'Well'}\n",
    "    \n",
    "#Apply renaming\n",
    "df_explo_references.rename(columns=rename_cols, inplace=True)\n",
    "df_explo_references.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine References and Documents dataframes\n",
    "df_refs_and_docs = df_explo_references.append(df_document) \n",
    "df_refs_and_docs.sort_values(['Well', 'Title'], ascending=[True, False], ignore_index=True, inplace=True)\n",
    "df_refs_and_docs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_to_csv(outname='wellbore_references_and_documents', df=df_refs_and_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame({'Data type':\n",
    "                        ['Exploration well header',\n",
    "                         'Development well header',\n",
    "                         'Exploration reference',\n",
    "                         'Development reference',\n",
    "                         'Core',\n",
    "                         'Core photo',\n",
    "                         'Thin section',\n",
    "                         'CO2',\n",
    "                         'Oil sample',\n",
    "                         'Lithostratigraphy',\n",
    "                         'Drill stem test',\n",
    "                         'Casing and leak-off test',\n",
    "                         'Drilling mud',\n",
    "                         'Document',\n",
    "                         'Document & Reference combined'\n",
    "                        ], \n",
    "                        'No. unique wells':\n",
    "                        [df_explo['Name'].nunique(),\n",
    "                         df_dev['Name'].nunique(),\n",
    "                         df_explo_references['Well'].nunique(),\n",
    "                         df_dev_references['Name'].nunique(),\n",
    "                         df_core['Well'].nunique(),\n",
    "                         df_core_photo['Well'].nunique(),\n",
    "                         df_thin_section['Well'].nunique(),\n",
    "                         df_co2['Well'].nunique(),\n",
    "                         df_oil_sample['Well'].nunique(),\n",
    "                         df_formation_top['Well'].nunique(),\n",
    "                         df_dst['Well'].nunique(),\n",
    "                         df_casinglot['Well'].nunique(),\n",
    "                         df_mud['Well'].nunique(),\n",
    "                         df_document['Well'].nunique(),\n",
    "                         df_refs_and_docs['Well'].nunique()\n",
    "                        ],\n",
    "                        'No. records':\n",
    "                        [df_explo['Name'].shape[0],\n",
    "                         df_dev['Name'].shape[0],\n",
    "                         df_explo_references['Well'].shape[0],\n",
    "                         df_dev_references['Name'].shape[0],\n",
    "                         df_core['Well'].shape[0],\n",
    "                         df_core_photo['Well'].shape[0],\n",
    "                         df_thin_section['Well'].shape[0],\n",
    "                         df_co2['Well'].shape[0],\n",
    "                         df_oil_sample['Well'].shape[0],\n",
    "                         df_formation_top['Well'].shape[0],\n",
    "                         df_dst['Well'].shape[0],\n",
    "                         df_casinglot['Well'].shape[0],\n",
    "                         df_mud['Well'].shape[0],\n",
    "                         df_document['Well'].shape[0],\n",
    "                         df_refs_and_docs['Well'].shape[0]]\n",
    "                       })\n",
    "\n",
    "print('Data for', (df_explo['Name'].nunique()+df_dev['Name'].nunique()), 'wells in total.')\n",
    "\n",
    "#Add thousands separators\n",
    "df_summary['No. unique wells'] = df_summary['No. unique wells'].apply(lambda x : \"{:,}\".format(x))\n",
    "df_summary['No. records'] = df_summary['No. records'].apply(lambda x : \"{:,}\".format(x))\n",
    "df_summary = df_summary.style.hide_index()\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break me here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data to SQL Server database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = 'DRIVER={ODBC Driver 13 for SQL Server};' \\\n",
    "         'SERVER=5SQFPQ2\\SQLEXPRESS;' \\\n",
    "         'PORT=1433;' \\\n",
    "         'DATABASE=Test51;' \\\n",
    "         'Trusted_Connection=yes;'\n",
    "            \n",
    "params = urllib.parse.quote_plus(params)\n",
    "\n",
    "engine = create_engine('mssql+pyodbc:///?odbc_connect=%s' % params, echo = True)\n",
    "\n",
    "metadata = MetaData()\n",
    "  \n",
    "connection = engine.connect()\n",
    "                       \n",
    "#pp.pprint(repr(wells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current time for use in Created and Modified columns\n",
    "\n",
    "now = datetime.now()\n",
    "now\n",
    "\n",
    "timestampStr = now.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "print('Current Timestamp : ', timestampStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'now' timestamp to OLE date/variant date\n",
    "\n",
    "def datetime2ole(date):\n",
    "    \n",
    "    #convert date string to a datetime object\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    #Calculate OLE manually from OLE origin date\n",
    "    OLE_TIME_ZERO = datetime(1899, 12, 30)\n",
    "    delta = date - OLE_TIME_ZERO\n",
    "    \n",
    "    return float(delta.days) + (float(delta.seconds) / 86400)  # 86,400 seconds in day\n",
    "\n",
    "now = datetime2ole(timestampStr)\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new Tables in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new table in database for each item is list\n",
    "\n",
    "new_table_names = ['DATA_Core_NPD',\n",
    "                   'DATA_Petrography_NPD',\n",
    "                   'DATA_CO2_NPD',\n",
    "                   'DATA_OilSample_NPD',\n",
    "                   'DATA_Lithostrat_NPD',\n",
    "                   'DATA_DST_NPD',\n",
    "                   'DATA_CasingLOT_NPD',\n",
    "                   'DATA_DrillingMud_NPD',\n",
    "                   'DATA_Tops_NPD']\n",
    "\n",
    "for new_table_name in new_table_names:\n",
    "    \n",
    "    # Template for 'DATA_' tables in IC\n",
    "    \n",
    "    students = Table(\n",
    "        new_table_name, metadata,\n",
    "        Column('pk_index', Integer, primary_key=True, nullable=False), # [pk_index] [int] IDENTITY(1,1) NOT NULL,\n",
    "        Column('data_type', Integer),\n",
    "        Column('top_depth', Float),\n",
    "        Column('base_depth', Float),\n",
    "        Column('top_boundary', SmallInteger),\n",
    "        Column('base_boundary', SmallInteger),\n",
    "        Column('symbol_id', Integer),\n",
    "        Column('legend', NVARCHAR),\n",
    "        Column('interpreter', NVARCHAR(6)),\n",
    "        Column('created', Float),\n",
    "        Column('modified', Float),\n",
    "        Column('obsno', Integer),\n",
    "        Column('mindepth', Float),\n",
    "        Column('maxdepth', Float),\n",
    "        Column('remark', NVARCHAR(80)),\n",
    "        Column('geofeature', NVARCHAR(40)),\n",
    "        Column('dipangle', Float),\n",
    "        Column('dipazimuth', Float),\n",
    "        Column('age', Float),\n",
    "        Column('owconf', NVARCHAR(2)),\n",
    "        Column('owqual', NVARCHAR(4)),\n",
    "        Column('owkind', NVARCHAR(42)),\n",
    "        Column('owbaseconf', NVARCHAR(2)),\n",
    "        Column('owbasequal', NVARCHAR(4)),\n",
    "        Column('owbasekind', NVARCHAR(42)),\n",
    "        Column('top_age', Float),\n",
    "        Column('base_age', Float),\n",
    "        Column('f_interpid', Integer),\n",
    "        Column('creator', NVARCHAR(64)),\n",
    "        Column('modifier', NVARCHAR(64)),\n",
    "        Column('abr', NVARCHAR(80)),\n",
    "        Column('source', NVARCHAR(255)),\n",
    "        Column('attr', NVARCHAR),\n",
    "        Column('well_id', Integer),\n",
    "              )\n",
    "    \n",
    "    metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify dbo.tablenames (insert IC Data Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "INSERT INTO dbo.tablenames \n",
    "(tabletype, tablename, tabledescription, hascores, fileprefix, interp, samplebased, f_tableid)\n",
    "VALUES \n",
    "('I', 'DATA_Core_NPD', 'Core (NPD)', 'False', 'A', '2', '0', '900'),\n",
    "('I', 'DATA_Petrography_NPD', 'Petrography (NPD)', 'False', 'A', '2', '0', '901'),\n",
    "('I', 'DATA_CO2_NPD', 'CO2 Content (NPD)', 'False', 'A', '2', '1', '902'),\n",
    "('I', 'DATA_OilSample_NPD', 'Oil Sample (NPD)', 'False', 'A', '2', '1', '903'),\n",
    "('I', 'DATA_Lithostrat_NPD', 'Lithostratigraphy (NPD)', 'False', 'A', '2', '0', '904'),\n",
    "('I', 'DATA_DST_NPD', 'Drill Stem Test (NPD)', 'False', 'A', '2', '1', '905'),\n",
    "('I', 'DATA_CasingLOT_NPD', 'Casing and LOT (NPD)', 'False', 'A', '2', '1', '906'),\n",
    "('I', 'DATA_DrillingMud_NPD', 'Drilling Mud (NPD)', 'False', 'A', '2', '1', '907'),\n",
    "('I', 'DATA_Tops_NPD', 'Tops (NPD)', 'False', 'A', '2', '0', '908');\n",
    "\n",
    "'''\n",
    "#pd.read_sql_query(sql, engine)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each IC Data Table is assigned a number, as above\n",
    "# Assign these to variables for use elsewhere\n",
    "# Used for tablenames.f_tableid and INTERVALCOLUMNS.dest_table\n",
    "\n",
    "core_npd_tablenum = 900\n",
    "petrography_tablenum = 901\n",
    "co2_content_tablenum = 902\n",
    "oil_sample_tablenum = 903\n",
    "lithostrat_tablenum = 904\n",
    "dst_tablenum = 905\n",
    "casinglot_tablenum = 906\n",
    "drillingmud_tablenum = 907\n",
    "tops_tablenum = 908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create new IC Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of default intervalcolumn values\n",
    "# Create list of valid IC data \"styles\"\n",
    "# Function intervalcolumns_variations to create new data types\n",
    "\n",
    "intervalcolumns_def = {'data_type': 0, #int\n",
    "                        'dest_table': 0, #int\n",
    "                        'description': 'description', #nvarchar(80)\n",
    "                        'title': 'title', #nvarchar(80)\n",
    "                        'graphic': 0, #bit\n",
    "                        'zonal': 0, #bit\n",
    "                        'boundaries': 0, #bit\n",
    "                        'legend': 0, #bit\n",
    "                        'dic_driven': 0, #bit\n",
    "                        'col_width': 15, #real\n",
    "                        'back_colour': '1;12648447;255', #nvarchar(50)\n",
    "                        'brush_style': 0, #smallint\n",
    "                        'hatch_style': 0, #smallint\n",
    "                        'int_colour': '8;12648447;8454143', #nvarchar(50)\n",
    "                        'row_height': 15, #smallint\n",
    "                        'table_header': 'Legend', #nvarchar(50)\n",
    "                        'horiz_justify': 1, #int\n",
    "                        'vert_justify': 1, #smallint\n",
    "                        'orientation': 0, #smallint\n",
    "                        'fontname': 'Arial', #nvarchar(50)\n",
    "                        'fontsize': 8, #smallint\n",
    "                        'fontweight': 0, #smallint\n",
    "                        'fontitalic': 0, #smallint\n",
    "                        'fontunderline': 0, #smallint\n",
    "                        'fontcolour': 0, #int\n",
    "                        'plotwith': 0, #nvarchar(20)\n",
    "                        'style': 'I', #nvarchar(50)\n",
    "                        'maximum': 100, #real\n",
    "                        'minimum': 0, #real\n",
    "                        'units': '_', #nvarchar(10)\n",
    "                        'horiz_grid': 0, #smallint\n",
    "                        'chartstyle': 0, #smallint\n",
    "                        'plotsymbol': 0, #int\n",
    "                        'labelpoints': 0, #smallint\n",
    "                        'mergeevents': 0, #smallint\n",
    "                        'eventalign': 0, #smallint\n",
    "                        'sbugs_igdtype': 0, #int\n",
    "                        'sbugs_igdplotpos': 0, #int\n",
    "                        'allowoverlap': 0, #smallint\n",
    "                        'alttable': 'NULL', #nvarchar(30)\n",
    "                        'altfield': 'NULL', #nvarchar(30)\n",
    "                        'plotassequence': 0, #smallint\n",
    "                        'owinterp': 'NULL', #nvarchar(30)\n",
    "                        'ordering': 0, #smallint\n",
    "                        'wellcore': 0, #smallint\n",
    "                        'surface': 0, #smallint\n",
    "                        'f_order': 0, #smallint\n",
    "                        'isdepthage': 0, #smallint\n",
    "                        'f_style': 0, #int\n",
    "                        'f_coreshiftid': 0 #int\n",
    "                         }\n",
    "\n",
    "#intervalcolumns_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_styles = ['interval_text-boundaries',\n",
    "                'interval_text-noboundaries',\n",
    "                'interval_notext-boundaries',\n",
    "                'interval_notext-noboundaries',\n",
    "                'interval_graphicfill-noboundaries',\n",
    "                'interval_externalimagefile',\n",
    "                'interval_value',\n",
    "                'interval_comment',\n",
    "                'interval_text-systemtract',\n",
    "                'point_pick-noboundary',\n",
    "                'point_pick-sequenceboundary',\n",
    "                'point_pick-boundary',\n",
    "                'point_comment',\n",
    "                'point_value',\n",
    "                'point_depth-agemapping',\n",
    "                'point_occurrenceevent',\n",
    "                'pointsample',\n",
    "                'point_symboltext',\n",
    "                'graphicqualifier',\n",
    "                'welltest-perforation',\n",
    "                'wellcore'\n",
    "               ]\n",
    "\n",
    "#valid_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build IC Data Types\n",
    "\n",
    "# Known knowns:\n",
    "# I know what Data Table I want to use (desttable)\n",
    "# And need to provide a unique datatype\n",
    "# I know what I want to name my Data Type (description and title)\n",
    "# With a known style (e.g. interval_value)\n",
    "# So pass in desttable, datatype, title and style.\n",
    "\n",
    "\n",
    "def intervalcolumns_var(desttable, datatype, title, style):\n",
    "    \n",
    "    # Start with default values for dbo.INTERVALCOLUMNS\n",
    "    #intervalcolumns_dict = copy.deepcopy(intervalcolumns_def)\n",
    "    intervalcolumns_dict = intervalcolumns_def.copy()\n",
    "    #intervalcolumns_dict = intervalcolumns_def\n",
    "    \n",
    "    # Update with values passed into function\n",
    "    intervalcolumns_dict.update({'data_type': datatype, 'dest_table': desttable, 'description': title, 'title': title})\n",
    "    \n",
    "    # Then update other values based on the style of IC Data Type\n",
    "    \n",
    "    if style == 'interval_text-boundaries':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'boundaries': 1, 'legend': 1})\n",
    "        \n",
    "    elif style == 'interval_text-noboundaries':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'legend': 1})\n",
    "\n",
    "    elif style == 'interval_notext-boundaries': \n",
    "        intervalcolumns_dict.update({'zonal': 1, 'boundaries': 1})\n",
    "        \n",
    "    elif style == 'interval_notext-noboundaries':\n",
    "        intervalcolumns_dict.update({'zonal': 1})\n",
    "        \n",
    "    elif style == 'interval_graphicfill-noboundaries':\n",
    "        intervalcolumns_dict.update({'graphic': 1, 'zonal': 1, 'dic_driven': 1})\n",
    "        \n",
    "    elif style == 'interval_externalimagefile':\n",
    "        intervalcolumns_dict.update({'graphic': 1, 'zonal': 1, 'style': 'IMAGEFILE'})\n",
    "        \n",
    "    elif style == 'interval_value':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'style': 'H', 'maximum': -2})\n",
    "\n",
    "    elif style == 'interval_comment':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'legend': 1, 'row_height': 45, 'style': 'C'})\n",
    "        \n",
    "    elif style == 'interval_text-systemtract':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'legend': 1, 'plot_as_sequence': 1})\n",
    "        \n",
    "    elif style == 'point_pick-noboundary':\n",
    "        intervalcolumns_dict.update({'legend': 1, 'int_colour': '1;12648447;255'})\n",
    "        \n",
    "    elif style == 'point_pick-sequenceboundary':\n",
    "        intervalcolumns_dict.update({'boundaries': 1, 'legend': 1, 'plot_as_sequence': 1})\n",
    "        \n",
    "    elif style == 'point_pick-boundary':\n",
    "        intervalcolumns_dict.update({'boundaries': 1, 'legend': 1, 'int_colour': '1;12648447;255'})\n",
    "\n",
    "    elif style == 'point_comment':\n",
    "        intervalcolumns_dict.update({'legend': 1, 'int_colour': '1;12648447;255', 'row_height': 45, 'style': 'C'})\n",
    "\n",
    "    elif style == 'point_value':\n",
    "        intervalcolumns_dict.update({'int_colour': '1;12648447;255', 'style': 'H', 'f_style': 189})\n",
    "        \n",
    "    elif style == 'point_depth-agemapping':\n",
    "        intervalcolumns_dict.update({'int_colour': '1;12648447;255', 'style': 'H', 'f_style': 190})\n",
    "        \n",
    "    elif style == 'point_occurrenceevent':\n",
    "        intervalcolumns_dict.update({'legend': 1, 'dic_driven': 1, 'int_colour': '1;12648447;255', 'table_header': 'Event', 'style': 'E'})\n",
    "        \n",
    "    elif style == 'pointsample':\n",
    "        intervalcolumns_dict.update({'graphic': 1, 'legend': 1, 'dic_driven': 1, 'int_colour': '1;12648447;255', 'style': 'SA'})\n",
    "        \n",
    "    elif style == 'point_symboltext':\n",
    "        intervalcolumns_dict.update({'graphic': 1, 'legend': 1, 'int_colour': '1;12648447;255', 'style': 'SC'})\n",
    "        \n",
    "    elif style == 'graphicqualifier':\n",
    "        intervalcolumns_dict.update({'graphic': 1, 'dic_driven': 1, 'int_colour': '1;12648447;255', 'style': 'S'})\n",
    "        \n",
    "    elif style == 'welltest-perforation':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'legend': 1, 'style': 'ENGDATA'})\n",
    "        \n",
    "    elif style == 'wellcore':\n",
    "        intervalcolumns_dict.update({'zonal': 1, 'legend': 1, 'wellcore': 1})\n",
    "        \n",
    "    else:\n",
    "        print('Invalid style')\n",
    "    \n",
    "    lst = list(intervalcolumns_dict.values())\n",
    "    return lst\n",
    "\n",
    "    # Clear the dictionary after each execution so clean copy is made of defaults\n",
    "    intervalcolumns_dict.clear()\n",
    "    \n",
    "    #print(intervalcolumns_dict)\n",
    "\n",
    "cols = list(intervalcolumns_def)\n",
    "cols_set = (', '.join(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datatype numbers using next() function\n",
    "dtypenums = list(range(9000, 9050, 1))\n",
    "dtypenums_iter = iter(dtypenums)\n",
    "\n",
    "# Cores (NPD) > Core Interval (NPD) and Core Photo (NPD)\n",
    "vals_0 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = core_npd_tablenum, title = 'Core Interval (NPD)', style = 'wellcore')\n",
    "vals_1 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = core_npd_tablenum, title = 'Core Photo (NPD)', style = 'interval_externalimagefile')\n",
    "\n",
    "# Petrography (NPD)\n",
    "vals_2 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = petrography_tablenum, title = 'Thin Section (NPD)', style = 'point_comment')\n",
    "\n",
    "# CO2 (NPD) - interval data\n",
    "vals_3 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = co2_content_tablenum, title = 'Sample sequence number (NPD)', style = 'interval_value')\n",
    "vals_4 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = co2_content_tablenum, title = 'CO2 [vol %] (NPD)', style = 'interval_value')\n",
    "vals_5 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = co2_content_tablenum, title = 'Sample type (NPD)', style = 'interval_comment')\n",
    "\n",
    "# Oil Sample (sample data, intervals)\n",
    "vals_6 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = oil_sample_tablenum, title = 'Test type (NPD)', style = 'interval_comment')\n",
    "vals_7 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = oil_sample_tablenum, title = 'Bottle number (NPD)', style = 'interval_comment')\n",
    "vals_8 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = oil_sample_tablenum, title = 'Fluid type (NPD)', style = 'interval_comment')\n",
    "vals_9 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = oil_sample_tablenum, title = 'Test time (NPD)', style = 'interval_comment')\n",
    "vals_10 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = oil_sample_tablenum, title = 'Received date (NPD)', style = 'interval_comment')\n",
    "\n",
    "# Lithostratigraphy (NPD)\n",
    "vals_11 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = lithostrat_tablenum , title = 'Group (NPD)', style = 'interval_text-noboundaries')\n",
    "vals_12 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = lithostrat_tablenum , title = 'Formation (NPD)', style = 'interval_text-noboundaries')\n",
    "vals_13 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = lithostrat_tablenum , title = 'Member (NPD)', style = 'interval_text-noboundaries')\n",
    "\n",
    "# Drill Stem Test (NPD) -sample data, intervals <--- note existing table \"Drill Stem Tests - DST\"\n",
    "vals_14 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Test number (NPD)', style = 'interval_value')\n",
    "vals_15 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Choke size [mm] (NPD)', style = 'interval_value')\n",
    "vals_16 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Final shut-in pressure [MPa] (NPD)', style = 'interval_value')\n",
    "vals_17 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Final flow pressure [MPa] (NPD)', style = 'interval_value')\n",
    "vals_18 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Bottom hole pressure [MPa] (NPD)', style = 'interval_value')\n",
    "vals_19 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Oil [Sm3/day] (NPD)', style = 'interval_value')\n",
    "vals_20 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Gas [Sm3/day] (NPD)', style = 'interval_value')\n",
    "vals_21 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Oil density [g/cm3] (NPD)', style = 'interval_value')\n",
    "vals_22 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Gas grav. rel.air (NPD)', style = 'interval_value')\n",
    "vals_23 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'GOR [m3/m3] (NPD)', style = 'interval_value')\n",
    "vals_24 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = dst_tablenum, title = 'Downhole temperature [°C] (NPD)', style = 'interval_value')\n",
    "\n",
    "# casing/lot (sample data, points) <--- note existing table \"Casing\"\n",
    "vals_25 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'Casing type (NPD)', style = 'point_comment')\n",
    "vals_26 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'Casing diam. [inch] (NPD)', style = 'point_value')\n",
    "vals_27 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'Hole diam. [inch] (NPD)', style = 'point_value')\n",
    "vals_28 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'Hole depth[m] (NPD)', style = 'point_value')\n",
    "vals_29 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'LOT mud eqv. [g/cm3] (NPD)', style = 'point_value')\n",
    "vals_30 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = casinglot_tablenum, title = 'Formation test type (NPD)', style = 'point_comment')\n",
    "\n",
    "# Drilling Mud (sample data, points)\n",
    "vals_31 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = drillingmud_tablenum, title = 'Mud weight [g/cm3] (NPD)', style = 'point_value')\n",
    "vals_32 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = drillingmud_tablenum, title = 'Visc. [mPa.s] (NPD)', style = 'point_value')\n",
    "vals_33 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = drillingmud_tablenum, title = 'Yield point [Pa] (NPD)', style = 'point_value')\n",
    "vals_34 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = drillingmud_tablenum, title = 'Mud type (NPD)', style = 'point_comment')\n",
    "vals_35 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = drillingmud_tablenum, title = 'Date measured (NPD)', style = 'point_comment')\n",
    "\n",
    "# Tops (NPD)\n",
    "vals_36 = intervalcolumns_var(datatype = next(dtypenums_iter), desttable = tops_tablenum, title = 'Tops_ALL (NPD)', style = 'point_pick-noboundary')\n",
    "\n",
    "\n",
    "sql_stmt_icdatatypes = str('INSERT INTO dbo.INTERVALCOLUMNS ({}) VALUES {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {};'.format(cols_set, vals_0, vals_1, vals_2, vals_3, vals_4,\n",
    "                                                                                                                                                                                                                    vals_5, vals_6, vals_7, vals_8, vals_9, \n",
    "                                                                                                                                                                                                                    vals_10, vals_11, vals_12, vals_13, vals_14, \n",
    "                                                                                                                                                                                                                    vals_15, vals_16, vals_17, vals_18, vals_19, \n",
    "                                                                                                                                                                                                                    vals_20, vals_21, vals_22, vals_23, vals_24, \n",
    "                                                                                                                                                                                                                    vals_25, vals_26, vals_27, vals_28, vals_29, \n",
    "                                                                                                                                                                                                                    vals_30, vals_31, vals_32, vals_33, vals_34, \n",
    "                                                                                                                                                                                                                    vals_35, vals_36))\n",
    "\n",
    "sql_stmt_icdatatypes = sql_stmt_icdatatypes.replace('[', '(').replace(']', ')') # Hack for SQL formatting\n",
    "print(sql_stmt_icdatatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute sql statement to create new IC Data Types\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql_stmt_icdatatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review created IC Data Tables and Data Types\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "ic.dest_table, tn.tablename, tn.tabledescription, ic.data_type, ic.description\n",
    "FROM dbo.tablenames AS tn\n",
    "JOIN dbo.INTERVALCOLUMNS AS ic\n",
    "ON tn.f_tableid = ic.dest_table\n",
    "WHERE data_type >= 9000\n",
    "ORDER BY ic.data_type;\n",
    "'''\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQLAlchemy Table Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = Table('WELLS', metadata, autoload=True, autoload_with=engine)\n",
    "wellsuserfieldsvalues = Table('t_WellsUserFieldsValues', metadata, autoload=True, autoload_with=engine)\n",
    "wellsuserfields = Table('t_WellsUserFields', metadata, autoload=True, autoload_with=engine)\n",
    "projects = Table('PROJECTS', metadata, autoload=True, autoload_with=engine)\n",
    "wellqueries = Table('WELLQUERIES', metadata, autoload=True, autoload_with=engine)\n",
    "projectwells = Table('PROJECTWELLS', metadata, autoload=True, autoload_with=engine)\n",
    "tablesnames = Table('tablenames', metadata, autoload=True, autoload_with=engine)\n",
    "intervalcolumns = Table('INTERVALCOLUMNS', metadata, autoload=True, autoload_with=engine)\n",
    "datacore = Table('DATA_Core_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datapetrography = Table('DATA_Petrography_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "dataco2 = Table('DATA_CO2_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "dataoilsample = Table('DATA_OilSample_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datalithostrat = Table('DATA_Lithostrat_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datadrillstemtests = Table('DATA_DST_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datacasing = Table('DATA_CasingLOT_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datadrillingmud = Table('DATA_DrillingMud_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "datatops = Table('DATA_Tops_NPD', metadata, autoload=True, autoload_with=engine)\n",
    "references_nglinks = Table('NG_LINKS', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "# = Table('', metadata, autoload=True, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to select all rows from table\n",
    "\n",
    "def sqlselect_rows(tablename):\n",
    "    \n",
    "    select_stmt = select(tablename)\n",
    "    result = connection.execute(select_stmt)\n",
    "    \n",
    "    # Equivalent to:\n",
    "    # result = engine.execute('SELECT * FROM PROJECTS')\n",
    "\n",
    "    for row in result:\n",
    "        print(row)\n",
    "    \n",
    "    result.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate T_WELLQUERYFOLDERS and dbo.WELLQUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "INSERT INTO dbo.T_WELLQUERYFOLDERS \n",
    "(f_key, f_value)\n",
    "VALUES\n",
    "('FolderIdCounter', '1'),\n",
    "('Folder:1:Info', '{\"FolderName\":\"NPD FactPages data\"}'),\n",
    "('Folder:1:Parent', 'ProjectFolder(1)'),\n",
    "('Folder:ProjectFolder(1):SubFolders', '[\"1\"]');\n",
    "\n",
    "'''\n",
    "\n",
    "#pd.read_sql_query(sql, engine)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Well Queries to use with Dynamic Projects (creating queries first!)\n",
    "\n",
    "wellqueries_data = {'invertresults' : (['0'] * 13),\n",
    "                    'category' : ['ProjectFolder(1)', \n",
    "                                  'ProjectFolder(2)', \n",
    "                                  'ProjectFolder(3)', \n",
    "                                  'ProjectFolder(4)',\n",
    "                                  '1', # catgory 1 is folder \"NPD FactPages data\"\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1',\n",
    "                                  '1'], \n",
    "                    'query_id' : ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13'], \n",
    "                    'project_id' : (['-1'] * 13),\n",
    "                    'title' : ['Country = NORWAY', \n",
    "                               'Location = North Sea', \n",
    "                               'Location = Norwegian Sea', \n",
    "                               'Location = Barents Sea',\n",
    "                               'Core Intervals (NPD) has data logged',\n",
    "                               'Core Photos (NPD) has data logged',\n",
    "                               'Thin Sections (NPD) has data logged',\n",
    "                               'CO2 Content (NPD) has data logged',\n",
    "                               'Oil Samples (NPD) has data logged',\n",
    "                               'Lithostrat - Group (NPD) has data logged',\n",
    "                               'DST (NPD) has data logged',\n",
    "                               'Casing & LOT (NPD) has data logged',\n",
    "                               'Drilling Mud (NPD) has data logged'], \n",
    "                    'nentries' : (['1'] * 13),\n",
    "                    'pencolour' : (['0'] * 13),\n",
    "                    'enttype' : ['4', '4', '4', '4', '1', '1', '1', '1', '1', '1', '1', '1', '1'],\n",
    "                    'entdatatype' : ['0', '0', '0', '0', '9000', '9001', '9002', '9003', '9006', '9011', '9014', '9025', '9031'],\n",
    "                    'entfunction' : ['=', \n",
    "                                     '=', \n",
    "                                     '=', \n",
    "                                     '=', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged', \n",
    "                                     'Has data logged'],\n",
    "                    'entvalue' : ['NORWAY', \n",
    "                                  'NORTH SEA', \n",
    "                                  'NORWEGIAN SEA', \n",
    "                                  'BARENTS SEA',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  '',\n",
    "                                  ''], \n",
    "                    'entinfokey' : ['Country', \n",
    "                                    'Location', \n",
    "                                    'Location', \n",
    "                                    'Location',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    '',\n",
    "                                    ''], \n",
    "                    'highlightstyle' : (['1'] * 13),\n",
    "                    'highlightsymbol' : (['4198'] * 13)}\n",
    "\n",
    "wellqueries_data\n",
    "\n",
    "df_wellqueries = pd.DataFrame(wellqueries_data)\n",
    "\n",
    "df_wellqueries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "\n",
    "df_wellqueries.to_sql('WELLQUERIES', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.WELLQUERIES')\n",
    "sqlselect_rows([wellqueries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Populate PROJECTS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate dbo.PROJECTS with relevant fields to create 4 new dynamic projects:\n",
    "# ALL WELLS, NORWAY NORTH SEA, NORWAY NORWEGIAN SEA & NORWAY BARENTS SEA\n",
    "\n",
    "# Blank IC database has 1 default project, 'NEW PROJECT'\n",
    "# Rename this to 'NORWAY ALL WELLS' and set to dynamic project\n",
    "# Add code to check for existing project_id so as not to conflict?\n",
    "\n",
    "sql = '''\n",
    "UPDATE dbo.PROJECTS\n",
    "SET title = 'NORWAY ALL WELLS', Units = 'M', Map = 'NULL', datum = 4230, utmzone = 'ED50 / UTM zone 31N', TVD_datum = 'MSL', OWTranslation = 2, defchronodatatype = 0, deftwtdata = 'NULL', deffaultsdatatype = 0, defchronointerpid = 0, WellGroupField = 0, WellGroupFieldIsUserDefined = 0, WellOrderField = 0, WellOrderFieldIsUserDefined = 0, WellPatternTable = 'NULL', WellPatternTableLayerField = 'NULL', WellPatternTablePolygonField = 'NULL', DefaultSummaryCharts = '{}', DefaultWellstickTemplates = '{}', f_dynamic = 1, f_WellQueryId = 1\n",
    "WHERE project_id = 1;\n",
    "'''\n",
    "#pd.read_sql_query(sql, engine)\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 new dynamic projects \n",
    "\n",
    "dyprojects_data = {'project_id' : ['2', '3', '4'],\n",
    "                   'title' : ['NORWAY NORTH SEA', 'NORWAY NORWEGIAN SEA', 'NORWAY BARENTS SEA'],\n",
    "                   #'client' : ['', '', ''],\n",
    "                   #'jobno' : ['', '', ''],\n",
    "                   #'code' : ['', '', ''],\n",
    "                   #'notes' : ['', '', ''],\n",
    "                   'Units' : 'M',\n",
    "                   'Map' : ['NULL', 'NULL', 'NULL'],\n",
    "                   'datum' : ['4230', '4230', '4230'],\n",
    "                   'utmzone' : ['ED50 / UTM zone 31N', 'ED50 / UTM zone 32N', 'ED50 / UTM zone 34N'],\n",
    "                   'TVD_datum' : 'MSL',\n",
    "                   'OWTranslation' : '2', \n",
    "                   #'f_fieldname' : ['', '', '', ''],\n",
    "                   'defchronodatatype' : '0',\n",
    "                   #'deftstprops' : ['', '', '', ''],\n",
    "                   'deftwtdata' : 'NULL',\n",
    "                   'deffaultsdatatype' : '0',\n",
    "                   'defchronointerpid' : '0',\n",
    "                   'WellGroupField' : '0',\n",
    "                   'WellGroupFieldIsUserDefined' : '0',\n",
    "                   'WellOrderField' : '0',\n",
    "                   'WellOrderFieldIsUserDefined' : '0',\n",
    "                   'WellPatternTable' : 'NULL',\n",
    "                   'WellPatternTableLayerField' : 'NULL',\n",
    "                   'WellPatternTablePolygonField' : 'NULL',\n",
    "                   #'RPMWellTypeField' : ['', '', '', ''],\n",
    "                   #'DefaultRPMTemplates' : ['', '', '', ''],\n",
    "                   'DefaultSummaryCharts' : '{}',\n",
    "                   'DefaultWellstickTemplates' : '{}',\n",
    "                   'f_dynamic' : '1',\n",
    "                   'f_WellQueryId' : ['2', '3', '4']}\n",
    "\n",
    "# Create temp index, not sent to db\n",
    "df_dyprojects = pd.DataFrame(dyprojects_data, index = ['project_2', 'project_3', 'project_4'])\n",
    "df_dyprojects.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "\n",
    "df_dyprojects.to_sql('PROJECTS', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.PROJECTS')\n",
    "sqlselect_rows([projects])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Populate WELLS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename well header columns from NPD to match dbo.WELLS\n",
    "# All column titles in dbo.WELLS table\n",
    "\n",
    "rename_for_sql = {'Name' : 'name',\n",
    "                'Alternate 1' : 'name1',\n",
    "                'Operator' : 'client',\n",
    "                'Licence number' : 'f_licenceNumber',\n",
    "                'Intent' : 'intent',\n",
    "                'Field' : 'field',\n",
    "                'SPUD date' : 'spud_date',\n",
    "                'Completion date' : 'completion_date',\n",
    "                'Discovery name' : 'discovery_name',\n",
    "                'Seismic line' : 'seismic_line',\n",
    "                'Country' : 'country',\n",
    "                'KBE' : 'kelly',\n",
    "                'Terminal depth' : 'terminal_depth',\n",
    "                'Water depth' : 'sea_bed',\n",
    "                'Location' : 'location',\n",
    "                'Facility' : 'facility',\n",
    "                'Geodatum' : 'geodatum',\n",
    "                'Latitude' : 'latitude',\n",
    "                'Longitude' : 'longtitude', #spelled incorrectly to match column longtitude in dbo.WELLS\n",
    "                'Grid system' : 'utmzone',\n",
    "                'Surface X' : 'grid_x',\n",
    "                'Surface Y' : 'grid_y',\n",
    "                'Quadrant' : 'quadrant',\n",
    "                'Block' : 'f_block'}\n",
    "    \n",
    "# Apply renaming to each of the dataframes\n",
    "df_explo.rename(columns=rename_for_sql, inplace=True)\n",
    "df_dev.rename(columns=rename_for_sql, inplace=True)\n",
    "\n",
    "# QC renamed columns\n",
    "print(\"Renamed attributes only:\")\n",
    "renamed_columns = list(rename_for_sql.values())\n",
    "df_explo[renamed_columns].head(3)\n",
    "# df_dev[renamed_columns].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for dbo.WELLS that are not in well header file\n",
    "\n",
    "df_explo['datum'] = 4\n",
    "df_dev['datum'] = 4\n",
    "df_explo['symbol_id'] = 3146 #correct later, refer to Status?\n",
    "df_dev['symbol_id'] = 3146 #correct later, refer to Status?\n",
    "df_explo['units'] = 'M'\n",
    "df_dev['units'] = 'M'\n",
    "df_explo['created'] = now\n",
    "df_dev['created'] = now\n",
    "df_explo['creator'] = 1 #correct later\n",
    "df_dev['creator'] = 1 #correct later\n",
    "\n",
    "# df_explo['modified'] = null\n",
    "# df_dev['modified'] = null\n",
    "# df_explo['modifier'] = null\n",
    "# df_dev['modifier'] = null\n",
    "# df_explo['project'] = null\n",
    "# df_dev['project'] = null\n",
    "\n",
    "# Check the result\n",
    "df_explo[['name', 'datum', 'symbol_id', 'units', 'created', 'creator']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate columns, preserving the original\n",
    "df_explo['well_id'] = df_explo['NPDID wellbore']\n",
    "df_dev['well_id'] = df_dev['NPDID wellbore']\n",
    "\n",
    "df_explo['f_uwi'] = df_explo['NPDID wellbore']\n",
    "df_dev['f_uwi'] = df_dev['NPDID wellbore']\n",
    "\n",
    "df_explo['code'] = df_explo['name']\n",
    "df_dev['code'] = df_dev['name']\n",
    "\n",
    "# Limit seismic_line to match nvarchar(80) limit\n",
    "df_explo[\"seismic_line\"] = df_explo[\"seismic_line\"].str[:80]\n",
    "\n",
    "# Check the result\n",
    "df_explo[['name', 'well_id', 'f_uwi', 'code', 'seismic_line']].head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explo wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and re-order explo_dbwells to match dbo.WELLS (filters out fields destined for User Fields!)\n",
    "\n",
    "explo_dbowells_order = [\"well_id\", \"units\", \"created\", \"creator\", \"modified\", \"modifier\", \"project\", \n",
    "                        \"sea_bed\", \"datum\", \"terminal_depth\", \"spud_date\", \"completion_date\", \"quadrant\", \"kelly\", \n",
    "                        \"symbol_id\", \"client\", \"utmzone\", \"code\", \"name\", \"field\", \"location\", \"country\", \"name1\", \n",
    "                        \"f_block\", \"grid_x\", \"grid_y\", \"latitude\", \"longtitude\", \"geodatum\", \"facility\", \"discovery_name\", \n",
    "                        \"seismic_line\", \"intent\", \"f_licenceNumber\", \"f_uwi\"]\n",
    "\n",
    "df_explo_dbowells = df_explo.filter(explo_dbowells_order)\n",
    "\n",
    "df_explo_dbowells = df_explo_dbowells.reindex(columns=explo_dbowells_order)\n",
    "\n",
    "df_explo_dbowells.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_dbowells.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_dbowells.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_dbowells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_explo_dbowells.to_sql('WELLS', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.WELLS')\n",
    "#sqlselect_rows([wells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dbo.WELLS table\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html#pandas.read_sql\n",
    "\n",
    "sql = '''\n",
    "SELECT *\n",
    "FROM dbo.WELLS\n",
    "WHERE intent = 'EXPLORATION'\n",
    "'''\n",
    "\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dev Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and re-order dev_dbwells to match dbo.WELLS (filters out fields destined for User Fields!)\n",
    "\n",
    "dev_dbowells_order = [\"well_id\", \"units\", \"created\", \"creator\", \"modified\", \"modifier\", \"project\", \n",
    "                        \"sea_bed\", \"datum\", \"terminal_depth\", \"spud_date\", \"completion_date\", \"quadrant\", \"kelly\", \n",
    "                        \"symbol_id\", \"client\", \"utmzone\", \"code\", \"name\", \"field\", \"location\", \"country\", \"name1\", \n",
    "                        \"f_block\", \"grid_x\", \"grid_y\", \"latitude\", \"longtitude\", \"geodatum\", \"facility\", \"discovery_name\", \n",
    "                        \"seismic_line\", \"intent\", \"f_licenceNumber\", \"f_uwi\"]\n",
    "\n",
    "df_dev_dbowells = df_dev.filter(dev_dbowells_order)\n",
    "\n",
    "df_dev_dbowells = df_dev_dbowells.reindex(columns=dev_dbowells_order)\n",
    "\n",
    "df_dev_dbowells.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "\n",
    "#####df_dev_dbowells.to_sql('WELLS', engine, if_exists='append', index = False)\n",
    "\n",
    "#####print('dbo.WELLS')\n",
    "#sqlselect_rows([wells])\n",
    "\n",
    "# Still to correct datum, symbol_id, creator. I think these might have to read other tables.\n",
    "# Is well_id ok as npdid_wellbore? Will this cause any problems creating new wells in IC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dbo.WELLS table\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html#pandas.read_sql\n",
    "\n",
    "sql = '''\n",
    "SELECT *\n",
    "FROM dbo.WELLS\n",
    "WHERE intent = 'PRODUCTION'\n",
    "'''\n",
    "\n",
    "#####pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate dbo.PROJECTWELLS (which wells in which Projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbo.PROJECTWELLS requires 3 columns (not null):\n",
    "    #pk_index 11 PK int not null\n",
    "    #well_id FK 2 int not null\n",
    "    #project_id 2 FK int not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = ' SELECT project_id, title FROM PROJECTS '\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projectwells_explo = df_explo_dbowells[['well_id', 'location']]\n",
    "df_projectwells_explo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to populate the project column in dbo.WELLS\n",
    "# This appears empty just now because none of my wells have projects.\n",
    "# Create a function that populates a project number based on well name\n",
    "\n",
    "def location_projectid(row):\n",
    "    if row == 'NORTH SEA':\n",
    "        return 2\n",
    "    elif row == 'NORWEGIAN SEA':\n",
    "        return 3\n",
    "    elif row == 'BARENTS SEA':\n",
    "        return 4\n",
    "    else:\n",
    "        0\n",
    "\n",
    "df_projectwells_explo['project_id'] = df_explo_dbowells['location'].apply(location_projectid)\n",
    "df_projectwells_explo = df_projectwells_explo[['well_id', 'project_id']]\n",
    "df_projectwells_explo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projectwells_explo.to_sql('PROJECTWELLS', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.PROJECTWELLS')\n",
    "sqlselect_rows([projectwells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Populate t_WellsUserFields</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well header columns that are not defaults in IC must be created as Well Attributes\n",
    "\n",
    "print(len(non_default_attributes), 'non-default well attributes:')\n",
    "print(non_default_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call DataFrame constructor on list of attributes\n",
    "\n",
    "df_wellsuserfields = pd.DataFrame(non_default_attributes, columns =['f_FieldName'])\n",
    "\n",
    "# And populate other required columns\n",
    "\n",
    "df_wellsuserfields['f_FieldID'] = range(1,len(df_wellsuserfields)+1)\n",
    "df_wellsuserfields['f_IsInputUsed'] = False\n",
    "df_wellsuserfields['f_InputID'] = 0\n",
    "df_wellsuserfields['f_Description'] = df_wellsuserfields['f_FieldName']\n",
    "df_wellsuserfields['f_Origin'] = 0\n",
    "df_wellsuserfields['f_SortOrder'] = range(1,len(df_wellsuserfields)+1)\n",
    "\n",
    "df_wellsuserfields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wellsuserfields.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_wellsuserfields.to_sql('t_WellsUserFields', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.t_WellsUserFields')\n",
    "sqlselect_rows([wellsuserfields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dbo.t_WellsUserFields\n",
    "\n",
    "sql = '''\n",
    "SELECT *\n",
    "FROM dbo.t_WellsUserFields\n",
    "'''\n",
    "\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Populate t_WellsUserFieldsValues</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use df_explo to build a new dataframe of wellsuserfieldsvalues\n",
    "df_explo_nondefaultattributes = df_explo.filter(non_default_attributes)\n",
    "\n",
    "# Append the name and well_id columns\n",
    "df_explo_nondefaultattributes['name'] = df_explo_dbowells['name']\n",
    "df_explo_nondefaultattributes['well_id'] = df_explo_dbowells['well_id']\n",
    "\n",
    "df_explo_nondefaultattributes.fillna('', inplace=True)\n",
    "\n",
    "df_explo_nondefaultattributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ALL columns to object/string, as they're all destined for one column.\n",
    "# f_StringValue column is dtype('O') in df (i.e. Object) and nvarchar(120) in database.\n",
    "# No point in attempting to maintain datetime data types, as lost when 'melt' is performed below.\n",
    "\n",
    "df_explo_nondefaultattributes = df_explo_nondefaultattributes.astype(str)\n",
    "\n",
    "df_explo_nondefaultattributes = df_explo_nondefaultattributes.replace(to_replace=' 00:00:00', value='', regex=True)\n",
    "\n",
    "df_explo_nondefaultattributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_explo_nondefaultattributes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dates in Well Attributes appear as \"#2019-10-03 00:00:00.0000000\"\n",
    "# # Reformat without time, but maintain datetime64[ns] data type.\n",
    "\n",
    "# # df_explo_nondefaultattributes['Date all updated'] = pd.to_datetime(df_explo_nondefaultattributes['Date all updated'].dt.strftime('%Y-%m-%d'))\n",
    "# df_explo_nondefaultattributes['Date main level updated'] = pd.to_datetime(df_explo_nondefaultattributes['Date main level updated'].dt.strftime('%Y-%m-%d'))\n",
    "# df_explo_nondefaultattributes['Publication date'] = pd.to_datetime(df_explo_nondefaultattributes['Publication date'].dt.strftime('%Y-%m-%d'))\n",
    "# df_explo_nondefaultattributes['Release date'] = pd.to_datetime(df_explo_nondefaultattributes['Release date'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "# # Convert 'Date sync NPD' to datetime\n",
    "# df_explo_nondefaultattributes['Date sync NPD'] = pd.to_datetime(df_explo_nondefaultattributes['Date sync NPD'])\n",
    "\n",
    "# #df_explo_nondefaultattributes.dtypes\n",
    "# df_explo_nondefaultattributes[['Date all updated',\n",
    "#                               'Date main level updated',\n",
    "#                               'Publication date',\n",
    "#                               'Release date',\n",
    "#                               'Date sync NPD']]\n",
    "\n",
    "# # Note that NaT is a pandas null value, pd.NaT\n",
    "\n",
    "# #df_explo_nondefaultattributes['f_StringValue'].dtypes\n",
    "# # Loose datetime data types when melting (below)\n",
    "# # f_StringValue column is dtype('O') in df (i.e. Object) and nvarchar(120) in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot DataFrame from wide to long format, appropriate for dbo.t_WellsUserFieldsValues\n",
    "df_explo_nondefaultattributes = pd.melt(df_explo_nondefaultattributes, id_vars='well_id')\n",
    "\n",
    "# Rename columns to match t_WellsUserFieldsValues\n",
    "df_explo_nondefaultattributes.columns = ['f_WellId', 'f_FieldName', 'f_StringValue']\n",
    "\n",
    "print(df_explo_nondefaultattributes['f_WellId'].nunique())\n",
    "df_explo_nondefaultattributes.sort_values(by='f_WellId').head(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_wellsuserfieldsvalues and df_explo_nondefaultattributes\n",
    "df_wellsuserfieldsvalues = df_wellsuserfields.merge(df_explo_nondefaultattributes, on='f_FieldName', how='inner')\n",
    "\n",
    "# Limit df_wellsuserfieldsvalues to three columns in dbo.t_WellsUserFieldsValues\n",
    "df_wellsuserfieldsvalues = df_wellsuserfieldsvalues[['f_WellId', 'f_FieldID', 'f_StringValue']].sort_values(by=['f_WellId','f_FieldID'])\n",
    "df_wellsuserfieldsvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wellsuserfieldsvalues.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entire df_wellsuserfieldsvalues to SQL Server database\n",
    "\n",
    "df_wellsuserfieldsvalues.to_sql('t_WellsUserFieldsValues', engine, if_exists='append', index=False)\n",
    "\n",
    "print('dbo.t_WellsUserFieldsValues')\n",
    "sqlselect_rows([wellsuserfieldsvalues])\n",
    "\n",
    "# Not sure why f_StringValue values have trailing spaces. These are not shown in IC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join WELLS, t_WellsUserFields and t_WellsUserFieldsValues\n",
    "\n",
    "sql = '''\n",
    "SELECT ufv.f_WellId, w.name, ufv.f_FieldID, uf.f_FieldName, ufv.f_StringValue\n",
    "FROM dbo.t_WellsUserFieldsValues AS ufv\n",
    "JOIN dbo.WELLS AS w\n",
    "ON w.well_id = ufv.f_WellId\n",
    "JOIN dbo.t_WellsUserFields AS uf\n",
    "ON uf.f_FieldId = ufv.f_FieldID\n",
    "--- WHERE (ufv.f_WellId = 99) AND (ufv.f_FieldID = 37)\n",
    "ORDER BY w.name\n",
    "'''\n",
    "temp_df = pd.read_sql_query(sql, engine)\n",
    "temp_df.head(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Well Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERVALCOLUMNS: Default columns\n",
    "\n",
    "Before populating dbo.INTERVALCOLUMNS:\n",
    "- Create a dict of default values in dbo.INTERVALCOLUMNS.\n",
    "- Create list of valid IC data \"styles\".\n",
    "- Create Function intervalcolumns_variations to generate new data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict of default values in dbo.INTERVALCOLUMNS\n",
    "\n",
    "selected_interpreter = None #int\n",
    "selected_creator = '1' #nvarchar (64)\n",
    "\n",
    "# Exclude from list of defaults:\n",
    "    # 'well_id': 0, #int NOT NULL\n",
    "    # 'data_type': 0, #int  NOT NULL\n",
    "    # 'top_depth': 0, #float  NOT NULL\n",
    "    # 'base_depth': 0, #float\n",
    "    # 'legend': '0', #nvarchar (max)\n",
    "    \n",
    "tablestyle_i_defaults = {'symbol_id': 0, #int  NOT NULL\n",
    "                    'top_age': 0, #float\n",
    "                    'base_age': 0, #float\n",
    "                    'owconf': None, #nvarchar (2)\n",
    "                    'owqual': None, #nvarchar (4)\n",
    "                    'owkind': None, #nvarchar (42)\n",
    "                    'owbaseconf': None, #nvarchar (2)\n",
    "                    'owbasequal': None, #nvarchar (4)\n",
    "                    'owbasekind': None, #nvarchar (42)\n",
    "                    'f_interpid': 0, #int\n",
    "                    'creator': selected_creator, #nvarchar (64)\n",
    "                    'modifier': None, #nvarchar (64)\n",
    "                    'abr': None, #nvarchar (80)\n",
    "                    'source': 'Script', #nvarchar (255)\n",
    "                    'attr': None, #nvarchar (max)\n",
    "                    'top_boundary': 0, #smallint\n",
    "                    'base_boundary': 0, #smallint\n",
    "                    'interpreter': selected_interpreter, #nvarchar (6)\n",
    "                    'created': now, #float\n",
    "                    'modified': None, #float\n",
    "                    'obsno': 0, #int\n",
    "                    'mindepth': 0, #float\n",
    "                    'maxdepth': 0, #float\n",
    "                    'remark': None, #nvarchar (80)\n",
    "                    'geofeature': None, #nvarchar (40)\n",
    "                    'dipangle': 0, #float\n",
    "                    'dipazimuth': 0, #float\n",
    "                    'age': 0 #float\n",
    "                   }\n",
    "\n",
    "tablestyle_i_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_Lithostrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatalithostrat = df_lithostrat.copy(deep=True)\n",
    "df_dbodatalithostrat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbo.DATA_Lithostrat stores Gp, Fm and Mbrs.\n",
    "# Use Level column to create data_type column\n",
    "\n",
    "def level_datatypes(row):\n",
    "    if row == 'GROUP':\n",
    "        return 9011\n",
    "    elif row == 'FORMATION':\n",
    "        return 9012\n",
    "    elif row == 'MEMBER':\n",
    "        return 9013\n",
    "    else:\n",
    "        0\n",
    "\n",
    "# New column 'data_type' by applying function 'level_datatypes' to column 'Level'\n",
    "df_dbodatalithostrat['data_type'] = df_dbodatalithostrat['Level'].apply(level_datatypes)\n",
    "df_dbodatalithostrat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match dbo.DATA_Lithostrat\n",
    "\n",
    "rename_stratcols = {'Top depth' : 'top_depth',\n",
    "                    'Base depth' : 'base_depth',\n",
    "                    'Legend' : 'legend',\n",
    "                    'NPDID wellbore' : 'well_id'}\n",
    "\n",
    "# Apply renaming to dataframe\n",
    "df_dbodatalithostrat.rename(columns=rename_stratcols, inplace=True)\n",
    "df_dbodatalithostrat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatalithostrat = df_dbodatalithostrat[['well_id', 'data_type', 'top_depth', 'base_depth', 'legend']]\n",
    "df_dbodatalithostrat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "\n",
    "# Do this with Multiple Column Assignment -\n",
    "# To assign multiple columns with different values, you can use assign with a dictionary.\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\n",
    "\n",
    "df_dbodatalithostrat = df_dbodatalithostrat.assign(**tablestyle_i_defaults)\n",
    "\n",
    "# Overwrite 'attr' column for Lithostrat data only?\n",
    "df_dbodatalithostrat['attr'] = '{\"ZoneColour\":-1,\"ZoneColourIsIpAuto\":true,\"EventSymbolId\":0,\"IsLocked\":false,\"OriginalZoneIndex\":0}'\n",
    "df_dbodatalithostrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatalithostrat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "\n",
    "df_dbodatalithostrat.to_sql('DATA_Lithostrat_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_Lithostrat_NPD')\n",
    "sqlselect_rows([datalithostrat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Tops from Lithostrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatalithostrat_tops = df_dbodatalithostrat.copy(deep=True)\n",
    "df_dbodatalithostrat_tops['data_type'] = 9036\n",
    "df_dbodatalithostrat_tops['base_depth'] = df_dbodatalithostrat_tops['top_depth']\n",
    "df_dbodatalithostrat_tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "\n",
    "df_dbodatalithostrat_tops.to_sql('DATA_Tops_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_Tops_NPD')\n",
    "sqlselect_rows([datatops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_Core_NPD with Cored Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacore = df_core.copy(deep=True)\n",
    "df_dbodatacore.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacore.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodatacore.drop(columns='Well', inplace=True)\n",
    "df_dbodatacore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match dbo.DATA_Core\n",
    "df_dbodatacore.columns = ['well_id', 'top_depth', 'base_depth', 'legend']\n",
    "df_dbodatacore.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data_type column for 'Core Interval (NPD)'\n",
    "df_dbodatacore['data_type'] = 9000\n",
    "\n",
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodatacore = df_dbodatacore.assign(**tablestyle_i_defaults)\n",
    "df_dbodatacore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_dbodatacore.to_sql('DATA_Core_NPD', engine, if_exists='append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC\n",
    "sql = '''\n",
    "SELECT COUNT(DISTINCT data_type) AS 'Data Types', COUNT(DISTINCT well_id) AS 'Wells', COUNT(legend) AS 'Records'\n",
    "FROM DATA_Core_NPD WHERE data_type = 9000\n",
    "'''\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_Core_NPD with Core Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacore_images = df_core_photo.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacore_images.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacore_images.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodatacore_images.drop(columns='Well', inplace=True)\n",
    "df_dbodatacore_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match dbo.DATA_Core\n",
    "df_dbodatacore_images.columns = ['well_id', 'top_depth', 'base_depth', 'legend']\n",
    "df_dbodatacore_images.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data_type column\n",
    "df_dbodatacore_images['data_type'] = 9001\n",
    "\n",
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodatacore_images = df_dbodatacore_images.assign(**tablestyle_i_defaults)\n",
    "df_dbodatacore_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_dbodatacore_images.to_sql('DATA_Core_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_Core_NPD')\n",
    "sqlselect_rows([datacore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = \" SELECT * FROM DATA_Core_NPD WHERE data_type = 9001 \"\n",
    "# pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_Petrography_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapetrography_thinsection = df_thin_section.copy(deep=True)\n",
    "df_datapetrography_thinsection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapetrography_thinsection.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datapetrography_thinsection.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_datapetrography_thinsection.drop(columns='Well', inplace=True)\n",
    "df_datapetrography_thinsection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match dbo.DATA_Petrography\n",
    "df_datapetrography_thinsection.columns = ['well_id', 'top_depth', 'legend']\n",
    "df_datapetrography_thinsection.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data_type column\n",
    "df_datapetrography_thinsection['data_type'] = 9002\n",
    "\n",
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_datapetrography_thinsection = df_datapetrography_thinsection.assign(**tablestyle_i_defaults)\n",
    "df_datapetrography_thinsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_datapetrography_thinsection.to_sql('DATA_Petrography_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_Petrography_NPD')\n",
    "sqlselect_rows([datapetrography])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = \" SELECT * FROM DATA_Petrography_NPD WHERE data_type = 9002 \"\n",
    "# pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_CO2_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dbodataco2_co2content = df_co2.copy(deep=True)\n",
    "df_dbodataco2_co2content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataco2_co2content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodataco2_co2content.drop(columns='Well', inplace=True)\n",
    "df_dbodataco2_co2content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_dbodataco2_co2content.columns = ['Sample sequence number', 'top_depth', 'base_depth', 'CO2 [vol %]', 'Sample type', 'well_id']\n",
    "df_dbodataco2_co2content.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataco2_co2content.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt dataframe BEFORE inserting additional columns\n",
    "# Converts columns into rows.\n",
    "\n",
    "df_dbodataco2_co2content = pd.melt(df_dbodataco2_co2content, \n",
    "                                   id_vars=['well_id', 'top_depth', 'base_depth'], \n",
    "                                   value_vars=['Sample sequence number', 'CO2 [vol %]', 'Sample type'], \n",
    "                                   var_name='data_type_str', value_name='legend')\n",
    "df_dbodataco2_co2content.sort_values(['data_type_str', 'well_id'], inplace=True)\n",
    "\n",
    "df_dbodataco2_co2content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new column for data_type\n",
    "\n",
    "def co2_datatypes(row):\n",
    "    if row['data_type_str'] == 'Sample sequence number':\n",
    "        val = 9003\n",
    "    elif row['data_type_str'] == 'CO2 [vol %]':\n",
    "        val = 9004\n",
    "    elif row['data_type_str'] == 'Sample type':\n",
    "        val = 9005\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "df_dbodataco2_co2content['data_type'] = df_dbodataco2_co2content.apply(co2_datatypes, axis=1)\n",
    "df_dbodataco2_co2content.drop(columns='data_type_str', inplace=True)\n",
    "df_dbodataco2_co2content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodataco2_co2content = df_dbodataco2_co2content.assign(**tablestyle_i_defaults)\n",
    "df_dbodataco2_co2content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "\n",
    "df_dbodataco2_co2content.to_sql('DATA_CO2_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_CO2_NPD')\n",
    "sqlselect_rows([dataco2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM DATA_CO2_NPD \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_OilSample_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataoilsample = df_oil_sample.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataoilsample.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodataoilsample.drop(columns='Well', inplace=True)\n",
    "df_dbodataoilsample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match dbo.DATA_OilSample\n",
    "df_dbodataoilsample.columns = ['Test type', 'Bottle number', 'top_depth', 'base_depth', 'Fluid type', 'Test time', 'Received date', 'well_id']\n",
    "df_dbodataoilsample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataoilsample['Test time'] = df_dbodataoilsample['Test time'].dt.date\n",
    "df_dbodataoilsample['Received date'] = df_dbodataoilsample['Received date'].dt.date\n",
    "df_dbodataoilsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodataoilsample.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt dataframe BEFORE inserting additional columns\n",
    "# Converts columns into rows\n",
    "\n",
    "df_dbodataoilsample = pd.melt(df_dbodataoilsample, id_vars=['well_id', 'top_depth', 'base_depth'], \n",
    "                              value_vars=['Test type', 'Bottle number', 'Fluid type', 'Test time', 'Received date'], \n",
    "                              var_name='data_type_str', value_name='legend')\n",
    "df_dbodataoilsample.sort_values(['data_type_str', 'well_id'], inplace=True)\n",
    "\n",
    "df_dbodataoilsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new column for data_type\n",
    "\n",
    "def oilsample_datatypes(row):\n",
    "    if row['data_type_str'] == 'Test type':\n",
    "        val = 9006\n",
    "    elif row['data_type_str'] == 'Bottle number':\n",
    "        val = 9007\n",
    "    elif row['data_type_str'] == 'Fluid type':\n",
    "        val = 9008\n",
    "    elif row['data_type_str'] == 'Test time':\n",
    "        val = 9009\n",
    "    elif row['data_type_str'] == 'Received date':\n",
    "        val = 9010\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "df_dbodataoilsample['data_type'] = df_dbodataoilsample.apply(oilsample_datatypes, axis=1)\n",
    "df_dbodataoilsample.drop(columns='data_type_str', inplace=True)\n",
    "df_dbodataoilsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodataoilsample = df_dbodataoilsample.assign(**tablestyle_i_defaults)\n",
    "df_dbodataoilsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "\n",
    "df_dbodataoilsample.to_sql('DATA_OilSample_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_OilSample_NPD')\n",
    "sqlselect_rows([dataoilsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM DATA_OilSample_NPD \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_DST_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatadst = df_dst.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatadst.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatadst.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodatadst.drop(columns='Well', inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "df_dbodatadst.rename(columns={'NPDID wellbore': 'well_id', 'Top depth': 'top_depth', 'Base depth': 'base_depth'}, inplace=True)\n",
    "df_dbodatadst.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt columns to rows\n",
    "\n",
    "df_dbodatadst = pd.melt(df_dbodatadst, id_vars=['well_id', 'top_depth', 'base_depth'], \n",
    "                              value_vars=['Test number', \n",
    "                                          'Choke size [mm]', \n",
    "                                          'Final shut-in pressure [MPa]', \n",
    "                                          'Final flow pressure [MPa]', \n",
    "                                          'Bottom hole pressure [MPa]',\n",
    "                                          'Oil [Sm3/day]',\n",
    "                                          'Gas [Sm3/day]',\n",
    "                                          'Oil density [g/cm3]',\n",
    "                                          'Gas grav. rel.air',\n",
    "                                          'GOR [m3/m3]',\n",
    "                                          'Downhole temperature [°C]'\n",
    "                                          ], \n",
    "                              var_name='data_type_str', value_name='legend')\n",
    "df_dbodatadst.sort_values(['data_type_str', 'well_id'], inplace=True)\n",
    "\n",
    "df_dbodatadst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new column for data_type\n",
    "\n",
    "def dst_datatypes(row):\n",
    "    if row['data_type_str'] == 'Test number':\n",
    "        val = 9014\n",
    "    elif row['data_type_str'] == 'Choke size [mm]':\n",
    "        val = 9015\n",
    "    elif row['data_type_str'] == 'Final shut-in pressure [MPa]':\n",
    "        val = 9016\n",
    "    elif row['data_type_str'] == 'Final flow pressure [MPa]':\n",
    "        val = 9017\n",
    "    elif row['data_type_str'] == 'Bottom hole pressure [MPa]':\n",
    "        val = 9018\n",
    "    elif row['data_type_str'] == 'Oil [Sm3/day]':\n",
    "        val = 9019\n",
    "    elif row['data_type_str'] == 'Gas [Sm3/day]':\n",
    "        val = 9020\n",
    "    elif row['data_type_str'] == 'Oil density [g/cm3]':\n",
    "        val = 9021\n",
    "    elif row['data_type_str'] == 'Gas grav. rel.air':\n",
    "        val = 9022\n",
    "    elif row['data_type_str'] == 'GOR [m3/m3]':\n",
    "        val = 9023 \n",
    "    elif row['data_type_str'] == 'Downhole temperature [°C]':\n",
    "        val = 9024\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "df_dbodatadst['data_type'] = df_dbodatadst.apply(dst_datatypes, axis=1)\n",
    "df_dbodatadst.drop(columns='data_type_str', inplace=True)\n",
    "df_dbodatadst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodatadst = df_dbodatadst.assign(**tablestyle_i_defaults)\n",
    "df_dbodatadst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_dbodatadst.to_sql('DATA_DST_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.')\n",
    "sqlselect_rows([datadrillstemtests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM DATA_DST_NPD \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_CasingLOT_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacasinglot = df_casinglot.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacasinglot.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatacasinglot.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodatacasinglot.drop(columns='Well', inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "df_dbodatacasinglot.rename(columns={'NPDID wellbore': 'well_id', 'Depth': 'top_depth'}, inplace=True)\n",
    "df_dbodatacasinglot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt columns to rows\n",
    "\n",
    "df_dbodatacasinglot = pd.melt(df_dbodatacasinglot, id_vars=['well_id', 'top_depth'], \n",
    "                              value_vars=['Casing type', \n",
    "                                          'Casing diam. [inch]', \n",
    "                                          'Hole diam. [inch]', \n",
    "                                          'Hole depth[m]', \n",
    "                                          'LOT mud eqv. [g/cm3]',\n",
    "                                          'Formation test type'\n",
    "                                          ], \n",
    "                              var_name='data_type_str', value_name='legend')\n",
    "\n",
    "df_dbodatacasinglot.sort_values(['data_type_str', 'well_id'], inplace=True)\n",
    "\n",
    "df_dbodatacasinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new column for data_type\n",
    "\n",
    "def casinglot_datatypes(row):\n",
    "    if row['data_type_str'] == 'Casing type':\n",
    "        val = 9025\n",
    "    elif row['data_type_str'] == 'Casing diam. [inch]':\n",
    "        val = 9026\n",
    "    elif row['data_type_str'] == 'Hole diam. [inch]':\n",
    "        val = 9027\n",
    "    elif row['data_type_str'] == 'Hole depth[m]':\n",
    "        val = 9028\n",
    "    elif row['data_type_str'] == 'LOT mud eqv. [g/cm3]':\n",
    "        val = 9029\n",
    "    elif row['data_type_str'] == 'Formation test type':\n",
    "        val = 9030\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "df_dbodatacasinglot['data_type'] = df_dbodatacasinglot.apply(casinglot_datatypes, axis=1)\n",
    "df_dbodatacasinglot.drop(columns='data_type_str', inplace=True)\n",
    "df_dbodatacasinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodatacasinglot = df_dbodatacasinglot.assign(**tablestyle_i_defaults)\n",
    "df_dbodatacasinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_dbodatacasinglot.to_sql('DATA_CasingLOT_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_CasingLOT_NPD')\n",
    "sqlselect_rows([datacasing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM DATA_CasingLOT_NPD \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate DATA_DrillingMud_NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatamud = df_mud.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatamud.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbodatamud.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_dbodatamud.drop(columns='Well', inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "# Note: no base_depth\n",
    "df_dbodatamud.rename(columns={'NPDID wellbore': 'well_id', 'Depth': 'top_depth'}, inplace=True)\n",
    "df_dbodatamud.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt columns to rows\n",
    "df_dbodatamud = pd.melt(df_dbodatamud, id_vars=['well_id', 'top_depth'], \n",
    "                              value_vars=['Mud weight [g/cm3]', \n",
    "                                          'Visc. [mPa.s]', \n",
    "                                          'Yield point [Pa]', \n",
    "                                          'Mud type', \n",
    "                                          'Date measured'\n",
    "                                          ], \n",
    "                              var_name='data_type_str', value_name='legend')\n",
    "df_dbodatamud.sort_values(['data_type_str', 'well_id'], inplace=True)\n",
    "\n",
    "df_dbodatamud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new column for data_type\n",
    "\n",
    "def mud_datatypes(row):\n",
    "    if row['data_type_str'] == 'Mud weight [g/cm3]':\n",
    "        val = 9031\n",
    "    elif row['data_type_str'] == 'Visc. [mPa.s]':\n",
    "        val = 9032\n",
    "    elif row['data_type_str'] == 'Yield point [Pa]':\n",
    "        val = 9033\n",
    "    elif row['data_type_str'] == 'Mud type':\n",
    "        val = 9034\n",
    "    elif row['data_type_str'] == 'Date measured':\n",
    "        val = 9035\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "df_dbodatamud['data_type'] = df_dbodatamud.apply(mud_datatypes, axis=1)\n",
    "df_dbodatamud.drop(columns='data_type_str', inplace=True)\n",
    "df_dbodatamud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate additional columns from dict tablestyle_i_defaults\n",
    "df_dbodatamud = df_dbodatamud.assign(**tablestyle_i_defaults)\n",
    "df_dbodatamud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_dbodatamud.to_sql('DATA_DrillingMud_NPD', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.DATA_DrillingMud_NPD')\n",
    "sqlselect_rows([datadrillingmud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM DATA_DrillingMud_NPD \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate NG_LINKS (well references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_references_nglinks = df_refs_and_docs.copy(deep=True)\n",
    "df_references_nglinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_references_nglinks.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_references_nglinks\n",
    "#df_references_nglinks.to_csv('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Well' column as IC will use well_id\n",
    "df_references_nglinks.drop(columns='Well', inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "df_references_nglinks.rename(columns={'NPDID wellbore': 'well_id', 'Title': 'title', 'URL': 'url'}, inplace=True)\n",
    "df_references_nglinks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image (nvarchar(255), null)\n",
    "# well_id (int, null)\n",
    "# title (nvarchar(80), null)\n",
    "# url (nvarchar(250), null)\n",
    "\n",
    "#print(df_references_nglinks['title'].str.len().max())\n",
    "#print(df_references_nglinks['url'].str.len().max())\n",
    "\n",
    "# Limit seismic_line to match nvarchar(80) limit\n",
    "df_references_nglinks[\"title\"] = df_references_nglinks[\"title\"].str[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df to database\n",
    "df_references_nglinks.to_sql('NG_LINKS', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.NG_LINKS')\n",
    "sqlselect_rows([references_nglinks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \" SELECT * FROM NG_LINKS \"\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes in project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show all the dataframes used in this notebook\n",
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break me here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'watermark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-886a0d3b25f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwatermark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'watermark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'watermark'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-v -m -p pandas,numpy,watermark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'watermark'"
     ]
    }
   ],
   "source": [
    "import watermark\n",
    "%load_ext watermark\n",
    "\n",
    "%watermark -v -m -p pandas,numpy,watermark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy=1.14.3=py36h9fa60d3_1\n",
    "pandas=0.23.0=py36h830ac7b_0\n",
    "pyodbc=4.0.23=py36h6538335_0\n",
    "python=3.6.5=h0c2934d_0\n",
    "requests=2.18.4=py36h4371aae_1\n",
    "sqlalchemy=1.2.7=py36ha85dd04_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print .__version__\n",
    "print .__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "784px",
    "left": "23px",
    "top": "133px",
    "width": "334px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
