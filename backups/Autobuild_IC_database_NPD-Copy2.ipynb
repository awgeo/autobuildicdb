{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Autobuild IC database - NPD</h1><br>\n",
    "Auto-populate a blank SQL Server IC database with data from the <a href=\"https://factpages.npd.no/factpages/Default.aspx?culture=en\", target=\"_blank\">NPD FactPages</a>.\n",
    "\n",
    "<b>Part 1. Downloads the following data types, reformats for IC and exports to .csv.</b>\n",
    "\n",
    "Exploration well headers<br>\n",
    "Development well headers<br>\n",
    "Cores<br>\n",
    "Core photos (also saves .jpf files to disc)<br>\n",
    "Thin sections<br>\n",
    "CO2<br>\n",
    "Oil samples<br>\n",
    "Lithostratigraphy<br>\n",
    "Drill stem tests<br>\n",
    "Casing and leak-off tests<br>\n",
    "Drilling mud<br>\n",
    "Documents<br>\n",
    "Shapefiles (download from <a href=\"https://www.npd.no/en/about-us/information-services/available-data/map-services/\", target=\"_blank\">NPD Map Services</a> and unzip)<br>\n",
    "\n",
    "<b>Part 2. Connects to and pushes data to the database.</b><br>\n",
    "Only works for Well Headers, References and Lithostrat just now.<br>\n",
    "Creates dynamic IC projects, well queries, and builds text dictionaries from Lithostrat.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1. Download the following data types from the NPD, reformat for IC and export .csv files.\n",
    "\n",
    "# Exploration well headers\n",
    "# Development well headers\n",
    "# Cores\n",
    "# Core photos (also saves .jpf files to disc)\n",
    "# Thin sections\n",
    "# CO2\n",
    "# Oil samples\n",
    "# Lithostratigraphy\n",
    "# Drill stem tests\n",
    "# Casing and leak-off tests\n",
    "# Drilling mud\n",
    "# Documents\n",
    "# Shapefiles (download and unzip)\n",
    "# See https://factpages.npd.no/factpages/Default.aspx?culture=en\n",
    "\n",
    "# Part 2. Connect to and push data to SQL Server database.\n",
    "# Also creates dynamic IC projects, well queries, builds text dictionaries from Lithostrat.\n",
    "\n",
    "# This notebook is missing section that creates text dictionaries from lithostrat (SYMBOLSDICTIONARY)\n",
    "# Writing files to C:\\Alan Python\\IC_wellheaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment your chosen data source -\n",
    "    # web: downloads data live using permanent NPD links\n",
    "    # file: if you have manually downloaded data in Excel format and saved to 'input data' folder \n",
    "# Use file for for testing purposes\n",
    "\n",
    "#source = 'web'\n",
    "source = 'file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1. Download NPD data, reformat for IC and output to .csv</h2>\n",
    "    \n",
    "<h3>Well Headers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import ExcelFile\n",
    "from pandas import ExcelWriter\n",
    "import urllib.request\n",
    "import requests, zipfile, io\n",
    "\n",
    "#Change Pandas display settings to show all columns\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "#pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploration well header column titles:\n",
      "['Wellbore name', 'Well name', 'Drilling operator', 'Drilled in production licence', 'Purpose', 'Status', 'Content', 'Type', 'Subsea', 'Entered date', 'Completed date', 'Field', 'Drill permit', 'Discovery', 'Discovery wellbore', 'Bottom hole temperature [째C]', 'Sitesurvey', 'Seismic location', 'Maximum inclination [째]', 'Kelly bushing elevation [m]', 'Final vertical depth (TVD) [m RKB]', 'Total depth (MD) [m RKB]', 'Water depth [m]', 'Kick off  point [m RKB]', 'Oldest penetrated age', 'Oldest penetrated formation', 'Main area', 'Drilling facility', 'Drilling facility type', 'Drilling facility category', 'Licensing activity awarded in', 'Multilateral', 'Purpose - planned', 'Entry year', 'Completed year', 'Reclassified from/to wellbore', 'Reentry activity', 'Plot symbol', '1st level with HC, formation', '1st level with HC, age', '2nd level with HC, formation', '2nd level with HC, age', '3rd level with HC, formation', '3rd level with HC, age', 'Drilling days', 'Reentry', 'Prod. licence for drilling target', 'Plugged and abondon date', 'Plugged date', 'Geodetic datum', 'NS degrees', 'NS minutes', 'NS seconds', 'NS code', 'EW degrees', 'EW minutes', 'EW seconds', 'EW code', 'NS decimal degrees', 'EW decimal degrees', 'NS UTM [m]', 'EW UTM [m]', 'UTM zone', 'Wellbore name, part 1', 'Wellbore name, part 2', 'Wellbore name, part 3', 'Wellbore name, part 4', 'Wellbore name, part 5', 'Wellbore name, part 6', 'Pressrelease url', 'FactPage url', 'Factmaps', 'DISKOS Well Type', 'DISKOS Wellbore Parent', 'Publication date', 'Release date', 'Reclassified date', 'NPDID wellbore', 'NPDID discovery', 'NPDID field', 'NPDID drilling facility', 'NPDID wellbore reclassified from', 'NPDID production licence drilled in', 'NPDID site survey', 'Date main level updated', 'Date all updated', 'Date sync NPD']\n",
      "\n",
      "Development well header column titles:\n",
      "['Wellbore name', 'Well name', 'Drilling operator', 'Drilled in production licence', 'Status', 'Purpose', 'Purpose - planned', 'Content', 'Type', 'Subsea', 'Entered date', 'Completed date', 'Predrilled entry date', 'Predrilled completion date', 'Field', 'Drill permit', 'Discovery', 'Discovery wellbore', 'Kelly bushing elevation [m]', 'Final vertical depth (TVD) [m RKB]', 'Total depth (MD) [m RKB]', 'Water depth [m]', 'Kick off  point [m RKB]', 'Main area', 'Drilling facility', 'Drilling facility type', 'Drilling facility category', 'Production facility', 'Licensing activity awarded in', 'Multilateral', 'Content - planned', 'Entry year', 'Completed year', 'Reclassified from/to wellbore', 'Plugged and abondon date', 'Plugged date', 'Prod. licence for drilling target', 'Plot symbol', 'Geodetic datum', 'NS degrees', 'NS minutes', 'NS seconds', 'NS code', 'EW degrees', 'EW minutes', 'EW seconds', 'EW code', 'NS decimal degrees', 'EW decimal degrees', 'NS UTM [m]', 'EW UTM [m]', 'UTM zone', 'Wellbore name, part 1', 'Wellbore name, part 2', 'Wellbore name, part 3', 'Wellbore name, part 4', 'Wellbore name, part 5', 'Wellbore name, part 6', 'FactPage url', 'Factmaps', 'DISKOS Well Type', 'DISKOS Wellbore Parent', 'NPDID wellbore', 'NPDID discovery', 'NPDID field', 'Publication date', 'Release date', 'NPDID production licence drilled in', 'NPDID production licence target', 'NPDID drilling facility', 'NPDID production facility', 'NPDID wellbore reclassified from', 'Date main level updated', 'Date all updated', 'Date sync NPD']\n"
     ]
    }
   ],
   "source": [
    "# Download the latest NPD well headers in Excel format\n",
    "# Navigate to NPD Factpages > Wellbore > Table View > Exploration/Development > All - Long List> Export Excel.\n",
    "# Assign to two dataframes, one for Exploraion wells and one for Development wells\n",
    "\n",
    "\n",
    "if source == 'web':\n",
    "    df_explo_web = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_exploration_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.169&CultureCode=en', \n",
    "                             sheet_name='wellbore_exploration_all')\n",
    "    df_dev_web = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_development_all&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.169&CultureCode=en', \n",
    "                           sheet_name='wellbore_development_all')\n",
    "\n",
    "if source == 'file':\n",
    "    # Navigate to NPD Factpages > Wellbore > Table View > Exploration/Development > All - Long List> Export Excel.\n",
    "    df_explo_file = pd.read_excel('input data/wellbore_exploration_all.xlsx', \n",
    "                             sheet_name='wellbore_exploration_all')\n",
    "    df_dev_file = pd.read_excel('input data/wellbore_development_all.xlsx', \n",
    "                           sheet_name='wellbore_development_all')\n",
    "\n",
    "Print the original column titles in each dataframe.\n",
    "print(\"\\nExploration well header column titles:\")\n",
    "print(list(df_explo.columns))\n",
    "print(\"\\nDevelopment well header column titles:\")\n",
    "print(list(df_dev.columns))\n",
    "\n",
    "# This will take a minute to download and process the two files. Ignore file size warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_explo_rows, num_explo_cols) = df_explo.shape\n",
    "(num_dev_rows, num_dev_cols) = df_dev.shape\n",
    "print('{} rows and {} columns in Exploration wells.'.format(num_explo_rows, num_explo_cols))\n",
    "print('{} rows and {} columns in Development wells.'.format(num_dev_rows, num_dev_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The first 3 rows of Exploration wells:')\n",
    "df_explo.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first 3 rows of Development wells:')\n",
    "df_dev.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>QC - Which column headers are unique to Exploration or Development wells?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explo_columns = df_explo.columns.tolist()\n",
    "dev_columns = df_dev.columns.tolist()\n",
    "\n",
    "# List well headers unqiue to each dataframe\n",
    "print('Attributes unique to Exploration wells:\\n', sorted(set(explo_columns) - set(dev_columns)))\n",
    "print('\\nAttributes unique to Development wells:\\n', sorted(set(dev_columns) - set(explo_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rename attributes for IC</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are IC's default well header attributes (when matching columns in Import Well Header File)\n",
    "# Try to use as many of these as possible when renaming below.\n",
    "# Any other columns will need to be added to IC as Well Attributes.\n",
    "\n",
    "# QUESTION - WHY IS THIS A DICT AND NOT LIST?\n",
    "\n",
    "ic_default_attributes = {'Name', 'Code', 'Alternate 1', 'Alternate 2', 'API number', 'UWI number', 'Comment', 'Geodatum', \n",
    "                         'Longitude', 'Latitude', 'Grid system', 'Surface X', 'Surface Y', 'Elevation Reference',\n",
    "                         'Elevation', 'KBE', 'RTE', 'DFE', 'GLE', 'SPUD date', 'Completion date', 'Status', \n",
    "                         'Quadrant', 'Block', 'Sub block', 'Field', 'Location', 'Operator', 'Country',\n",
    "                         'Basin', 'Province', 'County', 'State', 'Section', 'Township', 'Range', 'Terminal depth',\n",
    "                         'Water depth', 'Facility', 'Discovery name', 'Seismic line', 'Intent', 'Licence number'}\n",
    "\n",
    "# Rename columns from/to. \n",
    "# Check spelling and capitalisation carefully when renaming to match IC's default attributes.\n",
    "\n",
    "attributes_to_rename = {'Wellbore name' : 'Name',\n",
    "                        'Well name' : 'Alternate 1',\n",
    "                        'Drilling operator' : 'Operator',\n",
    "                        'Drilled in production licence' : 'Licence number',\n",
    "                        'Purpose' : 'Intent',\n",
    "                        'Purpose - planned' : 'Intent - planned',\n",
    "                        'Status' : 'Well status',\n",
    "                        'Content' : 'Well content',\n",
    "                        'Entered date' : 'SPUD date',\n",
    "                        'Completed date' : 'Completion date',\n",
    "                        'Discovery' : 'Discovery name',\n",
    "                        'Seismic location' : 'Seismic line',\n",
    "                        'Kelly bushing elevation [m]' : 'KBE',\n",
    "                        'Total depth (MD) [m RKB]' : 'Terminal depth',\n",
    "                        'Water depth [m]' : 'Water depth',\n",
    "                        'Kick off  point [m RKB]' : 'Kick off point [m RKB]', #remove extra space\n",
    "                        'Main area' : 'Location',\n",
    "                        'Drilling facility' : 'Facility',\n",
    "                        '1st level with HC, formation' : '1st level with HC formation', #remove commas to be csv friendly\n",
    "                        '1st level with HC, age' : '1st level with HC age',\n",
    "                        '2nd level with HC, formation' : '2nd level with HC formation',\n",
    "                        '2nd level with HC, age' : '2nd level with HC age',\n",
    "                        '3rd level with HC, formation' : '3rd level with HC formation',\n",
    "                        '3rd level with HC, age' : '3rd level with HC age',\n",
    "                        'Geodetic datum' : 'Geodatum',\n",
    "                        'NS decimal degrees' : 'Latitude',\n",
    "                        'EW decimal degrees' : 'Longitude',\n",
    "                        'NS UTM [m]' : 'Surface Y',\n",
    "                        'EW UTM [m]' : 'Surface X',\n",
    "                        'Wellbore name, part 1' : 'Quadrant',\n",
    "                        'Wellbore name, part 2' : 'Block', \n",
    "                        'Pressrelease url' : 'Press Release URL',\n",
    "                        'FactPage url' : 'FactPage URL',\n",
    "                        'Factmaps' : 'FactMaps URL'}\n",
    "\n",
    "# Apply renaming to each of the dataframes\n",
    "df_explo.rename(columns=attributes_to_rename, inplace=True)\n",
    "df_dev.rename(columns=attributes_to_rename, inplace=True)\n",
    "\n",
    "# QC only renamed columns\n",
    "print(\"Renamed attributes only:\")\n",
    "renamed_columns = list(attributes_to_rename.values())\n",
    "df_explo[renamed_columns].head(n=3)\n",
    "#df_dev[renamed_columns].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Delete some attributes we don't need in IC</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates are repeated elsewhere so we can delete the component parts from the dataframes.\n",
    "# And we've renamed Wellbore name parts 1 and 2 to Quadrant and Block, and do not need the other parts.\n",
    "\n",
    "attributes_to_drop = ['Plot symbol', 'NS degrees', 'NS minutes', 'NS seconds', 'NS code', 'EW degrees', 'EW minutes', 'EW seconds', 'EW code', \n",
    "                      'Wellbore name, part 3', 'Wellbore name, part 4', 'Wellbore name, part 5', 'Wellbore name, part 6']\n",
    "\n",
    "df_explo.drop(attributes_to_drop, axis=1, inplace=True)\n",
    "df_dev.drop(attributes_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print('Prove we still have well names and coordinates:')\n",
    "df_explo[['Name', 'Latitude', 'Longitude', 'Surface Y', 'Surface X']].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Truncate well list based on column and value(s)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the column and values you want to return, e.g. Location: BARENTS SEA, or Quadrant: 6204, 6205.\n",
    "fltr_column = 'Location'\n",
    "\n",
    "# List the names you want to *KEEP*!\n",
    "fltr_value = ['NORTH SEA', 'NORWEGIAN SEA', 'BARENTS SEA']\n",
    "\n",
    "# Apply the filter to the dataframes\n",
    "indexNames = df_explo[~df_explo[fltr_column].isin(fltr_value)].index\n",
    "df_explo.drop(indexNames , inplace=True)\n",
    "\n",
    "indexNames = df_dev[~df_dev[fltr_column].isin(fltr_value)].index\n",
    "df_dev.drop(indexNames , inplace=True)\n",
    "\n",
    "# Get dataframe shape and unpack tuples\n",
    "(exploRows, exploCols) = df_explo.shape\n",
    "(devRows, devCols) = df_dev.shape\n",
    "\n",
    "# Print out the results\n",
    "print(\"After filtering on {}: {}, you are left with:\\n {} rows for Exploration wells, and {} rows for Development wells.\"\n",
    "      .format(fltr_column, fltr_value, exploRows, devRows))\n",
    "print('The first and last rows are:')\n",
    "\n",
    "# Print the first and last rows of the Exploration dataframe to check that the filter has worked\n",
    "df_explo.iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>CREATE FILES - create Reference files for IC containing URLs for Exploration and Development wells</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts three URL columns into three rows. Adds a Title column and sorts by Well and Title.\n",
    "df_explo_references = df_explo[['Name', 'Press Release URL', 'FactPage URL', 'FactMaps URL']]\n",
    "df_explo_references = pd.melt(df_explo_references, id_vars='Name', value_vars=['Press Release URL', 'FactPage URL', 'FactMaps URL'], var_name='Title', value_name='URL')\n",
    "df_explo_references.sort_values(['Name', 'Title'], inplace=True)\n",
    "\n",
    "# Remove empty rows, specifically where no 'Press Release URL' for Exploration references\n",
    "df_explo_references['URL'].replace(' ', np.nan, inplace=True)\n",
    "df_explo_references.dropna(subset=['URL'], inplace=True)\n",
    "\n",
    "# Name and create file for Exploration wells\n",
    "explo_ref_filename = 'output data/IC_explo_references.csv'\n",
    "df_explo_references.to_csv(explo_ref_filename, index=False)\n",
    "print('Created file:', explo_ref_filename)\n",
    "df_explo_references.head(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, but creates 'Reference' file for Development Wells (minus the Press Release URL)\n",
    "df_dev_references = df_dev[['Name', 'FactPage URL', 'FactMaps URL']]\n",
    "df_dev_references = pd.melt(df_dev_references, id_vars='Name', \n",
    "                            value_vars=['FactPage URL', 'FactMaps URL'], \n",
    "                            var_name='Title', value_name='URL')\n",
    "df_dev_references.sort_values(['Name', 'Title'], inplace=True)\n",
    "\n",
    "# Name and create file for Development wells\n",
    "dev_ref_filename = 'output data/IC_dev_references.csv'\n",
    "df_dev_references.to_csv(dev_ref_filename, index=False)\n",
    "print('Created file:', dev_ref_filename)\n",
    "df_dev_references.head(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop URL attributes\n",
    "# Now that we've output the URLs to separate files, we no longer need them in the Exploration and Development dataframes.\n",
    "df_explo.drop(['Press Release URL', 'FactPage URL', 'FactMaps URL'], axis=1, inplace=True)\n",
    "df_dev.drop(['FactPage URL', 'FactMaps URL'], axis=1, inplace=True)\n",
    "\n",
    "df_explo.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column(s) and assign constant value, e.g. Country: NORWAY.\n",
    "df_explo['Country'] = 'NORWAY' \n",
    "df_dev['Country'] = 'NORWAY'\n",
    "\n",
    "# IC Version 4.3.1 and earlier only. Fixed in 4.3.2.\n",
    "# First lets rename an extraordinarily long string in column 'Seismic line' to avoid an error in IC.\n",
    "#df_explo['Seismic line'] = df_explo['Seismic line'].replace('TUN15M01 3D bin datasett: Inline reference: 12688 Croslline reference: between 12383 and 12384', 'TUN15M01 3D bin: Inline 12688 Crossline 12383-12384')\n",
    "\n",
    "# Remove decimal places introduced to the 'NPDIP' columns\n",
    "df_explo['NPDID discovery'] = df_explo['NPDID discovery'].fillna(0).astype(int)\n",
    "df_dev['NPDID discovery'] = df_dev['NPDID discovery'].fillna(0).astype(int)\n",
    "\n",
    "df_explo['NPDID drilling facility'] = df_explo['NPDID drilling facility'].fillna(0).astype(int)\n",
    "df_dev['NPDID drilling facility'] = df_dev['NPDID drilling facility'].fillna(0).astype(int)\n",
    "\n",
    "df_explo['NPDID field'] = df_explo['NPDID field'].fillna(0).astype(int)\n",
    "df_dev['NPDID field'] = df_dev['NPDID field'].fillna(0).astype(int)\n",
    "\n",
    "# Copy data from one column to another, preserving the original.\n",
    "df_explo['UWI number'] = df_explo['NPDID wellbore']\n",
    "df_dev['UWI number'] = df_dev['NPDID wellbore']\n",
    "\n",
    "# Check the result\n",
    "df_explo[['Name', 'Country', 'NPDID drilling facility', 'NPDID wellbore']].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Concatenate Well Status & Well Content to match IC's Well Symbols dictionary</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a new column called 'Status', combining 'Well Status' and 'Well Content'\n",
    "# These values should match IC's Well Symbols graphic dictionary entries, e.g. \"P & A Oil Shows\"\n",
    "\n",
    "# Change 'P&A' to 'P & A'.\n",
    "df_explo['Well status'] = df_explo['Well status'].replace(to_replace='P&A', value='P & A')\n",
    "# First letter of each word capitalised\n",
    "df_explo['Status'] = df_explo['Well status'].str.title() + ' ' + df_explo['Well content'].str.title()\n",
    "\n",
    "# As above but for Development wells\n",
    "df_dev['Well status'] = df_dev['Well status'].replace(to_replace='P&A', value='P & A')\n",
    "df_dev['Status'] = df_dev['Well status'].str.title() + ' ' + df_dev['Well content'].str.title()\n",
    "\n",
    "# Replace a few other things to help with matching\n",
    "df_explo = df_explo.replace({'Status' : { ' Not Available' : '', ' Not Applicable' : '', '/' : ' ', 'Oil Gas ' : 'Oil & Gas '}}, regex=True)\n",
    "df_dev = df_dev.replace({'Status' :     { ' Not Available' : '', ' Not Applicable' : '', '/' : ' ', 'Oil Gas ' : 'Oil & Gas '}}, regex=True)\n",
    "\n",
    "# Rename Status to status? (links to symbol_id??? e.g. 22)\n",
    "\n",
    "# Check the results\n",
    "df_explo[['Name', 'Status']].head(n=10)\n",
    "#df_dev[['Name', 'Status']].tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all unique entries under Status for all wells.\n",
    "# In IC, open Database > Graphic Dictionaries > Well Symbols, and ensure you have dictionary entries for each.\n",
    "\n",
    "lst_explo_status = sorted(set(df_explo['Status'].astype(str)))\n",
    "lst_dev_status = sorted(set(df_dev['Status'].astype(str)))\n",
    "\n",
    "lst_all_status = lst_explo_status + lst_dev_status\n",
    "\n",
    "print(\"{} unique status values to include in IC 'Well Symbols' graphic dictionary:\".format(len(lst_all_status)))\n",
    "print('')\n",
    "lst_unique_status = sorted(set(lst_all_status))\n",
    "print(', '.join(lst_unique_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Concatenate cells to create 'Grid system' in IC format</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# At time of writing, there are several problems with 'Geodatum' in the NPD datasets, including:\n",
    "#  - trailing spaces ('ED50 ') in all Explo wells\n",
    "#  - erroneous '56ED50', '60ED50' and '61ED50' values in Dev wells\n",
    "#  - missing 'ED50' values in two explo wells\n",
    "# Luckily, we can just force 'ED50' on all these cells!\n",
    "\n",
    "df_explo['Geodatum'] = 'ED50'\n",
    "df_dev['Geodatum'] = 'ED50'\n",
    "\n",
    "# Concatenate cells to create a new column 'Grid system' in IC format (e.g. \"ED50 / UTM Zone 31N\")\n",
    "\n",
    "df_explo['Grid system'] = df_explo['Geodatum'] + ' / ' + 'UTM zone ' + df_explo['UTM zone'].map(str) + 'N'\n",
    "df_dev['Grid system'] = df_dev['Geodatum'] + ' / ' + 'UTM zone ' + df_dev['UTM zone'].map(str) + 'N'\n",
    "\n",
    "print('Geodatum and Grid systems for IC:')\n",
    "df_explo[['Name', 'Geodatum', 'Grid system']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>QC - check and re-order well headers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out attributes lists, reflecting all the changes above.\n",
    "# Use these lists to check the current order of your columns in each, and consider how you might like to re-order them.\n",
    "# Any columns created above (including: Country, Status, Grid system) currently appear at the end of the lists.\n",
    "\n",
    "print('--- BEFORE RE-ORDERING ---\\n')\n",
    "print(len(df_explo.columns), 'Exploration attributes:\\n', list(df_explo.columns), '\\n')\n",
    "print(len(df_dev.columns), 'Development attributes:\\n', list(df_dev.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Re-order all columns (OPTIONAL)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specifies the order of columns for Exploration and Development wells in the final outputs.\n",
    "# # It's not compulsory to re-order columns, as IC lists all non-default attributes alphabetically.\n",
    "\n",
    "# explo_order = ['Name', 'Alternate 1', 'UWI number', 'Quadrant', 'Block', 'Operator', 'Licence number', 'Intent', \n",
    "#                 'Intent - planned', 'Well status', 'Well content', 'Status', 'Type', 'Subsea', 'SPUD date', \n",
    "#                 'Completion date', 'Field', 'Drill permit', 'Discovery name', 'Discovery wellbore', \n",
    "#                 'Bottom hole temperature [째C]', 'Seismic line', 'Maximum inclination [째]', 'KBE', \n",
    "#                 'Final vertical depth (TVD) [m RKB]', 'Terminal depth', 'Water depth', 'Kick off point [m RKB]', \n",
    "#                 'Oldest penetrated age', 'Oldest penetrated formation', 'Location', 'Country', 'Facility', \n",
    "#                 'Drilling facility type', 'Drilling facility category', 'Licensing activity awarded in', \n",
    "#                 'Multilateral', 'Entry year', 'Completed year', 'Reclassified from/to wellbore', 'Reentry activity', \n",
    "#                 'Plot symbol', '1st level with HC formation', '1st level with HC age', '2nd level with HC formation', \n",
    "#                 '2nd level with HC age', '3rd level with HC formation', '3rd level with HC age', 'Drilling days', \n",
    "#                 'Reentry', 'Geodatum', 'Latitude', 'Longitude', 'Surface X', 'Surface Y', 'UTM zone', 'Grid system', \n",
    "#                 'DISKOS Well Type', 'DISKOS Wellbore Parent', \n",
    "#                 'Publication date', 'Release date', 'NPDID wellbore', 'NPDID discovery', 'NPDID field', \n",
    "#                 'NPDID drilling facility', 'NPDID wellbore reclassified from', 'NPDID production licence drilled in', \n",
    "#                 'Date main level updated', 'Date all updated', 'Date sync NPD']\n",
    "\n",
    "# dev_order = ['Name', 'Alternate 1', 'UWI number', 'Quadrant', 'Block', 'Operator', 'Licence number', 'Intent', \n",
    "#               'Intent - planned', 'Well status', 'Well content',  'Status', 'Content - planned', 'Type', 'Subsea',\n",
    "#               'SPUD date', 'Completion date', 'Field', 'Predrilled entry date','Predrilled completion date', \n",
    "#               'Drill permit', 'Discovery name', 'Discovery wellbore', 'KBE', 'Final vertical depth (TVD) [m RKB]',\n",
    "#               'Terminal depth', 'Water depth', 'Kick off point [m RKB]', 'Location', 'Country', 'Facility', \n",
    "#               'Drilling facility type', 'Drilling facility category', 'Licensing activity awarded in', \n",
    "#               'Production facility', 'Multilateral', 'Entry year', 'Completed year','Reclassified from/to wellbore', \n",
    "#               'Plot symbol', 'Geodatum', 'Latitude', 'Longitude', 'Surface Y', 'Surface X', 'UTM zone',  'Grid system', \n",
    "#               'DISKOS Well Type', 'DISKOS Wellbore Parent', 'NPDID wellbore', \n",
    "#               'NPDID discovery', 'NPDID field', 'Publication date', 'Release date', 'NPDID production licence drilled in', \n",
    "#               'NPDID drilling facility', 'NPDID production facility','NPDID wellbore reclassified from', \n",
    "#               'Date main level updated', 'Date all updated', 'Date sync NPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if your list of re-ordered attributes is complete.\n",
    "# missing_explo = set(df_explo.columns).difference(explo_order)\n",
    "# missing_dev = set(df_dev.columns).difference(dev_order)\n",
    "\n",
    "# if len(missing_explo) > 0:\n",
    "#     print('Your re-ordered list of Exploration attributes is incomplete. You must include:\\n {}.\\n'.format(missing_explo))\n",
    "# else:\n",
    "#     print('Your re-ordered list of Exploration attributes is complete.\\n')\n",
    "    \n",
    "# if len(missing_dev) > 0:\n",
    "#     print('Your re-ordered list of Development attributes is incomplete. You must include:\\n {}.'.format(missing_dev))\n",
    "# else:\n",
    "#     print('Your re-ordered list of Development attributes is complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only when your re-ordered lists of Exploration and Development attributes are complete should you run this cell,\n",
    "# # Otherwise these will not be included in the output file!\n",
    "# # Applies the re-ordering to the dataframes\n",
    "\n",
    "# df_explo = df_explo.reindex(columns=explo_order)\n",
    "# df_dev = df_dev.reindex(columns=dev_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>QC column values</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all unique values for selected attributes (example: Operator and Field)\n",
    "\n",
    "def lstheaderfields (*args):\n",
    "    for arg in args:\n",
    "        print('---' , arg, '---')\n",
    "        print('')\n",
    "        words = [x for x in df_explo[arg].unique()]\n",
    "        print('Exploration wells:')\n",
    "        print(words)\n",
    "        print('')\n",
    "        words = [x for x in df_dev[arg].unique()]\n",
    "        print('Development wells:')\n",
    "        print(words)\n",
    "        print(\"\")\n",
    "        \n",
    "# Enter the names of columns you would like to check\n",
    "lstheaderfields('Operator', 'Field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>CREATE FILES - create well headers for exploration and development wells</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs CSV files for Exploration and Development well headers.\n",
    "# If Development dataframe contains more than 3000 wells, split in two for easier handling in IC.\n",
    "\n",
    "# Output filenames\n",
    "file_explo_all = 'output data/IC_wellbore_exploration_all.csv'\n",
    "file_dev_all = 'output data/IC_wellbore_development_all.csv'\n",
    "\n",
    "df_explo.to_csv(file_explo_all, encoding='utf-8-sig', index=False)\n",
    "print('Created file: {}, which includes {} wells from {} to {}.'.format(file_explo_all, len(df_explo), \n",
    "                                                                        df_explo['Name'][df_explo.index[0]], df_explo['Name'][df_explo.index[-1]]))\n",
    "\n",
    "df_dev.to_csv(file_dev_all, encoding='utf-8-sig', index=False)\n",
    "print('Created file: {}, which includes {} wells from {} to {}.'.format(file_dev_all, len(df_dev), \n",
    "                                                                        df_dev['Name'][df_dev.index[0]], df_dev['Name'][df_dev.index[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Well Attributes to create in IC</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following attributes are not IC defaults and need to be created under Wells > Attributes.\n",
    "# Alternatively, use the SQL code produced in the next cell to create these rows in SSMS. \n",
    "\n",
    "# Find the full list of attributes after all the editing you've done above\n",
    "all_attributes = set(list(df_explo.columns) + list(df_dev.columns))\n",
    "\n",
    "# Find and count those attributes you'll need to create in IC\n",
    "non_default_attributes = list(set(all_attributes).difference(ic_default_attributes))\n",
    "non_default_attributes.sort()\n",
    "num_non_default_attributes = len(non_default_attributes)\n",
    "\n",
    "print('The following {} attributes are not IC defaults and must be added to IC:\\n'.format(num_non_default_attributes))\n",
    "print(list(non_default_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you have database administration privileges, you can use this cell to generate the SQL Query code that will create Well Attributes in IC in the format:\n",
    "    #INSERT INTO t_WellsUserFields (f_FieldId, f_FieldName, f_IsInputUsed, f_InputID, f_Description, f_Origin, f_SortOrder)\n",
    "    #VALUES (1, 'Attribute', 'False', 0, 'Description of attribute', 0, 1);\n",
    "# This assumes you have no yet created any Well Attributes in IC. If you have already, you'll need to tweak the 3 variables below.\n",
    "\n",
    "pk_index = 0 #Enter one less than your highest pk_index\n",
    "original_pk_index = 0 #Enter the same number as above (this one we won't change)\n",
    "f_sortorder = 0 #Enter the next appropriate f_sortorder\n",
    "\n",
    "print(\"INSERT INTO t_WellsUserFields\")\n",
    "print(\"  (f_FieldId, f_FieldName, f_IsInputUsed, f_InputID, f_Description, f_Origin, f_SortOrder)\")\n",
    "print(\"VALUES\")\n",
    "\n",
    "for i in non_default_attributes:\n",
    "    pk_index += 1\n",
    "    f_sortorder += 1\n",
    "    if pk_index < (num_non_default_attributes + original_pk_index):\n",
    "        print(\"  ({x}, '{y}', 'False', 0, 'Userfield {y}', 0, {z}),\".format(x = pk_index, y = i, z = f_sortorder))\n",
    "    else:\n",
    "        print(\"  ({x}, '{y}', 'False', 0, 'Userfield {y}', 0, {z});\".format(x = pk_index, y = i, z = f_sortorder))\n",
    "\n",
    "# Follow these steps:\n",
    "# 1. Open your IC database in SQL Server Management Studio. IC must be closed/computer restarted to open a LocalDB in SSMS.\n",
    "# 2. Expand 'Tables', scroll down to 't_WellsUserFields' and right-click 'Edit Top 200 Rows'.\n",
    "# 3. Press Ctrl+N to create a new query, copy and paste the following SQL code to the blank query and hit F5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ensure the correct co-ordinate systems are added to your IC project</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In IC, open Project > Properties > Coords > Coordinate Systems\n",
    "# Ensure each of the following co-ordinate system are installed **before importing well headers**\n",
    "\n",
    "lstfield = sorted(set(df_explo['Geodatum'].astype(str)))\n",
    "print('Geodatum:', ', '.join(lstfield))\n",
    "\n",
    "lstfield = sorted(set(df_explo['Grid system'].astype(str)))\n",
    "print('Grid systems:', ', '.join(lstfield))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import the data to IC</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before importing data to IC, ensure you follow the last few steps to:\n",
    "# - Create the appropriate Well Attributes in your IC Database.\n",
    "# - Add the correct coordinate systems to your IC Project.\n",
    "\n",
    "# Import reference files via Import > Well References\n",
    "# Import well headers via Import > Headers.\n",
    "\n",
    "# Note that, while the well header data imports very quickly, \n",
    "# IC is a bit slow to create the wells if they don't already exist. Patience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_explo.shape)\n",
    "print(df_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Core (Cored Intervals)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core = pd.read_excel('input data/wellbore_core.xlsx', sheet_name='wellbore_core')\n",
    "df_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core['Core sample depth - uom'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core.iterrows():\n",
    "    if row['Core sample depth - uom'] == '[ft  ]':\n",
    "        df_core.loc[index, 'Top depth'] = (row['Core sample - top depth'] * 0.3048)\n",
    "    else:\n",
    "        df_core.loc[index, 'Top depth'] = row['Core sample - top depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core.iterrows():\n",
    "    if row['Core sample depth - uom'] == '[ft  ]':\n",
    "        df_core.loc[index, 'Base depth'] = (row['Core sample -  bottom depth'] * 0.3048)\n",
    "    else:\n",
    "        df_core.loc[index, 'Base depth'] = row['Core sample -  bottom depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core.dtypes\n",
    "#Note extra space in 'Core sample -  bottom depth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_core['Thickness'] = (df_core['Base depth'] - df_core['Top depth'])\n",
    "\n",
    "#filt = (df_core['Wellbore'] == '1/2-1') | (df_core['Wellbore'] == '1/3-1') | (df_core['Wellbore'] == '31/2-17 A') \n",
    "\n",
    "# df_core[['Wellbore', 'Core sample number', 'Core sample - top depth', 'Core sample -  bottom depth', \n",
    "#           'Total core sample length [m]', 'Top depth', 'Base depth']].loc[filt].round(2)\n",
    "\n",
    "df_core = df_core[['Wellbore', 'Top depth', 'Base depth', 'Core sample number']].round(2)\n",
    "df_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Core sample number': 'Legend'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_core.rename(columns=rename_cols, inplace=True)\n",
    "df_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_core.drop(labels=['Core sample - top depth',\n",
    "#                       'Core sample -  bottom depth',\n",
    "#                       'Core sample depth - uom',\n",
    "#                       'NPDID wellbore', \n",
    "#                       'Date updated', \n",
    "#                       'Date sync NPD'], axis=1, inplace=True)\n",
    "# df_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output df_documents to file\n",
    "file = 'output data/wellbore_core.csv'\n",
    "df_core.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Core Photos</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_core_photo = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_core_photo&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.129.189&CultureCode=en', sheet_name='wellbore_core_photo')\n",
    "\n",
    "df_core_photo = pd.read_excel('input data/wellbore_core_photo.xlsx', sheet_name='wellbore_core_photo')\n",
    "\n",
    "df_core_photo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://pythex.org/\n",
    "\n",
    "# Match patterns:\n",
    "# 10208-10228ft\n",
    "# 1802-1805m\n",
    "\n",
    "pat = '\\d{3,5}-\\d{3,5}\\D{1,2}'\n",
    "    \n",
    "#filt = df_core_photo['Core photo title'].str.extract(pat)\n",
    "filt = df_core_photo['Core photo title'].str.contains(pat)\n",
    "\n",
    "# Check rows that match pattern\n",
    "df_core_photo[filt].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows that do not match pattern and make corrections\n",
    "df_core_photo[~filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note 95 rows with erroneous values\n",
    "# Apply obvious corrections then drop the rest.\n",
    "\n",
    "# Values for well 2/4-X-47 are obviously in ft.\n",
    "filt_correction = df_core_photo['Wellbore'] == '2/4-X-47'\n",
    "df_core_photo.loc[filt_correction, 'Core photo title'] = (df_core_photo['Core photo title'] + 'm')\n",
    "df_core_photo.loc[filt_correction]\n",
    "\n",
    "# There are other obvious corrections to be made, but leave for now.\n",
    "# Example below, but don't do this on .loc as index may as more wells added.\n",
    "\n",
    "#['Core photo title'].loc[14234] = '2482-2483m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign rows that do not match pattern to new dataframe\n",
    "\n",
    "df_core_photo_deletedrows = df_core_photo[~filt]\n",
    "\n",
    "df_core_photo_deletedrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows that do match pattern\n",
    "# Dumps the rest (e.g. '2044', 'Core 2')\n",
    "\n",
    "df_core_photo = df_core_photo[filt]\n",
    "print(df_core_photo.shape)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_core_photo.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "df_core_photo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: REPLACE AFTER SPLITTING OUT?????????? \n",
    "\n",
    "df_core_photo['Core photo title'].replace({'mj': 'm', #one erroneous 'mj' value\n",
    "                                           'n': ',m', #one erroneous 'n' value\n",
    "                                           'm': ',m', #then replace all 'm'\n",
    "                                           'M': ',m',\n",
    "                                           'ft': ',ft',\n",
    "                                           'FT': ',ft'\n",
    "                                          }, regex=True, inplace=True)\n",
    "\n",
    "df_core_photo['Core photo title'].replace({'-': ','}, regex=True, inplace=True)\n",
    "\n",
    "df_core_photo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo[['Top depth', 'Base depth', 'Unit']] = df_core_photo['Core photo title'].str.split(pat=',', n=2, expand=True)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo['Unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that contain nulls\n",
    "df_core_photo.isna().sum()\n",
    "\n",
    "#1 extra well with no unit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core_photo.iterrows():\n",
    "    if row['Unit'] == 'ft':\n",
    "        df_core_photo.loc[index, 'Top depth'] = int(row['Top depth']) * 0.3048\n",
    "    else:\n",
    "        df_core_photo.loc[index, 'Top depth'] = int(row['Top depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_core_photo.iterrows():\n",
    "    if row['Unit'] == 'ft':\n",
    "        df_core_photo.loc[index, 'Base depth'] = int(row['Base depth']) * 0.3048\n",
    "    else:\n",
    "        df_core_photo.loc[index, 'Base depth'] = int(row['Base depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo.round(2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://stackabuse.com/download-files-with-python/\n",
    "# Using the request Module\n",
    "\n",
    "# Would also be useful to create folders for each wellbore\n",
    "\n",
    "def save_core_photo():\n",
    "    \n",
    "    for index, row in df_core_photo_deletedrows.iterrows(): # Using filterered dataframe for speed\n",
    "        \n",
    "        url = row['Core photo URL']\n",
    "        filename = url.split('/')[-1]\n",
    "        filepath = 'core_photo_jpgs\\\\'\n",
    "\n",
    "        print('Beginning file download with requests: ', url)\n",
    "        r = requests.get(url)\n",
    "        \n",
    "        with open('{}/{}'.format(filepath, filename), 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print('Saved to: {}/{}'.format(filepath, filename))\n",
    "\n",
    "        # Retrieve HTTP meta-data\n",
    "        print(r.status_code)\n",
    "        print(r.headers['content-type'])\n",
    "        print(r.encoding)\n",
    "        \n",
    "########################################################################################save_core_photo()\n",
    "\n",
    "# Sometimes getting error: \n",
    "# SSLError: HTTPSConnectionPool(host='factpages.npd.no', port=443): \n",
    "# Max retries exceeded with url: /pbl/core_photo_jpgs/3279_06_2044_2049m.jpg \n",
    "# (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)\",),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the filepath, e.g. '.\\3279_06_2044_2049m.jpg'\n",
    "# Where . represents the current directory\n",
    "\n",
    "# Escaped insert backslash\n",
    "df_core_photo['Legend'] = '.\\\\' + df_core_photo['Core photo URL'].str.split('/').str[-1]\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well'}\n",
    "\n",
    "# Apply renaming\n",
    "df_core_photo.rename(columns=rename_cols, inplace=True)\n",
    "df_core_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_core_photo = df_core_photo[['Well', 'Top depth', 'Base depth', 'Legend']]\n",
    "df_core_photo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_core_photo.csv'\n",
    "df_core_photo.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Thin Sections</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_thin_section = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_thin_section&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "df_thin_section = pd.read_excel('input data/wellbore_thin_section.xlsx', sheet_name='wellbore_thin_section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_thin_section.shape)\n",
    "df_thin_section.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section['Unit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_thin_section.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_thin_section.iterrows():\n",
    "    if row['Unit'] == '[ft  ]':\n",
    "        df_thin_section.loc[index, 'Depth'] = row['Depth'] * 0.3048\n",
    "    else:\n",
    "        df_thin_section.loc[index, 'Depth'] = row['Depth']\n",
    "        \n",
    "df_thin_section.drop(columns='Unit', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_thin_section.iterrows():\n",
    "    df_thin_section.loc[index, 'Legend'] = 'Thin section no. ' + str(row['Number'])\n",
    "    \n",
    "df_thin_section.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_thin_section.rename(columns=rename_cols, inplace=True)\n",
    "df_thin_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thin_section = df_thin_section[['Well', 'Depth', 'Legend']]\n",
    "df_thin_section = df_thin_section.round(2)\n",
    "df_thin_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output df_documents to file\n",
    "file = 'output data/wellbore_thin_section.csv'\n",
    "df_thin_section.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point - comment\n",
    "# No new IC data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CO2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_co2 = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_co2&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en',\n",
    "#                      skiprows=[0])\n",
    "\n",
    "df_co2 = pd.read_excel('input data/wellbore_co2.xlsx', sheet_name='wellbore_co2', skiprows=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_co2.shape)\n",
    "df_co2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_co2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.drop(labels=['Sample method', 'NPDID wellbore', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore name' : 'Well',\n",
    "               'Sample top depth [m]' : 'Top depth',\n",
    "               'Sample bottom depth [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_co2.rename(columns=rename_cols, inplace=True)\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_co2.csv'\n",
    "df_co2.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval-point\n",
    "# Create data types: 'Sample sequence number', 'CO2 [vol %]', 'Sample type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Oil Samples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_oil_sample = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_oil_sample&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en',\n",
    "#                      skiprows=[0])\n",
    "\n",
    "df_oil_sample = pd.read_excel('input data/wellbore_oil_sample.xlsx', sheet_name='wellbore_oil_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_oil_sample.shape)\n",
    "df_oil_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_oil_sample.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.drop(labels=['NPDID wellbore', 'Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Top depth MD [m]' : 'Top depth',\n",
    "               'Bottom depth MD [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_oil_sample.rename(columns=rename_cols, inplace=True)\n",
    "df_oil_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_oil_sample.csv'\n",
    "df_oil_sample.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oil_sample.columns\n",
    "\n",
    "# Interval - point\n",
    "# Create data types: 'Test type', 'Bottle number', 'Fluid type', 'Test time ', 'Received date'\n",
    "\n",
    "# What to do about rows with only with 0/Nan values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lithostratigraphy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lithostrat available in two places. \n",
    "# Compare the length, and number of unique wells in both sources.\n",
    "\n",
    "# (A) NPD FactPages > Wellbore > Table View > With > Lithostratigraphy\n",
    "    # File: wellbore_formation_top.xlsx\n",
    "    # Sheet: wellbore_formation_top\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_a = pd.read_excel('input data/wellbore_formation_top.xlsx', sheet_name='wellbore_formation_top')\n",
    "print('Source A:', df_a.shape)\n",
    "print(df_a['Wellbore name'].nunique())\n",
    "\n",
    "# (B) NPD FactPages > Stratigraphy > Table View > Wellbores\n",
    "    # File: strat_litho_wellbore.xlsx\n",
    "    # Sheet: strat_litho_wellbore\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/strat_litho_wellbore&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_b = pd.read_excel('input data/strat_litho_wellbore.xlsx', sheet_name='strat_litho_wellbore')\n",
    "print('Source B:', df_b.shape)\n",
    "print(df_b['Wellbore name'].nunique())\n",
    "\n",
    "# Both contain (almost) the same number of rows.\n",
    "# Source A is preferrable as it has an exra column, 'Lithostrat. unit, parent'\n",
    "# which will come in handy assigning parents to each text dictionary entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Source A\n",
    "\n",
    "# df_formation_top = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en', \n",
    "#                          sheet_name='wellbore_formation_top')\n",
    "\n",
    "df_lithostrat = pd.read_excel('input data/wellbore_formation_top.xlsx', sheet_name='wellbore_formation_top')\n",
    "\n",
    "# Print column titles\n",
    "print(\"Lithostratigraphy wellbore well header column titles:\")\n",
    "print(list(df_lithostrat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_lithostrat_rows, num_lithostrat_cols) = df_lithostrat.shape\n",
    "print('{} rows and {} columns in Exploration wells.'.format(num_lithostrat_rows, num_lithostrat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for csv\n",
    "rename_stratcols = {'Wellbore name' : 'Well',\n",
    "                    'Top depth [m]' : 'Top depth',\n",
    "                    'Bottom depth [m]' : 'Base depth',\n",
    "                    'Lithostrat. unit' : 'Legend'\n",
    "                    }\n",
    "\n",
    "#Apply renaming to dataframe\n",
    "df_lithostrat.rename(columns=rename_stratcols, inplace=True)\n",
    "\n",
    "# Create new dataframe called \"df_formation_top\"\n",
    "# Need to keep other columns df_lithostrat for later when writing to database\n",
    "\n",
    "df_formation_top = df_lithostrat[['Well', 'Top depth', 'Base depth', 'Legend']]\n",
    "df_formation_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_formation_top.csv'\n",
    "df_formation_top.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Drill stem tests</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dst = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_dst&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "df_dst = pd.read_excel('input data/wellbore_dst.xlsx', sheet_name='wellbore_dst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_dst.shape)\n",
    "df_dst.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dst.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dst.drop(labels=['NPDID wellbore', 'Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dst.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Rename well header columns to match dbo.WELLS (does capitalisation matter?)\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'From depth MD [m]' : 'Top depth',\n",
    "               'To depth MD [m]' : 'Base depth'\n",
    "               }\n",
    "    \n",
    "#Apply renaming\n",
    "df_dst.rename(columns=rename_cols, inplace=True)\n",
    "df_dst.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_dst.csv'\n",
    "df_dst.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note overlapping depths in DST data\n",
    "# For example in well 1/9-1\n",
    "\n",
    "filt = df_dst['Well'] == '1/9-1'\n",
    "df_dst[['Well', 'Top depth', 'Base depth']][filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate overlap in Drill Stem Tests\n",
    "# Zero overlap for first test in each well, no negative overlaps (representing gaps)\n",
    "# Iterate over DataFrame rows as (index, Series) pairs.\n",
    "\n",
    "for index, row in df_dst[1:].iterrows():\n",
    "    \n",
    "    current_row = df_dst.loc[index]\n",
    "    last_row = df_dst.iloc[df_dst.index.get_loc(index) - 1]\n",
    "    \n",
    "    # Zero overlap for first test in each well\n",
    "    if current_row['Well'] != last_row['Well']:\n",
    "        df_dst.loc[index, 'Overlap'] = 0\n",
    "    \n",
    "    else:\n",
    "        # Difference between base of last row and top of current row\n",
    "        if current_row['Top depth'] < last_row['Base depth']:\n",
    "            df_dst.loc[index, 'Overlap'] = last_row['Base depth'] - current_row['Top depth']\n",
    "        else:\n",
    "            df_dst.loc[index, 'Overlap'] = 0 \n",
    "\n",
    "df_dst_overlap = df_dst[['Well', 'Test number', 'Top depth', 'Base depth', 'Overlap']].round(1)\n",
    "\n",
    "# Output to file\n",
    "file = \"output data/calc_dst_overlap.xlsx\"\n",
    "df_dst_overlap.to_excel(file, index=False)\n",
    "pd.read_excel(file).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n",
    "                  'population': [1864, 22000, 80000]},\n",
    "                  index=['panda', 'polar', 'koala'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, content in df.items():\n",
    "    print('label:', label)\n",
    "    print('content:', content, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, content in df.iterrows():\n",
    "    print('label:', label)\n",
    "    print('content:', content, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Casing and leak-off tests</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_casinglot = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_casing_and_lot&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "df_casinglot = pd.read_excel('input data/wellbore_casing_and_lot.xlsx', sheet_name='wellbore_casing_and_lot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_casinglot.shape)\n",
    "df_casinglot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_casinglot.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casinglot.drop(labels=['NPDID wellbore', 'Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casinglot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Casing depth [m]' : 'Depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_casinglot.rename(columns=rename_cols, inplace=True)\n",
    "df_casinglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_casing_and_lot.csv'\n",
    "df_casinglot.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Drilling mud</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mud = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_mud&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en')\n",
    "\n",
    "df_mud = pd.read_excel('input data/wellbore_mud.xlsx', sheet_name='wellbore_mud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_mud.shape)\n",
    "df_mud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mud.drop(labels='Unnamed: 0', axis=1, inplace=True)\n",
    "df_mud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_mud.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mud.drop(labels=['NPDID wellbore', 'Date updated', 'Date sync NPD'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mud.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Depth MD [m]' : 'Depth'\n",
    "               }\n",
    "    \n",
    "# Apply renaming\n",
    "df_mud.rename(columns=rename_cols, inplace=True)\n",
    "df_mud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_mud.csv'\n",
    "df_mud.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Documents</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_corephotos = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_document&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.188&CultureCode=en')\n",
    "\n",
    "df_document = pd.read_excel('input data/wellbore_document.xlsx', sheet_name='wellbore_document')\n",
    "df_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document['Title'] = df_document['Wellbore'] + ' ' + df_document['Document type'] + ': ' + df_document['Document name'] + ' (' + df_document['Document format'] + ')'\n",
    "df_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document = df_document[['Wellbore', 'Title', 'Document URL']]\n",
    "df_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_document\n",
    "rename_cols = {'Wellbore' : 'Well',\n",
    "               'Document URL' : 'URL'\n",
    "               }\n",
    "    \n",
    "#Apply renaming\n",
    "df_document.rename(columns=rename_cols, inplace=True)\n",
    "df_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_references.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_explo_references\n",
    "rename_cols = {'Name' : 'Well',\n",
    "               }\n",
    "    \n",
    "#Apply renaming\n",
    "df_explo_references.rename(columns=rename_cols, inplace=True)\n",
    "df_explo_references.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine References and Documents dataframes\n",
    "df_refs_and_docs = df_explo_references.append(df_document) \n",
    "df_refs_and_docs.sort_values(['Well', 'Title'], ascending=[True, False], ignore_index=True, inplace=True)\n",
    "df_refs_and_docs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Output df_document to file\n",
    "file = 'output data/wellbore_references_and_documents.csv'\n",
    "df_refs_and_docs.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration well headers\n",
    "# Development well headers\n",
    "# Cores\n",
    "# Core photos\n",
    "# Thin sections\n",
    "# CO2\n",
    "# Oil samples\n",
    "# Lithostratigraphy\n",
    "# Drill stem tests\n",
    "# Casing and leak-off tests\n",
    "# Drilling mud\n",
    "# Documents\n",
    "\n",
    "df_explo_references.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame({'Data type':\n",
    "                        ['Exploration well headers',\n",
    "                         'Development well headers',\n",
    "                         'Exploration references',\n",
    "                         'Development references',\n",
    "                         'Cores',\n",
    "                         'Core photos',\n",
    "                         'Thin sections',\n",
    "                         'CO2',\n",
    "                         'Oil samples',\n",
    "                         'Lithostratigraphy',\n",
    "                         'Drill stem tests',\n",
    "                         'Casing and leak-off tests',\n",
    "                         'Drilling mud',\n",
    "                         'Documents',\n",
    "                         'Documents & References combined'\n",
    "                        ], \n",
    "                        'No. unique wells':\n",
    "                        [df_explo['Name'].nunique(),\n",
    "                         df_dev['Name'].nunique(),\n",
    "                         df_explo_references['Well'].nunique(),\n",
    "                         df_dev_references['Name'].nunique(),\n",
    "                         df_core['Well'].nunique(),\n",
    "                         df_core_photo['Well'].nunique(),\n",
    "                         df_thin_section['Well'].nunique(),\n",
    "                         df_co2['Well'].nunique(),\n",
    "                         df_oil_sample['Well'].nunique(),\n",
    "                         df_formation_top['Well'].nunique(),\n",
    "                         df_dst['Well'].nunique(),\n",
    "                         df_casinglot['Well'].nunique(),\n",
    "                         df_mud['Well'].nunique(),\n",
    "                         df_document['Well'].nunique(),\n",
    "                         df_refs_and_docs['Well'].nunique()\n",
    "                        ],\n",
    "                        'No. records':\n",
    "                        [df_explo['Name'].shape[0],\n",
    "                         df_dev['Name'].shape[0],\n",
    "                         df_explo_references['Well'].shape[0],\n",
    "                         df_dev_references['Name'].shape[0],\n",
    "                         df_core['Well'].shape[0],\n",
    "                         df_core_photo['Well'].shape[0],\n",
    "                         df_thin_section['Well'].shape[0],\n",
    "                         df_co2['Well'].shape[0],\n",
    "                         df_oil_sample['Well'].shape[0],\n",
    "                         df_formation_top['Well'].shape[0],\n",
    "                         df_dst['Well'].shape[0],\n",
    "                         df_casinglot['Well'].shape[0],\n",
    "                         df_mud['Well'].shape[0],\n",
    "                         df_document['Well'].shape[0],\n",
    "                         df_refs_and_docs['Well'].shape[0]]\n",
    "                       })\n",
    "\n",
    "print('Data for', (df_explo['Name'].nunique()+df_dev['Name'].nunique()), 'wells in total.')\n",
    "\n",
    "#Add thousands separators\n",
    "df_summary['No. unique wells'] = df_summary['No. unique wells'].apply(lambda x : \"{:,}\".format(x))\n",
    "df_summary['No. records'] = df_summary['No. records'].apply(lambda x : \"{:,}\".format(x))\n",
    "df_summary = df_summary.style.hide_index()\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NPD Shapefiles</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPD shapefiles at https://www.npd.no/en/about-us/information-services/available-data/map-services/\n",
    "\n",
    "npd_shapefiles = {\n",
    "    'AFEX': 'https://factpages.npd.no/downloads/shape/afxAreaCurrent.zip',\n",
    "    'AFEX_block': 'https://factpages.npd.no/downloads/shape/afxAreaSplitByBlock.zip',\n",
    "    'Licence': 'https://factpages.npd.no/downloads/shape/prlAreaCurrent.zip',\n",
    "    'Licence_block': 'https://factpages.npd.no/downloads/shape/prlAreaSplitByBlock.zip',\n",
    "    'Licencing APA': 'https://factpages.npd.no/downloads/shape/apaAreaGross.zip',\n",
    "    'Licencing APA_block': 'https://factpages.npd.no/downloads/shape/apaAreaNet.zip',\n",
    "    'Wellbore': 'https://factpages.npd.no/downloads/shape/wlbPoint.zip',\n",
    "    #Ignore Wellbore - Fontfile for presentation TTF\n",
    "    'BAA': 'https://factpages.npd.no/downloads/shape/baaAreaCurrent.zip',\n",
    "    'BAA_block': 'https://factpages.npd.no/downloads/shape/baaAreaSplitByBlock.zip',\n",
    "    'Field': 'https://factpages.npd.no/downloads/shape/fldArea.zip',\n",
    "    'Discovery': 'https://factpages.npd.no/downloads/shape/dscArea.zip',\n",
    "    'Facility': 'https://factpages.npd.no/downloads/shape/fclPoint.zip',\n",
    "    'Survey': 'https://factpages.npd.no/downloads/shape/seaArea.zip',\n",
    "    'TUF': 'https://factpages.npd.no/downloads/shape/pipLine.zip',\n",
    "    'Block': 'https://factpages.npd.no/downloads/shape/blkArea.zip',\n",
    "    'Quadrant': 'https://factpages.npd.no/downloads/shape/qadArea.zip',\n",
    "    'Sub area': 'https://factpages.npd.no/downloads/shape/subArea.zip'\n",
    "}\n",
    "\n",
    "for key, value in npd_shapefiles.items(): \n",
    "    print(value)\n",
    "\n",
    "#print(npd_shapefiles.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
    "# https://factpages.npd.no/downloads/shape/afxAreaCurrent.zip\n",
    "\n",
    "def save_shapefiles():\n",
    "    \n",
    "    for key, value in npd_shapefiles.items(): \n",
    "        \n",
    "        filepath = 'shapefiles\\\\'\n",
    "        zip_file_url = value\n",
    "\n",
    "        print('Beginning file download with requests: ', zip_file_url)\n",
    "        r = requests.get(zip_file_url)\n",
    "\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(filepath)\n",
    "\n",
    "        print('Files extracted to: {}'.format(filepath))\n",
    "        \n",
    "save_shapefiles()\n",
    "\n",
    "# Import shapefile to IC?\n",
    "# Populate colours, e.g. Fields and Discoveries with OGW colours?\n",
    "\n",
    "# Error:\n",
    "# SSLError: HTTPSConnectionPool(host='factpages.npd.no', port=443): \n",
    "# Max retries exceeded with url: /downloads/shape/afxAreaCurrent.zip \n",
    "# (Caused by SSLError(SSLError(\"bad handshake: \n",
    "# Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)\",),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2: Prepare data for import to SQL Server</h1><br>\n",
    "May split the following out into separate module later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from xlrd import xldate\n",
    "import pyodbc\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, Table, delete, insert, select, func, sql\n",
    "from sqlalchemy.types import Integer\n",
    "import urllib\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Well Headers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All column titles in dbo.WELLS table\n",
    "\n",
    "# IS THIS USED???\n",
    "# ic_dbowells_columns = {\"pk_index\", \"well_id\", \"units\", \"created\", \"creator\", \"modified\", \"modifier\", \"project\", \n",
    "#                           \"rte\", \"sea_bed\", \"rig_elevation\", \"datum\", \"terminal_depth\", \"spud_date\", \"completion_date\", \n",
    "#                           \"quadrant\", \"sub_block\", \"kelly\", \"symbol_id\", \"client\", \"utmzone\", \"code\", \"name\", \n",
    "#                           \"field\", \"location\", \"country\", \"basin\", \"name1\", \"name2\", \"strat_schemes\", \"grnd_elev\", \n",
    "#                           \"f_block\", \"grid_x\", \"grid_y\", \"latitude\", \"longtitude\", \"geodatum\", \"facility\", \n",
    "#                           \"discovery_name\", \"seismic_line\", \"intent\", \"f_ipid\", \"f_licenceNumber\", \"f_api\", \n",
    "#                           \"f_comment\", \"f_province\", \"f_county\", \"f_state\", \"f_section\", \"f_Township\", \"f_range\", \"f_uwi\"}\n",
    "\n",
    "# Rename well header columns to match dbo.WELLS (does capitalisation matter?)\n",
    "rename_for_sql = {'Name' : 'name',\n",
    "                'Alternate 1' : 'name1',\n",
    "                'Operator' : 'client',\n",
    "                'Licence number' : 'f_licenceNumber',\n",
    "                'Intent' : 'intent',\n",
    "                'Field' : 'field',\n",
    "                'SPUD date' : 'spud_date',\n",
    "                'Completion date' : 'completion_date',\n",
    "                'Discovery name' : 'discovery_name',\n",
    "                'Seismic line' : 'seismic_line',\n",
    "                'Country' : 'country',\n",
    "                'KBE' : 'kelly',\n",
    "                'Terminal depth' : 'terminal_depth',\n",
    "                'Water depth' : 'sea_bed',\n",
    "                'Location' : 'location',\n",
    "                'Facility' : 'facility',\n",
    "                'Geodatum' : 'geodatum',\n",
    "                'Latitude' : 'latitude',\n",
    "                'Longitude' : 'longtitude', #spelled incorrectly to match column longtitude in dbo.WELLS!\n",
    "                'Grid system' : 'utmzone',\n",
    "                'Surface X' : 'grid_x',\n",
    "                'Surface Y' : 'grid_y',\n",
    "                'Quadrant' : 'quadrant',\n",
    "                'Block' : 'f_block'}\n",
    "    \n",
    "# Apply renaming to each of the dataframes\n",
    "df_explo.rename(columns=rename_for_sql, inplace=True)\n",
    "df_dev.rename(columns=rename_for_sql, inplace=True)\n",
    "\n",
    "# QC renamed columns\n",
    "print(\"Renamed attributes only:\")\n",
    "renamed_columns = list(rename_for_sql.values())\n",
    "df_explo[renamed_columns].head(n=3)\n",
    "# df_dev[renamed_columns].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now\n",
    "\n",
    "timestampStr = now.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "print('Current Timestamp : ', timestampStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this function to created and modified columns, then push to_sql.\n",
    "\n",
    "def datetime2ole(date):\n",
    "    #convert date string to a datetime object\n",
    "    date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    #Calculate OLE manually from OLE origin date\n",
    "    OLE_TIME_ZERO = datetime(1899, 12, 30)\n",
    "    delta = date - OLE_TIME_ZERO\n",
    "    return float(delta.days) + (float(delta.seconds) / 86400)  # 86,400 seconds in day\n",
    "\n",
    "now = datetime2ole(timestampStr)\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column(s) for dbo.WELLS but not in well header file\n",
    "\n",
    "df_explo['datum'] = 4\n",
    "df_dev['datum'] = 4\n",
    "df_explo['symbol_id'] = 3146 #correct later, refer to Status?\n",
    "df_dev['symbol_id'] = 3146 #correct later, refer to Status?\n",
    "df_explo['units'] = 'M'\n",
    "df_dev['units'] = 'M'\n",
    "df_explo['created'] = now\n",
    "df_dev['created'] = now\n",
    "df_explo['creator'] = 1 #correct later\n",
    "df_dev['creator'] = 1 #correct later\n",
    "# df_explo['modified'] = null\n",
    "# df_dev['modified'] = null\n",
    "# df_explo['modifier'] = null\n",
    "# df_dev['modifier'] = null\n",
    "# df_explo['project'] = null\n",
    "# df_dev['project'] = null\n",
    "\n",
    "# Check the result\n",
    "df_explo[['name', 'datum', 'symbol_id', 'units', 'created', 'creator']].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate columns, preserving the original\n",
    "df_explo['well_id'] = df_explo['NPDID wellbore']\n",
    "df_dev['well_id'] = df_dev['NPDID wellbore']\n",
    "\n",
    "df_explo['f_uwi'] = df_explo['NPDID wellbore']\n",
    "df_dev['f_uwi'] = df_dev['NPDID wellbore']\n",
    "\n",
    "df_explo['code'] = df_explo['name']\n",
    "df_dev['code'] = df_dev['name']\n",
    "\n",
    "# Limit seismic_line to match nvarchar(80) limit\n",
    "df_explo[\"seismic_line\"] = df_explo[\"seismic_line\"].str[:80]\n",
    "\n",
    "# Check the result\n",
    "df_explo[['name', 'well_id', 'f_uwi', 'code', 'seismic_line']].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and re-order explo_dbwells to match dbo.WELLS (minus those not required, listed below)\n",
    "\n",
    "explo_dbowells_order = [\"well_id\", \"units\", \"created\", \"creator\", \"modified\", \"modifier\", \"project\", \n",
    "                        \"sea_bed\", \"datum\", \"terminal_depth\", \"spud_date\", \"completion_date\", \"quadrant\", \"kelly\", \n",
    "                        \"symbol_id\", \"client\", \"utmzone\", \"code\", \"name\", \"field\", \"location\", \"country\", \"name1\", \n",
    "                        \"f_block\", \"grid_x\", \"grid_y\", \"latitude\", \"longtitude\", \"geodatum\", \"facility\", \"discovery_name\", \n",
    "                        \"seismic_line\", \"intent\", \"f_licenceNumber\", \"f_uwi\"]\n",
    "\n",
    "df_explo_dbowells = df_explo.filter(explo_dbowells_order)\n",
    "\n",
    "df_explo_dbowells = df_explo_dbowells.reindex(columns=explo_dbowells_order)\n",
    "\n",
    "df_explo_dbowells.head(1)\n",
    "\n",
    "\n",
    "# NOT REQUIRED\n",
    "#Columns in dbo.WELLS (correct order) that are not in df_explo_dbowells:\n",
    "#rte, rig_elevation, sub_block, basin, name2, strat_schemes, grnd_elev, \"f_ipid, f_api, f_comment, \n",
    "#f_province, f_county, f_state, f_section, f_Township, f_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and connect to SQL Server database\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=10)\n",
    "\n",
    "params = 'DRIVER={ODBC Driver 13 for SQL Server};' \\\n",
    "         'SERVER=5SQFPQ2\\SQLEXPRESS;' \\\n",
    "         'PORT=1433;' \\\n",
    "         'DATABASE=Test6;' \\\n",
    "         'Trusted_Connection=yes;'\n",
    "            \n",
    "params = urllib.parse.quote_plus(params)\n",
    "\n",
    "engine = create_engine('mssql+pyodbc:///?odbc_connect=%s' % params)\n",
    "\n",
    "metadata = MetaData()\n",
    "  \n",
    "#Create Table objects\n",
    "wells = Table('WELLS', metadata, autoload=True, autoload_with=engine)\n",
    "wellsuserfieldsvalues = Table('t_WellsUserFieldsValues', metadata, autoload=True, autoload_with=engine)\n",
    "wellsuserfields = Table('t_WellsUserFields', metadata, autoload=True, autoload_with=engine)\n",
    "datalithostrat = Table('DATA_Lithostrat', metadata, autoload=True, autoload_with=engine)\n",
    "projects = Table('PROJECTS', metadata, autoload=True, autoload_with=engine)\n",
    "wellqueries = Table('WELLQUERIES', metadata, autoload=True, autoload_with=engine)\n",
    "projectwells = Table('PROJECTWELLS', metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "connection = engine.connect()\n",
    "                       \n",
    "#pp.pprint(repr(wells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.t_wellsuserfieldsvalues and dbo.WELLS\n",
    "\n",
    "# Select statement\n",
    "stmt = select([func.count(wells.columns.name)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(projectwells)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Delete all data from t_wellsuserfieldsvalues table first due to dependency\n",
    "# See https://www.codeproject.com/Questions/677277/I-am-getting-error-while-delete-entry\n",
    "delete_stmt = delete(wellsuserfieldsvalues)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Delete all records from WELLS table\n",
    "delete_stmt = delete(wells)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())\n",
    "\n",
    "# TO DO: Tried to turn the above into a function to avoid repetition but the sqlalchemy select statement doesn't accept parameters.\n",
    "# See doc on https://stackoverflow.com/questions/19314342/python-sqlalchemy-pass-parameters-in-connection-execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_dbowells.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function \n",
    "def sqlselect_rows(tablename):\n",
    "    s = select(tablename)\n",
    "    result = connection.execute(s)\n",
    "    \n",
    "    #These two lines are equivalent to:\n",
    "    #result = engine.execute('SELECT * FROM PROJECTS')\n",
    "\n",
    "    for row in result:\n",
    "        print(row)\n",
    "    \n",
    "    result.close()\n",
    "    \n",
    "# Use sqlselect_rows([tablename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_dbowells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entire df_explo_dbwells to SQL Server database\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html\n",
    "\n",
    "df_explo_dbowells.to_sql('WELLS', engine, if_exists='append', index = False)\n",
    "\n",
    "print('dbo.WELLS')\n",
    "sqlselect_rows([wells])\n",
    "\n",
    "# Still to correct datum, symbol_id, creator. I think these might have to read other tables ()\n",
    "# Is well_id ok as npdid_wellbore? Will this cause any problems creating new wells in IC?\n",
    "\n",
    "# ProgrammingError: ('The SQL contains 2014 parameter markers, but 67550 parameters were supplied', 'HY000')\n",
    "# WELLS is referenced by PROJECTWELLS.well_id and t_WellsUserFieldsValues.well_id\n",
    "# Original script uses Append rather than replace, refer to Azure Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entire dbo.WELLS table\n",
    "# See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html#pandas.read_sql\n",
    "\n",
    "sql = '''\n",
    "SELECT *\n",
    "FROM dbo.WELLS\n",
    "'''\n",
    "\n",
    "pd.read_sql_query(sql, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up status ~ symbol_id from dbo.SYMBOLS\n",
    "# Status symbols correspond to the Well Symbols dictionary, dic_id 32003.\n",
    "df_wellsymbols = pd.read_sql('SELECT symbol_id, dic_id, description FROM dbo.SYMBOLS WHERE dic_id=32003', engine)\n",
    "df_wellsymbols[['symbol_id', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look user_id in dbo.userdef which is used in creator & modifier?\n",
    "# Status symbols correspond to the Well Symbols dictionary, dic_id 32003.\n",
    "df_users = pd.read_sql('SELECT user_id, usrid, name FROM dbo.userdef', engine)\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(non_default_attributes))\n",
    "print(non_default_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call DataFrame constructor on list of attributes\n",
    "df_wellsuserfields = pd.DataFrame(non_default_attributes, columns =['f_FieldName'])\n",
    "\n",
    "df_wellsuserfields['f_FieldID'] = range(1,len(df_wellsuserfields)+1)\n",
    "df_wellsuserfields['f_IsInputUsed'] = False\n",
    "df_wellsuserfields['f_InputID'] = 0\n",
    "df_wellsuserfields['f_Description'] = df_wellsuserfields['f_FieldName']\n",
    "df_wellsuserfields['f_Origin'] = 0\n",
    "df_wellsuserfields['f_SortOrder'] = range(1,len(df_wellsuserfields)+1)\n",
    "\n",
    "df_wellsuserfields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wellsuserfields.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.t_WellsUserFields\n",
    "\n",
    "stmt = select([func.count(wellsuserfields.columns.f_FieldName)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(wellsuserfields)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entire df_wellsuserfields to SQL Server database\n",
    "df_wellsuserfields.to_sql('t_WellsUserFields', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.t_WellsUserFields')\n",
    "sqlselect_rows([wellsuserfields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbo.t_WellsUserFieldsValues\n",
    "# Build dataframe of wellsuserfieldsvalues\n",
    "\n",
    "df_explo_nondefaultattributes = df_explo.filter(non_default_attributes)\n",
    "df_explo_nondefaultattributes['name'] = df_explo_dbowells['name']\n",
    "df_explo_nondefaultattributes['well_id'] = df_explo_dbowells['well_id']\n",
    "\n",
    "#df_explo_nondefaultattributes = df_explo_nondefaultattributes.reindex(columns=df_explo_nondefaultattributes)\n",
    "\n",
    "df_explo_nondefaultattributes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates in attributes appear as \"#2019-10-03 00:00:00.0000000\"\n",
    "# Reformat without time, but maintain datetime64[ns] data type.\n",
    "\n",
    "df_explo_nondefaultattributes['Date all updated'] = pd.to_datetime(df_explo_nondefaultattributes['Date all updated'].dt.strftime('%Y-%m-%d'))\n",
    "df_explo_nondefaultattributes['Date main level updated'] = pd.to_datetime(df_explo_nondefaultattributes['Date main level updated'].dt.strftime('%Y-%m-%d'))\n",
    "df_explo_nondefaultattributes['Publication date'] = pd.to_datetime(df_explo_nondefaultattributes['Publication date'].dt.strftime('%Y-%m-%d'))\n",
    "df_explo_nondefaultattributes['Release date'] = pd.to_datetime(df_explo_nondefaultattributes['Release date'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "# Date sync NPD appears as \"25.01.2020\", which is Python dtype('O') for Object.\n",
    "# df_explo_nondefaultattributes['Date sync NPD'].dtypes\n",
    "# Convert to datetime\n",
    "\n",
    "df_explo_nondefaultattributes['Date sync NPD'] = pd.to_datetime(df_explo_nondefaultattributes['Date sync NPD'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "#df_explo_nondefaultattributes.dtypes\n",
    "df_explo_nondefaultattributes[['Date all updated',\n",
    "                              'Date main level updated',\n",
    "                              'Publication date',\n",
    "                              'Release date',\n",
    "                              'Date sync NPD']]\n",
    "\n",
    "# Note: NaT is a pandas null value, pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explo_nondefaultattributes = pd.melt(df_explo_nondefaultattributes, id_vars='well_id')\n",
    "\n",
    "df_explo_nondefaultattributes.columns = ['f_WellId', 'f_FieldName', 'f_StringValue']\n",
    "\n",
    "#df_explo_nondefaultattributes.sort_values(by='f_WellId')\n",
    "\n",
    "df_explo_nondefaultattributes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wellsuserfieldsvalues = df_wellsuserfields.merge(df_explo_nondefaultattributes, on='f_FieldName', how='inner')\n",
    "df_wellsuserfieldsvalues = df_wellsuserfieldsvalues[['f_WellId', 'f_FieldID', 'f_StringValue']].sort_values(by=['f_WellId','f_FieldID'])\n",
    "df_wellsuserfieldsvalues\n",
    "\n",
    "# Why are there wells with f_WellId = NaN with left and right joins? Check original data?\n",
    "# Works fine with inner join but is this cutting out some wells?\n",
    "# Show 2 copies of this table, one with well names and one without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wellsuserfieldsvalues.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.t_WellsUserFields\n",
    "\n",
    "stmt = select([func.count(wellsuserfieldsvalues.columns.f_FieldID)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(wellsuserfieldsvalues)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entire df_wellsuserfieldsvalues to SQL Server database\n",
    "\n",
    "df_wellsuserfieldsvalues.to_sql('t_WellsUserFieldsValues', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.t_WellsUserFieldsValues')\n",
    "sqlselect_rows([wellsuserfieldsvalues])\n",
    "\n",
    "# WHERE IS WELL ID 1? CHECK THAT 8991 IS THE LAST????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT STATEMENT\n",
    "\n",
    "# # Build an insert statement to insert a record into the data table: insert_stmt\n",
    "# insert_stmt = insert(wells).values(well_id=55, name='9/9-15', created=now)\n",
    "\n",
    "# # Execute the insert statement via the connection: results\n",
    "# results = connection.execute(insert_stmt)\n",
    "\n",
    "# # Print result rowcount\n",
    "# print(results.rowcount)\n",
    "\n",
    "# # Build a select statement to validate the insert: select_stmt\n",
    "# select_stmt = select([wells]).where(wells.columns.name == '9/9-15')\n",
    "\n",
    "# # Print the result of executing the query.\n",
    "# print(connection.execute(select_stmt).first())\n",
    "\n",
    "# #repr(wells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lithostratigraphy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lithostrat available in two places. \n",
    "# Compare the length, and number of unique wells in both sources.\n",
    "\n",
    "# (A) NPD FactPages > Wellbore > Table View > With > Lithostratigraphy\n",
    "    # File: wellbore_formation_top.xlsx\n",
    "    # Sheet: wellbore_formation_top\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_a = pd.read_excel('input data/wellbore_formation_top.xlsx', sheet_name='wellbore_formation_top')\n",
    "print('Source A:', df_a.shape)\n",
    "print(df_a['Wellbore name'].nunique())\n",
    "\n",
    "# (B) NPD FactPages > Stratigraphy > Table View > Wellbores\n",
    "    # File: strat_litho_wellbore.xlsx\n",
    "    # Sheet: strat_litho_wellbore\n",
    "    # Link: https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/strat_litho_wellbore&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en\n",
    "\n",
    "df_b = pd.read_excel('input data/strat_litho_wellbore.xlsx', sheet_name='strat_litho_wellbore')\n",
    "print('Source B:', df_b.shape)\n",
    "print(df_b['Wellbore name'].nunique())\n",
    "\n",
    "# Both contain (almost) the same number of rows.\n",
    "# Source A is preferrable as it has an exra column, 'Lithostrat. unit, parent'\n",
    "# which will come in handy assigning parents to each text dictionary entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Source A\n",
    "\n",
    "# df_formation_top = pd.read_excel('https://factpages.npd.no/ReportServer_npdpublic?/FactPages/TableView/wellbore_formation_top&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&rs:Format=EXCEL&Top100=false&IpAddress=108.171.128.189&CultureCode=en', \n",
    "#                          sheet_name='wellbore_formation_top')\n",
    "\n",
    "df_lithostrat = pd.read_excel('input data/wellbore_formation_top.xlsx', sheet_name='wellbore_formation_top')\n",
    "\n",
    "# Print column titles\n",
    "print(\"Lithostratigraphy wellbore well header column titles:\")\n",
    "print(list(df_lithostrat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_lithostrat_rows, num_lithostrat_cols) = df_lithostrat.shape\n",
    "print('{} rows and {} columns in Exploration wells.'.format(num_lithostrat_rows, num_lithostrat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for csv\n",
    "rename_stratcols = {'Wellbore name' : 'Well',\n",
    "                    'Top depth [m]' : 'Top depth',\n",
    "                    'Bottom depth [m]' : 'Base depth',\n",
    "                    'Lithostrat. unit' : 'Legend'\n",
    "                    }\n",
    "\n",
    "# Apply renaming to dataframe\n",
    "df_lithostrat.rename(columns=rename_stratcols, inplace=True)\n",
    "\n",
    "# Create new dataframe called \"df_formation_top\"\n",
    "# Need to keep other columns df_lithostrat for later when writing to database\n",
    "\n",
    "df_formation_top = df_lithostrat[['Well', 'Top depth', 'Base depth', 'Legend']]\n",
    "df_formation_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to file\n",
    "file = 'output data/wellbore_formation_top.csv'\n",
    "df_formation_top.to_csv(file, index=False)\n",
    "\n",
    "# Check output\n",
    "pd.read_csv(file).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SQL Lithostratigraphy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME FOR DATABASE!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Rename only the 4 columns that will be imported to the database\n",
    "rename_stratcols = {'Top depth [m]' : 'top_depth',\n",
    "                'Bottom depth [m]' : 'base_depth',\n",
    "                'Lithostrat. unit' : 'legend',\n",
    "                'NPDID wellbore' : 'well_id'}\n",
    "\n",
    "# Apply renaming to dataframe\n",
    "df_lithostrat.rename(columns=rename_stratcols, inplace=True)\n",
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat.drop(labels=['well_id', 'NPDID lithostrat. unit', 'NPDID parent lithostrat. unit', 'Date updated', 'Date sync NPD'],\n",
    "                   axis=1, inplace=True)\n",
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbo.DATA_Lithostrat stores Gp, Fm and Mbrs.\n",
    "# Use Level column to create data_type column, numering Group (110), Formation (111) and Members ().\n",
    "\n",
    "def level_datatypes(row):\n",
    "    if row == 'GROUP':\n",
    "        return 110\n",
    "    elif row == 'FORMATION':\n",
    "        return 111\n",
    "    elif row == 'MEMBER':\n",
    "        return 112\n",
    "    else:\n",
    "        0\n",
    "\n",
    "df_lithostrat['data_type'] = df_lithostrat['Level'].apply(level_datatypes)\n",
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithostrat = df_lithostrat[['well_id', 'data_type', 'top_depth', 'base_depth', 'legend']]\n",
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new columns\n",
    "\n",
    "df_lithostrat['symbol_id'] = 0\n",
    "df_lithostrat['f_interpid'] = 0\n",
    "df_lithostrat['creator'] = 1\n",
    "df_lithostrat['modifier'] = 1\n",
    "df_lithostrat['source'] = 'Python script'\n",
    "df_lithostrat['attr'] = 'test' \n",
    "#'{\"ZoneColour\":-1,\"ZoneColourIsIpAuto\":true,\"EventSymbolId\":0,\"IsLocked\":false,\"OriginalZoneIndex\":0}'\n",
    "df_lithostrat['top_boundary'] = 0\n",
    "df_lithostrat['base_boundary'] = 0\n",
    "df_lithostrat['created '] = now\n",
    "df_lithostrat['modified'] = now\n",
    "df_lithostrat['obsno'] = 0\n",
    "df_lithostrat['mindepth'] = 0\n",
    "df_lithostrat['maxdepth'] = 0\n",
    "df_lithostrat['dipangle'] = 0\n",
    "df_lithostrat['dipazimuth'] = 0\n",
    "df_lithostrat['age'] = 0\n",
    "\n",
    "# Additional columns in dbo.Lithostrat that don't need autopopulated\n",
    "# top_age, base_age, owconf, owqual, owkind, owbaseconf, owbasequal, owbasekind, abr, interpreter, remark, geofeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lithostrat = df_lithostrat.merge(df_explo_nondefaultattributes, on='f_FieldName', how='inner')\n",
    "# df_wellsuserfieldsvalues = df_wellsuserfieldsvalues[['f_WellId', 'f_FieldID', 'f_StringValue']].sort_values(by=['f_WellId','f_FieldID'])\n",
    "# df_wellsuserfieldsvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is 'created' column entirely NaN (count 35963 rows) after reindexing?\n",
    "\n",
    "strat_order = ['well_id', 'data_type', 'top_depth', 'symbol_id', 'f_interpid', 'creator', \n",
    "              'modifier', 'source', 'attr', 'top_boundary', 'base_depth', 'base_boundary', 'legend', \n",
    "              'created', 'modified', 'obsno', 'mindepth', 'maxdepth', 'dipangle', 'dipazimuth', 'age']\n",
    "\n",
    "df_lithostrat = df_lithostrat.reindex(columns=strat_order)\n",
    "df_lithostrat.head()\n",
    "\n",
    "# Will 'pk_index' be auto generated again? On this list it came after symbol_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is \"extended geological info\" stored (Geologic Feature, Remark)\n",
    "# Note that source column in DATA_Lithostrat is nvarchar(255)  - change seismic_line from 64 to 255?\n",
    "\n",
    "#df_lithostrat['created'] = df_lithostrat['created'].apply(sqlalchemy.DateTime)\n",
    "#df_lithostrat['modified'] = pd.to_datetime\n",
    "\n",
    "#df_lithostrat[['created', 'modified']] = df_lithostrat[['created', 'modified']].apply(pd.to_datetime)\n",
    "df_lithostrat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well_id and data_type columns should be int, not float\n",
    "df_lithostrat.well_id = df_lithostrat.well_id.astype('int64')\n",
    "df_lithostrat.data_type = df_lithostrat.data_type.astype('int64')\n",
    "\n",
    "# When trying to insert 'created' and 'modified' columns to database, get error:\n",
    "# TypeError: cannot astype a datetimelike from [datetime64[ns]] to [float64]\n",
    "# Why are these floats in dbo.DATA_Lithostrat but in other tables like dbo.WELLS they're nvarchar/srt?\n",
    "# Try importing vs manually adding created and modified fields in IC.\n",
    "# See what it does with floats!\n",
    "\n",
    "# Useful doc: Data type mappings between Python and SQL Server\n",
    "# https://docs.microsoft.com/en-us/sql/advanced-analytics/python/python-libraries-and-data-types?view=sql-server-ver15\n",
    "\n",
    "# Need to Cconvert datetime64[ns] column for 'created' and 'modified' to float64\n",
    "# But you can't do that! TypeError: cannot astype a datetimelike from [datetime64[ns]] to [float64]\n",
    "\n",
    "####df_lithostrat['created'] = df_lithostrat['created'].astype('float64')\n",
    "####df_lithostrat['modified'] = df_lithostrat['modified'].astype('float64')\n",
    "\n",
    "#df_lithostrat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily drop 'created' and 'modified' columns\n",
    "df_lithostrat.drop(['created', 'modified'], axis=1, inplace=True)\n",
    "df_lithostrat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.DATA_Lithostrat\n",
    "stmt = select([func.count(datalithostrat.columns.well_id)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from table\n",
    "delete_stmt = delete(datalithostrat)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "#Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df_lithostrat to SQL Server database\n",
    "\n",
    "df_lithostrat.to_sql('DATA_Lithostrat', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.DATA_Lithostrat')\n",
    "sqlselect_rows([datalithostrat])\n",
    "\n",
    "# df_lithostrat.to_sql(name='DATA_Lithostrat', con=engine, if_exists='append', index=False,\n",
    "#             dtype={'created': sqlalchemy.DateTime(), \n",
    "#                    'modified': sqlalchemy.DateTime()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create IC Dynamic Projects</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df using arrays\n",
    "# Populate dbo.PROJECTS with relevant fields to create 4 new dynamic projects:\n",
    "# ALL WELLS, NORWAY NORTH SEA, NORWAY NORWEGIAN SEA & NORWAY BARENTS SEA\n",
    "\n",
    "# Note: six not null columns:\n",
    "    # pk_index int e.g. 1, \n",
    "    # project_id int e.g. 113,\n",
    "    # WellGroupFieldIsUserDefined bit, e.g. 0\n",
    "    # WellOrderFieldsIsUserDefined bit, e.g. 0\n",
    "    # f_dynamic bit e.g. 1 (for \n",
    "    # f_WellQueryId int (0 for non-dynamic or e.g. 129 for \"Exploration Offshore\" project with project_id = 21)\n",
    "    \n",
    "# IC dialogue requires you to enter a project \"Title\".\n",
    "\n",
    "# IC auto-selects \"Type: Static\", \"display units: metres\", \"Datum: MSL\", \"Geodatum: ED50\", \n",
    "    #\"Grid system: ED50 / UTM zone 30N\". Display tab \"Group wells by: [No Grouping]\", \"Order wells by: Name\".\n",
    "\n",
    "# Database auto-populates:  \"TVD_datum: MSL\", \n",
    "    #defchronodatatype: 0 (74 for several - probably links to Chronostratigraphy - Age), \n",
    "    #deftwtdata: NULL, \n",
    "    #defchronointerpid: 0, \n",
    "    #WellGroupField: 4,0,5,1, etc, \n",
    "    #WellGroupFieldIsUserDefined: 0, \n",
    "    #WellOrderField: 0,2, \n",
    "    #WellOrderFieldIsUserDefined: 0, \n",
    "    #WellPatternTable:0, \n",
    "    #WellPatternTableLayerField:0, \n",
    "    #WellPatternTablePolygonField:0, \n",
    "    #DefaultPRMTemplates:{}, \n",
    "    #DefaultSummaryCharts{}, \n",
    "    #DefaultWellstickTemplates:{}\n",
    "    \n",
    "# Is it OK just to overwrite the existing default IC project with these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for dynamic projects\n",
    "\n",
    "dyprojects_data = {'project_id' : ['1', '2', '3', '4'],\n",
    "                 'title' : \n",
    "                   ['ALL WELLS', \n",
    "                    'NORWAY NORTH SEA', \n",
    "                    'NORWAY NORWEGIAN SEA', \n",
    "                    'NORWAY BARENTS SEA'],\n",
    "                   #'client' : ['', '', '', ''],\n",
    "                   #'jobno' : ['', '', '', ''],\n",
    "                   #'code' : ['', '', '', ''],\n",
    "                #'notes' : ['', '', '', ''],\n",
    "                'Units' : ['M', 'M', 'M', 'M'],\n",
    "                'Map' : ['NULL', 'NULL', 'NULL', 'NULL'],\n",
    "                'datum' : ['4230', '4230', '4230', '4230'],\n",
    "                'utmzone' : \n",
    "                   ['ED50 / UTM zone 31N', \n",
    "                    'ED50 / UTM zone 31N', \n",
    "                    'ED50 / UTM zone 32N', \n",
    "                    'ED50 / UTM zone 34N'],\n",
    "                'TVD_datum' : ['MSL', 'MSL', 'MSL', 'MSL'],\n",
    "                'OWTranslation' : ['2', '2', '2', '2'],\n",
    "                #'f_fieldname' : ['', '', '', ''],\n",
    "                'defchronodatatype' : ['0', '0', '0', '0'],\n",
    "                #'deftstprops' : ['', '', '', ''],\n",
    "                'deftwtdata' : ['NULL', 'NULL', 'NULL', 'NULL'],\n",
    "                'deffaultsdatatype' : ['0', '0', '0', '0'],\n",
    "                'defchronointerpid' : ['0', '0', '0', '0'],\n",
    "                'WellGroupField' : ['0', '0', '0', '0'],\n",
    "                'WellGroupFieldIsUserDefined' : ['0', '0', '0', '0'],\n",
    "                'WellOrderField' : ['0', '0', '0', '0'],\n",
    "                'WellOrderFieldIsUserDefined' : ['0', '0', '0', '0'],\n",
    "                'WellPatternTable' : ['NULL', 'NULL', 'NULL', 'NULL'],\n",
    "                'WellPatternTableLayerField' : ['NULL', 'NULL', 'NULL', 'NULL'],\n",
    "                'WellPatternTablePolygonField' : ['NULL', 'NULL', 'NULL', 'NULL'],\n",
    "                #'RPMWellTypeField' : ['', '', '', ''],\n",
    "                #'DefaultRPMTemplates' : ['', '', '', ''],\n",
    "                'DefaultSummaryCharts' : ['{}', '{}', '{}', '{}'],\n",
    "                'DefaultWellstickTemplates' : ['{}', '{}', '{}', '{}'],\n",
    "                'f_dynamic' : ['1', '1', '1', '1'],\n",
    "                'f_WellQueryId' : ['1', '2', '3', '4']}\n",
    "\n",
    "# Create temp index, not sent to db\n",
    "df_dyprojects = pd.DataFrame(dyprojects_data, index = ['project_1', 'project_2', 'project_3', 'project_4'])\n",
    "df_dyprojects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use just populated columns\n",
    "#df_dyprojects_short = df_dyprojects.filter(['project_id', 'title', 'WellGroupFieldIsUserDefined', 'WellOrderFieldsIsUserDefined', 'f_dynamic', 'f_WellQueryId'], axis=1)\n",
    "#df_dyprojects_short.head()\n",
    "\n",
    "# To do: had trouble filtering this database (thinks its a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.PROJECTS\n",
    "stmt = select([func.count(projects.columns.project_id)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(projects)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df_dyprojects to SQL Server database\n",
    "\n",
    "df_dyprojects.to_sql('PROJECTS', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.PROJECTS')\n",
    "sqlselect_rows([projects])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create well queries to use with dynamic projects\n",
    "\n",
    "wellqueries_data = {'invertresults' : ['0', '0', '0', '0'], \n",
    "                    'category' : ['ProjectFolder(1)', 'ProjectFolder(2)', 'ProjectFolder(3)', 'ProjectFolder(4)'], \n",
    "                    'query_id' : ['1', '2', '3', '4'], \n",
    "                    'project_id' : ['-1', '-1', '-1', '-1'], \n",
    "                    'title' : ['Country = NORWAY', \n",
    "                               'Location = North Sea', \n",
    "                               'Location = Norwegian Sea', \n",
    "                               'Location = Barents Sea'], \n",
    "                    'nentries' : ['1', '1', '1', '1'], \n",
    "                    'pencolour' : ['0', '0', '0', '0'], \n",
    "                    'enttype' : ['4', '4', '4', '4'], \n",
    "                    'entdatatype' : ['0', '0', '0', '0'], \n",
    "                    'entfunction' : ['=', '=', '=', '='], \n",
    "                    'entvalue' : ['NORWAY', 'NORTH SEA', 'NORWEGIAN SEA', 'BARENTS SEA'], \n",
    "                    'entinfokey' : ['Country', 'Location', 'Location', 'Location'], \n",
    "                    'highlightstyle' : ['1', '1', '1', '1'], \n",
    "                    'highlightsymbol' : ['4198', '4198', '4198', '4198']}\n",
    "\n",
    "# Only pk_index is not null\n",
    "\n",
    "# Create temp index, not sent to db\n",
    "df_wellqueries = pd.DataFrame(wellqueries_data, index = ['wellquery_1', 'wellquery_2', 'wellquery_3', 'wellquery_4'])\n",
    "df_wellqueries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.WELLQUERIES\n",
    "stmt = select([func.count(wellqueries.columns.query_id)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(wellqueries)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())\n",
    "\n",
    "# CAN I TURN THE ABOVE INTO A FUNCTION, AS I KEEP RE-USING THE SAME CODE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df_dyprojects to SQL Server database\n",
    "\n",
    "df_wellqueries.to_sql('WELLQUERIES', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.WELLQUERIES')\n",
    "sqlselect_rows([wellqueries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PROJECTWELLS (which wells in which Projects)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets show all the dataframes used in this notebook\n",
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which of these are written to database via pandas.DataFrame.to_sql:\n",
    "    #df_explo_dbowells (still to do the same for dev wells?)\n",
    "    #df_wellsuserfields\n",
    "    #df_wellsuserfieldsvalues\n",
    "    #df_lithostrat\n",
    "    #df_dyprojects\n",
    "    #df_wellqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first row of wells and project dataframes\n",
    "# Merge on project = project_id?\n",
    "\n",
    "df_explo_dbowells.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dyprojects.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to populate the project column in dbo.WELLS\n",
    "# This appears empty just now because none of my wells have projects.\n",
    "# Create a function that populates a project number based on well name\n",
    "\n",
    "#df_explo_dbowells.location.unique()\n",
    "#df_explo_dbowells.location.isnull().sum()\n",
    "\n",
    "def location_projectid(row):\n",
    "    if row == 'NORTH SEA':\n",
    "        return 2\n",
    "    elif row == 'NORWEGIAN SEA':\n",
    "        return 3\n",
    "    elif row == 'BARENTS SEA':\n",
    "        return 4\n",
    "    else:\n",
    "        0\n",
    "\n",
    "df_explo_dbowells['project'] = df_explo_dbowells['location'].apply(location_projectid)\n",
    "df_explo_dbowells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns you're joining on (left_on and right_on) must be the same data type! \n",
    "\n",
    "# In the SQL database, project in WELLS is nvarchar(25) while project_id in PROJECTS is int.\n",
    "# In this pandas dataframes I'm writing to the database, these 2 columns are float64 and object respectively.\n",
    "# Sort this by doing a complete review of datatypes that I'm writing to_sql, and ensure each is correct wrt. the database.\n",
    "\n",
    "df_explo_dbowells['project'] = df_explo_dbowells['project'].astype(str)\n",
    "df_dyprojects['project_id'] = df_dyprojects['project_id'].astype(str)\n",
    "\n",
    "print(\"df_explo_dbowells['project']: \", df_explo_dbowells['project'].dtypes)\n",
    "print(\"df_dyprojects['project_id']: \", df_dyprojects['project_id'].dtypes)\n",
    "\n",
    "# Because I created all wells with no project, they're all null/NaN so can't convert the column to int! Produced error:\n",
    "# ValueError: invalid literal for int() with base 10: 'nan'\n",
    "# So I converted them both to str (i.e. object?), the Python equivalent of nvarchar which seemed to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbo.PROJECTWELLS requires 3 columns (all not null):\n",
    "    #pk_index 11 PK int not null\n",
    "    #well_id FK 2 int not null\n",
    "    #project_id 2 FK int not null\n",
    "    \n",
    "# Needs to join dbo.WELLS and dbo.PROJECTS?\n",
    "\n",
    "df_projectwells = df_explo_dbowells.merge(df_dyprojects, left_on='project', right_on='project_id', how='inner')\n",
    "#Error - ValueError: You are trying to merge on float64 and object columns. If you wish to proceed you should use pd.concat\n",
    "\n",
    "df_projectwells[['name', 'well_id', 'project', 'project_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projectwells = df_projectwells[['well_id', 'project_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projectwells[df_projectwells.project_id == '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all data from dbo.WELLQUERIES\n",
    "stmt = select([func.count(projectwells.columns.project_id)])\n",
    "\n",
    "# Execute the select statement and use the scalar() fetch method to save the record count\n",
    "connection.execute(stmt).scalar()\n",
    "\n",
    "# Delete all records from ? table\n",
    "delete_stmt = delete(projectwells)\n",
    "result_proxy = connection.execute(delete_stmt)\n",
    "\n",
    "# Print affected row count\n",
    "result_proxy.rowcount\n",
    "\n",
    "# Print results of the executing statement to verify there are no rows\n",
    "print(connection.execute(stmt).fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projectwells.to_sql('PROJECTWELLS', engine, if_exists='replace', index = False)\n",
    "\n",
    "print('dbo.PROJECTWELLS')\n",
    "sqlselect_rows([projectwells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
